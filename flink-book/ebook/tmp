<?xml version="1.0" encoding="UTF-8"?>
<?asciidoc-toc maxdepth="4"?>
<?asciidoc-numbered?>
<article xmlns="http://docbook.org/ns/docbook" xmlns:xl="http://www.w3.org/1999/xlink" version="5.0" xml:lang="en">
<info>
<title>Flink教程</title>
<date>2021-06-03</date>
</info>
<section xml:id="_有状态的流式处理">
<title>有状态的流式处理</title>
<simpara>Apache
Flink是一个分布式流处理器，具有直观和富有表现力的API，可实现有状态的流处理应用程序。它以容错的方式有效地大规模运行这些应用程序。
Flink于2014年4月加入Apache软件基金会作为孵化项目，并于2015年1月成为顶级项目。从一开始，Flink就拥有一个非常活跃且不断增长的用户和贡献者社区。到目前为止，已有超过五百人为Flink做出贡献，并且它已经发展成为最复杂的开源流处理引擎之一，并得到了广泛采用的证明。
Flink为不同行业和全球的许多公司和企业提供大规模的商业关键应用。</simpara>
<simpara>流处理技术在大大小小的公司中越来越受欢迎，因为它为许多已建立的用例（如数据分析，ETL和事务应用程序）提供了卓越的解决方案，同时也促进了新颖的应用程序，软件架构和商机。接下来我们将讨论，为什么有状态流处理变得如此受欢迎并评估其潜力。我们首先回顾传统的数据应用程序架构并指出它们的局限性。接下来，我们介绍基于状态流处理的应用程序设计
与传统方法相比，它具有许多有趣的特征最后，我们简要讨论开源流处理器的发展，并在本地Flink实例上运行流应用程序。</simpara>
<section xml:id="_传统数据处理架构">
<title>传统数据处理架构</title>
<simpara>数十年来，数据和数据处理在企业中无处不在。多年来，数据的收集和使用一直在增长，公司已经设计并构建了基础架构来管理数据。大多数企业实施的传统架构区分了两种类型的数据处理：事务处理（OLTP）和分析处理（OLAP）。</simpara>
<section xml:id="_事务处理">
<title>事务处理</title>
<simpara>公司将各种应用程序用于日常业务活动，例如企业资源规划（ERP）系统，客户关系管理（CRM）软件和基于Web的应用程序。这些系统通常设计有单独的层，用于数据处理（应用程序本身）和数据存储（事务数据库系统），如图所示。</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/spaf_0101.png"/>
</imageobject>
<textobject><phrase>传统架构</phrase></textobject>
</mediaobject>
</informalfigure>
<simpara>应用程序通常连接到外部服务或直接面向用户，并持续处理传入的事件，如网站上的订单，电子邮件或点击。处理事件时，应用程序将会读取远程数据库的状态，或者通过运行事务来更新它。通常，一个数据库系统可以服务于多个应用程序，它们有时会访问相同的数据库或表。</simpara>
<simpara>当应用程序需要扩展时，这样的设计可能会导致问题。由于多个应用程序可能会同时用到相同的数据表示，或者共享相同的基础设施，因此想要更改表的结构或扩展数据库，就需要仔细的规划和大量的工作。克服紧耦合应用程序的最新方法是微服务设计模式。微服务被设计为小型、完备且独立的应用程序。他们遵循UNIX的理念，即<literal>只做一件事并且把它做好</literal>。通过将几个微服务相互连接来构建更复杂的应用程序，这些微服务仅通过标准化接口（例如RESTful
HTTP连接）进行通信。由于微服务严格地彼此分离并且仅通过明确定义的接口进行通信，因此每个微服务都可以用不同技术栈来实现，包括编程语言、类库和数据存储。微服务和所有必需的软件和服务通常捆绑在一起并部署在独立的容器中。下图描绘了一种微服务架构。</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/spaf_0102.png"/>
</imageobject>
<textobject><phrase>微服务架构</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="_分析处理">
<title>分析处理</title>
<simpara>大量数据存储在公司的各种事务数据库系统中，它们可以为公司业务运营提供宝贵的参考意见。例如，分析订单处理系统的数据，可以获得销量随时间的增长曲线；可以识别延迟发货的原因；还可以预测未来的销量以便提前调整库存。但是，事务数据通常分布在多个数据库中，它们往往汇总起来联合分析时更有价值。而且，数据通常需要转换为通用格式。</simpara>
<simpara>所以我们一般不会直接在事务数据库上运行分析查询，而是复制数据到数据仓库。数据仓库是对工作负载进行分析和查询的专用数据存储。为了填充数据仓库，需要将事务数据库系统管理的数据复制过来。将数据复制到数据仓库的过程称为extract-transform-load（ETL）。
ETL过程从事务数据库中提取数据，将其转换为某种通用的结构表示，可能包括验证，值的规范化，编码，重复数据删除（去重）和模式转换，最后将其加载到分析数据库中。
ETL过程可能非常复杂，并且通常需要技术复杂的解决方案来满足性能要求。
ETL过程需要定期运行以保持数据仓库中的数据同步。</simpara>
<simpara>将数据导入数据仓库后，可以查询和分析数据。通常，在数据仓库上执行两类查询。第一种类型是定期报告查询，用于计算与业务相关的统计信息，比如收入、用户增长或者输出的产量。这些指标汇总到报告中，帮助管理层评估业务的整体健康状况。第二种类型是即席查询，旨在提供特定问题的答案并支持关键业务决策，例如收集统计在投放商业广告上的花费，和获取的相应收入，以评估营销活动的有效性。两种查询由批处理方式由数据仓库执行，如图所示。</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/spaf_0103.png"/>
</imageobject>
<textobject><phrase>在线分析</phrase></textobject>
</mediaobject>
</informalfigure>
<simpara>如今，Apache
Hadoop生态系统的组件，已经是许多企业IT基础架构中不可或缺的组成部分。现在的做法不是直接将所有数据都插入关系数据库系统，而是将大量数据（如日志文件，社交媒体或Web点击日志）写入Hadoop的分布式文件系统（HDFS）、S3或其他批量数据存储库，如Apache
HBase，以较低的成本提供大容量存储容量。驻留在此类存储系统中的数据可以通过SQL-on-Hadoop引擎查询和处理，例如Apache
Hive，Apache Drill或Apache
Impala。但是，基础结构与传统数据仓库架构基本相同。</simpara>
</section>
</section>
<section xml:id="_有状态的流式处理_2">
<title>有状态的流式处理</title>
<simpara>日常生活中，所有数据都是作为连续的事件流创建的。比如网站或者移动应用中的用户交互动作，订单的提交，服务器日志或传感器测量数据：所有这些都是事件流。实际上，很少有应用场景，能一次性地生成所需要的完整（有限）数据集。实际应用中更多的是无限事件流。有状态的流处理就是用于处理这种无限事件流的应用程序设计模式，在公司的IT基础设施中有广泛的应用场景。在我们讨论其用例之前，我们将简要介绍有状态流处理的工作原理。</simpara>
<simpara>如果我们想要无限处理事件流，并且不愿意繁琐地每收到一个事件就记录一次，那这样的应用程序就需要是有状态的，也就是说能够存储和访问中间数据。当应用程序收到一个新事件时，它可以从状态中读取数据，或者向该状态写入数据，总之可以执行任何计算。原则上讲，我们可以在各种不同的地方存储和访问状态，包括程序变量（内存）、本地文件，还有嵌入式或外部数据库。</simpara>
<simpara>Apache
Flink将应用程序状态，存储在内存或者嵌入式数据库中。由于Flink是一个分布式系统，因此需要保护本地状态以防止在应用程序或计算机故障时数据丢失。
Flink通过定期将应用程序状态的一致性检查点（check
point）写入远程且持久的存储，来保证这一点。状态、状态一致性和Flink的检查点将在后面的章节中更详细地讨论，但是，现在，下图显示了有状态的流式Flink应用程序。</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/spaf_0104.png"/>
</imageobject>
<textobject><phrase>有状态的流式处理</phrase></textobject>
</mediaobject>
</informalfigure>
<simpara>有状态的流处理应用程序，通常从事件日志中提取输入事件。事件日志就用来存储和分发事件流。事件被写入持久的仅添加（append-only）日志，这意味着无法更改写入事件的顺序。写入事件日志的流，可以被相同或不同的消费者多次读取。由于日志的仅附加（append-only）属性，事件始终以完全相同的顺序发布给所有消费者。现在已有几种事件日志系统，其中Apache
Kafka是最受欢迎的，可以作为开源软件使用，或者是云计算提供商提供的集成服务。</simpara>
<simpara>在Flink上运行的有状态的流处理应用程序，是很有意思的一件事。在这个架构中，事件日志会按顺序保留输入事件，并且可以按确定的顺序重播它们。如果发生故障，Flink将从先前的检查点（check
point）恢复其状态，并重置事件日志上的读取位置，这样就可以恢复整个应用。应用程序将重放（并快进）事件日志中的输入事件，直到它到达流的尾部。此技术一般用于从故障中恢复，但也可用于更新应用程序、修复bug或者修复以前发出的结果，另外还可以用于将应用程序迁移到其他群集，或使用不同的应用程序版本执行A
/ B测试。</simpara>
<simpara>如前所述，有状态的流处理是一种通用且灵活的设计架构，可用于许多不同的场景。在下文中，我们提出了三类通常使用有状态流处理实现的应用程序：（1）事件驱动应用程序，（2）数据管道应用程序，以及（3）数据分析应用程序。</simpara>
<simpara>我们将应用程序分类描述，是为了强调有状态流处理适用于多种业务场景；而实际的应用中，往往会具有以上多种情况的特征。</simpara>
<section xml:id="_事件驱动应用程序">
<title>事件驱动应用程序</title>
<simpara>事件驱动的应用程序是有状态的流应用程序，它们使用特定的业务逻辑来提取事件流并处理事件。根据业务逻辑，事件驱动的应用程序可以触发诸如发送警报、或电子邮件之类的操作，或者将事件写入向外发送的事件流以供另一个应用程序使用。</simpara>
<simpara>事件驱动应用程序的典型场景包括：</simpara>
<itemizedlist>
<listitem>
<simpara>实时推荐（例如，在客户浏览零售商网站时推荐产品）</simpara>
</listitem>
<listitem>
<simpara>行为模式检测或复杂事件处理（例如，用于信用卡交易中的欺诈检测）</simpara>
</listitem>
<listitem>
<simpara>异常检测（例如，检测侵入计算机网络的尝试）</simpara>
</listitem>
</itemizedlist>
<simpara>事件驱动应用程序是微服务的演变。它们通过事件日志而不是REST调用进行通信，并将应用程序数据保存为本地状态，而不是将其写入外部数据存储区（例如关系数据库或键值数据库）。下图显示了由事件驱动的流应用程序组成的服务架构。</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/spaf_0105.png"/>
</imageobject>
<textobject><phrase>事件驱动的流应用程序</phrase></textobject>
</mediaobject>
</informalfigure>
<simpara>上图中的应用程序通过事件日志连接。一个应用程序将其输出发送到事件日志通道（kafka），另一个应用程序使用其他应用程序发出的事件。事件日志通道将发送者和接收者分离，并提供异步、非阻塞的事件传输。每个应用程序都可以是有状态的，并且可以本地管理自己的状态而无需访问外部数据存储。应用程序也可以单独处理和扩展。</simpara>
<simpara>与事务性应用程序或微服务相比，事件驱动的应用程序具有多种优势。与读写远程数据库相比，本地状态访问提供了非常好的性能。扩展性和容错性都由流处理器来保证，并且以事件日志作为输入源，应用程序的整个输入数据可以可靠地存储，并且可以确定性地重放。此外，Flink可以将应用程序的状态重置为先前的保存点（save
point），从而可以在不丢失状态的情况下更新或重新扩展应用程序。</simpara>
<simpara>事件驱动的应用程序对运行它们的流处理器有很高的要求，并不是所有流处理器都适合运行事件驱动的应用程序。
API的表现力，以及对状态处理和事件时间支持的程度，决定了可以实现和执行的业务逻辑。这方面取决于流处理器的API，主要看它能提供什么样的状态类型，以及它对事件时间处理的支持程度。此外，精确一次（exactly-once）的状态一致性和扩展应用程序的能力是事件驱动应用程序的基本要求。
Apache Flink符合所有的这些要求，是运行此类应用程序的一个非常好的选择。</simpara>
</section>
<section xml:id="_数据管道">
<title>数据管道</title>
<simpara>当今的IT架构包括许多不同的数据存储，例如关系型数据库和专用数据库系统、事件日志、分布式文件系统，内存中的缓存和搜索索引。所有这些系统都以不同的格式和数据结构存储数据，为其特定的访问模式提供最佳性能。公司通常将相同的数据存储在多个不同的系统中，以提高数据访问的性能。例如，网上商店中提供的产品的信息，可以存储在交易数据库中，同时也存储在缓存（如redis）和搜索索引（如ES）中。由于数据的这种复制，数据存储必须保持同步。</simpara>
<simpara>在不同存储系统中同步数据的传统方法是定期ETL作业。但是，它们不能满足当今许多场景的延迟要求。另一种方法是使用事件日志（event
log）来发布更新。更新将写入事件日志并由事件日志分发。日志的消费者获取到更新之后，将更新合并到受影响的数据存储中。根据使用情况，传输的数据可能需要标准化、使用外部数据进行扩展，或者在目标数据存储提取之前进行聚合。</simpara>
<simpara>以较低的延迟，来提取、转换和插入数据是有状态流处理应用程序的另一个常见应用场景。这种类型的应用程序称为数据管道（data
pipeline）。数据管道必须能够在短时间内处理大量数据。操作数据管道的流处理器还应具有许多源（source）和接收器（sink）的连接器，以便从各种存储系统读取数据并将数据写入各种存储系统。当然，同样地，Flink完成了所有这些功能。</simpara>
</section>
<section xml:id="_流分析">
<title>流分析</title>
<simpara>ETL作业定期将数据导入数据存储区，数据的处理是由即席查询（用户自定义查询）或设定好的通常查询来做的。无论架构是基于数据仓库还是基于Hadoop生态系统的组件，这都是批处理。多年来最好的处理方式就是，定期将数据加载到数据分析系统中，但它给分析管道带了的延迟相当大，而且无法避免。</simpara>
<simpara>根据设定好的时间间隔，可能需要数小时或数天才能将数据点包含在报告中。我们前面已经提到，数据管道可以实现低延迟的ETL，所以在某种程度上，可以通过使用数据管道将数据导入存储区来减少延迟。但是，即使持续不停地进行ETL操作，在用查询来处理事件之前总会有延迟。虽然这种延迟在过去可能是可以接受的，但是今天的应用程序，往往要求必须能够实时收集数据，并立即对其进行操作（例如，在手机游戏中去适应不断变化的条件，或者在电商网站中提供个性化的用户体验）。</simpara>
<simpara>流式分析应用程序不是等待定期触发，而是连续地提取事件流，并且通过纳入最新事件来更新其计算结果，这个过程是低延迟的。这有些类似于数据库中用于更新视图（views）的技术。通常，流应用程序将其结果存储在支持更新的外部数据存储中，例如数据库或键值（key-value）存储。流分析应用程序的实时更新结果可用于驱动监控仪表板（dashboard）应用程序，如下图所示。</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/spaf_0106.png"/>
</imageobject>
<textobject><phrase>仪表盘</phrase></textobject>
</mediaobject>
</informalfigure>
<simpara>流分析应用程序最大的优势就是，将每个事件纳入到分析结果所需的时间短得多。除此之外，流分析应用程序还有另一个不太明显的优势。传统的分析管道由几个独立的组件组成，例如ETL过程、存储系统、对于基于Hadoop的环境，还包括用于触发任务（jobs）的数据处理和调度程序。相比之下，如果我们运行一个有状态流应用程序，那么流处理器就会负责所有这些处理步骤，包括事件提取、带有状态维护的连续计算以及更新结果。此外，流处理器可以从故障中恢复，并且具有精确一次（exactly-once）的状态一致性保证，还可以调整应用程序的计算资源。像Flink这样的流处理器还支持事件时间（event-time）处理，这可以保证产生正确和确定的结果，并且能够在很短的时间内处理大量数据。</simpara>
<simpara>流分析应用程序通常用于：</simpara>
<itemizedlist>
<listitem>
<simpara>监控手机网络的质量分析</simpara>
</listitem>
<listitem>
<simpara>移动应用中的用户行为</simpara>
</listitem>
<listitem>
<simpara>实时数据的即席分析</simpara>
</listitem>
</itemizedlist>
<simpara>虽然我们不在此处介绍，但Flink还提供对流上的分析SQL查询的支持。</simpara>
</section>
</section>
<section xml:id="_开源流处理的演进">
<title>开源流处理的演进</title>
<simpara>数据流处理并不是一项新技术。一些最初的研究原型和商业产品可以追溯到20世纪90年代（1990s）。然而，在很大程度上，过去采用的流处理技术是由成熟的开源流处理器驱动的。如今，分布式开源流处理器在不同行业的许多企业中，处理着核心业务应用，比如电商、社交媒体、电信、游戏和银行等。开源软件是这一趋势的主要驱动力，主要原因有两个：</simpara>
<itemizedlist>
<listitem>
<simpara>开源流处理软件是大家每一个人都可以评估和使用的产品。</simpara>
</listitem>
<listitem>
<simpara>由于许多开源社区的努力，可扩展流处理技术正在迅速成熟和发展。</simpara>
</listitem>
</itemizedlist>
<simpara>仅仅一个Apache软件基金会就支持了十几个与流处理相关的项目。新的分布式流处理项目不断进入开源阶段，并不断增加新的特性和功能。开源社区不断改进其项目的功能，并正在推动流处理的技术边界。我们将简要介绍一下过去，看看开源流处理的起源和今天的状态。</simpara>
<simpara>第一代分布式开源流处理器（2011）专注于具有毫秒延迟的事件处理，并提供了在发生故障时防止事件丢失的保证。这些系统具有相当低级的API，并且对于流应用程序的准确性和结果的一致性，不提供内置支持，因为结果会取决于到达事件的时间和顺序。另外，即使事件没有丢失，也可能不止一次地处理它们。与批处理器相比，第一代开源流处理器牺牲了结果准确性，用来获得更低的延迟。为了让当时的数据处理系统，可以同时提供快速和准确的结果，人们设计了所谓的lambda架构，如图所示。</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/spaf_0107.png"/>
</imageobject>
<textobject><phrase>lambda架构</phrase></textobject>
</mediaobject>
</informalfigure>
<simpara>lambda架构增强了传统的批处理架构，其<literal>快速层</literal>（speed
layer）由低延迟的流处理器来支持。数据到达之后由流处理器提取出来，并写入批处理存储。流处理器近乎实时地计算近似结果并将它们写入<literal>快速表</literal>（speed
table）。批处理器定期处理批量存储中的数据，将准确的结果写入批处理表，并从速度表中删除相应的不准确结果。应用程序会合并快速表中的近似结果和批处理表中的准确结果，然后消费最终的结果。</simpara>
<simpara>lambda架构现在已经不再是最先进的，但仍在许多地方使用。该体系结构的最初目标是改善原始批处理分析体系结构的高延迟。但是，它有一些明显的缺点。首先，它需要对一个应用程序，做出两个语义上等效的逻辑实现，用于两个独立的、具有不同API的处理系统。其次，流处理器计算的结果只是近似的。第三，lambda架构很难建立和维护。</simpara>
<simpara>通过在第一代基础上进行改进，下一代分布式开源流处理器（2013）提供了更好的故障保证，并确保在发生故障时，每个输入记录仅对结果产生一次影响（exactly
-once）。此外，编程API从相当低级的操作符接口演变为高级API。但是，一些改进（例如更高的吞吐量和更好的故障保证）是以将处理延迟从毫秒增加到几秒为代价的。此外，结果仍然取决于到达事件的时间和顺序。</simpara>
<simpara>第三代分布式开源流处理器（2015）解决了结果对到达事件的时间和顺序的依赖性。结合精确一次（exactly-once）的故障语义，这一代系统是第一个具有计算一致性和准确结果的开源流处理器。通过基于实际数据来计算结果（<literal>重演</literal>数据），这些系统还能够以与<literal>实时</literal>数据相同的方式处理历史数据。另一个改进是解决了延迟/吞吐量无法同时保证的问题。先前的流处理器仅能提供高吞吐量或者低延迟（其中之一），而第三代系统能够同时提供这两个特性。这一代的流处理器使得lambda架构过时了。当然，这一代流处理以flink为代表。</simpara>
<simpara>除了目前讨论的特性，例如容错、性能和结果准确性之外，流处理器还不断添加新的操作功能，例如高可用性设置，与资源管理器（如YARN或Kubernetes）的紧密集成，以及能够动态扩展流应用程序。其他功能包括：支持升级应用程序代码，或将作业迁移到其他群集或新版本的流处理器，而不会丢失当前状态。</simpara>
</section>
<section xml:id="_flink简介">
<title>Flink简介</title>
<simpara>Apache
Flink是第三代分布式流处理器，它拥有极富竞争力的功能。它提供准确的大规模流处理，具有高吞吐量和低延迟。特别的是，以下功能使Flink脱颖而出：</simpara>
<itemizedlist>
<listitem>
<simpara>事件时间（event-time）和处理时间（processing-tme）语义。即使对于无序事件流，事件时间（event-time）语义仍然能提供一致且准确的结果。而处理时间（processing-time）语义可用于具有极低延迟要求的应用程序。</simpara>
</listitem>
<listitem>
<simpara>精确一次（exactly-once）的状态一致性保证。</simpara>
</listitem>
<listitem>
<simpara>每秒处理数百万个事件，毫秒级延迟。
Flink应用程序可以扩展为在数千个核（cores）上运行。</simpara>
</listitem>
<listitem>
<simpara>分层API，具有不同的权衡表现力和易用性。本书介绍了DataStream
API和过程函数（process
function），为常见的流处理操作提供原语，如窗口和异步操作，以及精确控制状态和时间的接口。本书不讨论Flink的关系API，SQL和LINQ风格的Table
API。</simpara>
</listitem>
<listitem>
<simpara>连接到最常用的存储系统，如Apache Kafka，Apache
Cassandra，Elasticsearch，JDBC，Kinesis和（分布式）文件系统，如HDFS和S3。</simpara>
</listitem>
<listitem>
<simpara>由于其高可用的设置（无单点故障），以及与Kubernetes，YARN和Apache
Mesos的紧密集成，再加上从故障中快速恢复和动态扩展任务的能力，Flink能够以极少的停机时间7*24全天候运行流应用程序。</simpara>
</listitem>
<listitem>
<simpara>能够更新应用程序代码并将作业（jobs）迁移到不同的Flink集群，而不会丢失应用程序的状态。</simpara>
</listitem>
<listitem>
<simpara>详细且可自定义的系统和应用程序指标集合，以提前识别问题并对其做出反应。</simpara>
</listitem>
<listitem>
<simpara>最后但同样重要的是，Flink也是一个成熟的批处理器。</simpara>
</listitem>
</itemizedlist>
<simpara>除了这些功能之外，Flink还是一个非常易于开发的框架，因为它易于使用的API。嵌入式执行模式，可以在单个JVM进程中启动应用程序和整个Flink系统，这种模式一般用于在IDE中运行和调试Flink作业。在开发和测试Flink应用程序时，此功能非常有用。</simpara>
</section>
<section xml:id="_编写第一个flink程序">
<title>编写第一个Flink程序</title>
<section xml:id="_在idea中编写flink程序">
<title>在IDEA中编写Flink程序</title>
<orderedlist numeration="arabic">
<listitem>
<simpara>使用Intellij IDEA创建一个Maven新项目</simpara>
</listitem>
<listitem>
<simpara>勾选Create from archetype，然后点击Add Archetype按钮</simpara>
</listitem>
<listitem>
<simpara>GroupId中输入org.apache.flink，ArtifactId中输入flink-quickstart-java，Version中输入Flink的版本号，然后点击OK</simpara>
</listitem>
<listitem>
<simpara>点击向右箭头，出现下拉列表，选中flink-quickstart-java:版本号，点击Next</simpara>
</listitem>
<listitem>
<simpara>Name中输入FlinkTutorial，GroupId中输入com.atguigu，ArtifactId中输入FlinkTutorial，点击Next</simpara>
</listitem>
<listitem>
<simpara>最好使用IDEA默认的Maven工具：Bundled（Maven
3），点击Finish，等待一会儿，项目就创建好了</simpara>
</listitem>
</orderedlist>
<simpara>编写`WordCount.java`程序</simpara>
<programlisting language="java" linenumbering="unnumbered">public class WordCount {

    public static void main(String[] args) throws Exception {
        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        DataStream&lt;String&gt; stream = env.fromElements("Hello World", "Hello World");

        stream
                .flatMap(new Tokenizer())
                .keyBy(r -&gt; r.f0)
                .sum(1)
                .print();

        env.execute("单词计数");
    }

    public static class Tokenizer implements FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt; {
        @Override
        public void flatMap(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out) throws Exception {
            String[] stringList = value.split("\\s");
            for (String s : stringList) {
                out.collect(Tuple2.of(s, 1));
            }
        }
    }
}</programlisting>
</section>
<section xml:id="_下载flink运行时环境提交jar包的运行方式">
<title>下载Flink运行时环境，提交Jar包的运行方式</title>
<simpara>先下载压缩包，然后进行解压。然后进入文件夹。</simpara>
<programlisting language="bash" linenumbering="unnumbered">$ ./bin/start-cluster.sh</programlisting>
<simpara>可以打开Flink WebUI查看集群状态：http://localhost:8081</simpara>
<simpara>在IDEA中使用maven package打包。</simpara>
<simpara>提交打包好的JAR包：</simpara>
<programlisting language="bash" linenumbering="unnumbered">$ ./bin/flink run 打包好的JAR包的绝对路径</programlisting>
<simpara>停止Flink集群</simpara>
<programlisting language="bash" linenumbering="unnumbered">$ ./bin/stop-cluster.sh</programlisting>
<simpara>查看标准输出日志的位置，在log文件夹中。</simpara>
<programlisting language="bash" linenumbering="unnumbered">$ cd log/</programlisting>
</section>
</section>
</section>
<section xml:id="_流处理基础">
<title>流处理基础</title>
<section xml:id="_数据流编程">
<title>数据流编程</title>
<simpara>在我们深入研究流处理的基础知识之前，让我们来看看在数据流程编程的背景和使用的术语。</simpara>
<section xml:id="_数据流图">
<title>数据流图</title>
<simpara>顾名思义，数据流程序描述了数据如何在算子之间流动。数据流程序通常表示为有向图，其中节点称为算子，用来表示计算，边表示数据之间的依赖性。算子是数据流程序的基本功能单元。他们从输入消耗数据，对它们执行计算，并生成数据输出用于进一步处理。一个数据流图必须至少有一个数据源和一个数据接收器。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0201.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>像上图中的数据流图被称为逻辑流图，因为它们表示了计算逻辑的高级视图。为了执行一个数据流程序，Flink会将逻辑流图转换为物理数据流图，详细说明程序的执行方式。例如，如果我们使用分布式处理引擎，每个算子在不同的物理机器可能有几个并行的任务运行。图2-2显示了图2-1逻辑图的物理数据流图。而在逻辑数据流图中节点表示算子，在物理数据流图中，节点是任务。<literal>Extract
hashtags</literal>和<literal>Count</literal>算子有两个并行算子任务，每个算子任务对输入数据的子集执行计算。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0202.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
</section>
<section xml:id="_数据并行和任务并行">
<title>数据并行和任务并行</title>
<simpara>我们可以以不同方式利用数据流图中的并行性。第一，我们可以对输入数据进行分区，并在数据的子集上并行执行具有相同算子的任务并行。这种类型的并行性被称为数据并行性。数据并行是有用的，因为它允许处理大量数据，并将计算分散到不同的计算节点上。第二，我们可以将不同的算子在相同或不同的数据上并行执行。这种并行性称为任务并行性。使用任务并行性，我们可以更好地利用计算资源。</simpara>
</section>
<section xml:id="_数据交换策略">
<title>数据交换策略</title>
<simpara>数据交换策略定义了在物理执行流图中如何将数据分配给任务。数据交换策略可以由执行引擎自动选择，具体取决于算子的语义或我们明确指定的语义。在这里，我们简要回顾一些常见的数据交换策略，如图2-3所示。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0203.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<itemizedlist>
<listitem>
<simpara>前向策略将数据从一个任务发送到接收任务。如果两个任务都位于同一台物理计算机上（这通常由任务调度器确保），这种交换策略可以避免网络通信。</simpara>
</listitem>
<listitem>
<simpara>广播策略将所有数据发送到算子的所有的并行任务上面去。因为这种策略会复制数据和涉及网络通信，所以代价相当昂贵。</simpara>
</listitem>
<listitem>
<simpara>基于键控的策略通过Key值(键)对数据进行分区保证具有相同Key的数据将由同一任务处理。在图2-2中，输出<literal>Extract
hashtags</literal>算子使用键来分区（hashtag），以便count算子的任务可以正确计算每个`#`标签的出现次数。</simpara>
</listitem>
<listitem>
<simpara>随机策略统一将数据分配到算子的任务中去，以便均匀地将负载分配到不同的计算任务。</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="_并行处理流数据">
<title>并行处理流数据</title>
<simpara>既然我们熟悉了数据流编程的基础知识，现在是时候看看这些概念如何应用于并行的处理数据流了。但首先，让我们定义术语数据流：数据流是一个可能无限的事件序列。</simpara>
<simpara>数据流中的事件可以表示监控数据，传感器测量数据，信用卡交易数据，气象站观测数据，在线用户交互数据，网络搜索数据等。在本节中，我们将学习如何并行处理无限流，使用数据流编程范式。</simpara>
<section xml:id="_延迟和吞吐量">
<title>延迟和吞吐量</title>
<simpara>流处理程序不同与批处理程序。在评估性能时，要求也有所不同。对于批处理程序，我们通常关心一个作业的总的执行时间，或我们的处理引擎读取输入所需的时间，执行计算，并回写结果。由于流处理程序是连续运行的，输入可能是无界的，所以数据流处理中没有总执行时间的概念。
相反，流处理程序必须尽可能快的提供输入数据的计算结果。我们使用延迟和吞吐量来表征流处理的性能要求。</simpara>
</section>
<section xml:id="_延迟">
<title>延迟</title>
<simpara>延迟表示处理事件所需的时间。它是接收事件和看到在输出中处理此事件的效果之间的时间间隔。要直观的理解延迟，考虑去咖啡店买咖啡。当你进入咖啡店时，可能还有其他顾客在里面。因此，你排队等候直到轮到你下订单。收银员收到你的付款并通知准备饮料的咖啡师。一旦你的咖啡准备好了，咖啡师会叫你的名字，你可以到柜台拿你的咖啡。服务延迟是从你进入咖啡店的那一刻起，直到你喝上第一口咖啡之间的时间间隔。</simpara>
<simpara>在数据流中，延迟是以时间为单位测量的，例如毫秒。根据应用程序，我们可能会关心平均延迟，最大延迟或百分位延迟。例如，平均延迟值为10ms意味着处理事件的平均时间在10毫秒内。或者，延迟值为95%，10ms表示95%的事件在10ms内处理完毕。平均值隐藏了处理延迟的真实分布，可能会让人难以发现问题。如果咖啡师在准备卡布奇诺之前用完了牛奶，你必须等到他们从供应室带来一些。虽然你可能会因为这么长时间的延迟而生气，但大多数其他客户仍然会感到高兴。</simpara>
<simpara>确保低延迟对于许多流应用程序来说至关重要，例如欺诈检测，系统警报，网络监控和提供具有严格服务水平协议的服务。低延迟是流处理的关键特性，它实现了我们所谓的实时应用程序。像Apache
Flink这样的现代流处理器可以提供低至几毫秒的延迟。相比之下，传统批处理程序延迟通常从几分钟到几个小时不等。在批处理中，首先需要收集事件批次，然后才能处理它们。因此，延迟是受每个批次中最后一个事件的到达时间的限制。所以自然而然取决于批的大小。真正的流处理不会引入这样的人为延迟，因此可以实现真正的低延迟。真的流模型，事件一进入系统就可以得到处理。延迟更密切地反映了在每个事件上必须进行的实际工作。</simpara>
</section>
<section xml:id="_吞吐量">
<title>吞吐量</title>
<simpara>吞吐量是衡量系统处理能力的指标，也就是处理速率。也就是说，吞吐量告诉我们每个时间单位系统可以处理多少事件。重温咖啡店的例子，如果商店营业时间为早上7点至晚上7点。当天为600个客户提供了服务，它的平均吞吐量将是每小时50个客户。虽然我们希望延迟尽可能低，但我们通常也需要吞吐量尽可能高。</simpara>
<simpara>吞吐量以每个时间单位系统所能处理的事件数量或操作数量来衡量。值得注意的是，事件处理速率取决于事件到达的速率，低吞吐量并不一定表示性能不佳。
在流式系统中，我们通常希望确保我们的系统可以处理最大的预期事件到达的速率。也就是说，我们主要的关注点在于确定的峰值吞吐量是多少，当系统处于最大负载时性能怎么样。为了更好地理解峰值吞吐量的概念，让我们考虑一个流处理
程序没有收到任何输入的数据，因此没有消耗任何系统资源。当第一个事件进来时，它会尽可能以最小延迟立即处理。例如，如果你是第一个出现在咖啡店的顾客，在早上开门后，你将立即获得服务。理想情况下，您希望此延迟保持不变
，并且独立于传入事件的速率。但是，一旦我们达到使系统资源被完全使用的事件传入速率，我们将不得不开始缓冲事件。在咖啡店里
，午餐后会看到这种情况发生。许多人出现在同一时间，必须排队等候。在此刻，咖啡店系统已达到其峰值吞吐量，进一步增加
事件传入的速率只会导致更糟糕的延迟。如果系统继续以可以处理的速率接收数据，缓冲区可能变为不可用，数据可能会丢失。这种情况是众所周知的
作为背压，有不同的策略来处理它。</simpara>
</section>
<section xml:id="_延迟与吞吐量的对比">
<title>延迟与吞吐量的对比</title>
<simpara>此时，应该清楚延迟和吞吐量不是独立指标。如果事件需要在处理流水线中待上很长时间，我们不能轻易确保高吞吐量。同样，如果系统容量很小，事件将被缓冲，而且必须等待才能得到处理。</simpara>
<simpara>让我们重温一下咖啡店的例子来阐明一下延迟和吞吐量如何相互影响。首先，应该清楚存在没有负载时的最佳延迟。也就是说，如果你是咖啡店的唯一客户，会很快得到咖啡。然而，在繁忙时期，客户将不得不排队等待，并且会有延迟增加。另一个影响延迟和吞吐量的因素是处理事件所花费的时间或为每个客户提供服务所花费的时间。想象一下，期间圣诞节假期，咖啡师不得不为每杯咖啡画圣诞老人。这意味着准备一杯咖啡需要的时间会增加，导致每个人花费
更多的时间在等待咖啡师画圣诞老人，从而降低整体吞吐量。</simpara>
<simpara>那么，你可以同时获得低延迟和高吞吐量吗？或者这是一个无望的努力？我们可以降低得到咖啡的延迟
，方法是：聘请一位更熟练的咖啡师来准备咖啡。在高负载时，这种变化也会增加吞吐量，因为会在相同的时间内为更多的客户提供服务。
实现相同结果的另一种方法是雇用第二个咖啡师来利用并行性。这里的主要想法是降低延迟来增加吞吐量。当然，如果系统可以更快的执行操作，它可以在相同的时间内执行更多操作。
事实上，在流中利用并行性时也会发生这种情况。通过并行处理多个流，在同时处理更多事件的同时降低延迟。</simpara>
</section>
</section>
<section xml:id="_数据流上的操作">
<title>数据流上的操作</title>
<simpara>流处理引擎通常提供一组内置操作：摄取(ingest)，转换(transform)和输出流(output)。这些操作可以
结合到数据流图中来实现逻辑流处理程序。在本节中，我们描述最常见的流处理操作。</simpara>
<simpara>操作可以是无状态的或有状态的。无状态操作不保持任何内部状态。也就是说，事件的处理不依赖于过去看到的任何事件，也没有保留历史。
无状态操作很容易并行化，因为事件可以彼此独立地处理，也独立于事件到达的顺序(和事件到达顺序没有关系)。
而且，在失败的情况下，无状态操作可以是简单的重新启动并从中断处继续处理。相反，
有状态操作可能会维护之前收到的事件的信息。此状态可以通过传入事件更新，也可以用于未来事件的处理逻辑。有状态的流
处理应用程序更难以并行化和以容错的方式来运行，因为状态需要有效的进行分区和在发生故障的情况下可靠地恢复。</simpara>
<section xml:id="_数据摄入和数据吞吐量">
<title>数据摄入和数据吞吐量</title>
<simpara>数据摄取和数据出口操作允许流处理程序与外部系统通信。数据摄取是操作从外部源获取原始数据并将其转换为其他格式(ETL)。实现数据提取逻辑的运算符被称为数据源。数据源可以从TCP
Socket，文件，Kafka
Topic或传感器数据接口中提取数据。数据出口是以适合消费的形式产出到外部系统。执行数据出口的运算符称为数据接收器，包括文件，数据库，消息队列和监控接口。</simpara>
</section>
<section xml:id="_转换算子">
<title>转换算子</title>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0204.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>转换算子是单遍处理算子，碰到一个事件处理一个事件。这些操作在使用后会消费一个事件，然后对事件数据做一些转换，产生一个新的输出流。转换逻辑可以集成在
操作符中或由UDF函数提供，如图所示图2-4。程序员编写实现自定义计算逻辑。</simpara>
<simpara>操作符可以接受多个输入流并产生多个输出流。他们还可以通过修改数据流图的结构要么将流分成多个流，要么将流合并为一条流。</simpara>
</section>
<section xml:id="_滚动聚合">
<title>滚动聚合</title>
<simpara>滚动聚合是一种聚合，例如sum，minimum和maximum，为每个输入事件不断更新。
聚合操作是有状态的，并将当前状态与传入事件一起计算以产生更新的聚合值。请注意能够有效地将当前状态与事件相结合
产生单个值，聚合函数必须是关联的和可交换的。否则，操作符必须存储完整的流数据历史。图2-5显示了最小滚动
聚合。操作符保持当前的最小值和相应地为每个传入的事件来更新最小值。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0205.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
</section>
<section xml:id="_窗口操作符">
<title>窗口操作符</title>
<simpara>转换和滚动聚合一次处理一个事件产生输出事件并可能更新状态。但是，有些操作必须收集并缓冲数据以计算其结果。
例如，考虑不同流之间的连接或整体聚合这样的操作，例如中值函数。为了在无界流上高效运行这些操作符，我们需要限制
这些操作维护的数据量。在本节中，我们将讨论窗口操作，提供此服务。</simpara>
<simpara>窗口还可以在语义上实现关于流的比较复杂的查询。我们已经看到了滚动聚合的方式，以聚合值编码整个流的历史数据来为每个事件提供低延迟的结果。
但如果我们只对最近的数据感兴趣的话会怎样？考虑给司机提供实时交通信息的应用程序。这个程序可以使他们避免拥挤的路线。在这种场景下，你想知道某个位置在最近几分钟内是否有事故发生。
另一方面，了解所有发生过的事故在这个应用场景下并没有什么卵用。更重要的是，通过将流历史缩减为单一聚合值，我们将丢失这段时间内数据的变化。例如，我们可能想知道每5分钟有多少车辆穿过
某个路口。</simpara>
<simpara>窗口操作不断从无限事件流中创建有限的事件集，好让我们执行有限集的计算。通常会基于数据属性或基于时间的窗口来分配事件。
要正确定义窗口运算符语义，我们需要确定如何给窗口分配事件以及对窗口中的元素进行求值的频率是什么样的。
窗口的行为由一组策略定义。窗口策略决定何时创建新的窗口以及要分配的事件属于哪个窗口，以及何时对窗口中的元素进行求值。
而窗口的求值基于触发条件。一旦触发条件得到满足，窗口的内容将会被发送到求值函数，求值函数会将计算逻辑应用于窗口中的元素。
求值函数可以是sum或minimal或自定义的聚合函数。
求值策略可以根据时间或者数据属性计算(例如，在过去五秒内收到的事件或者最近的一百个事件等等)。
接下来，我们描述常见窗口类型的语义。</simpara>
<itemizedlist>
<listitem>
<simpara>滚动窗口是将事件分配到固定大小的不重叠的窗口中。当通过窗口的结尾时，全部事件被发送到求值函数进行处理。基于计数的滚动窗口定义了在触发求值之前需要收集多少事件。图2-6显示了一个基于计数的翻滚窗口，每四个元素一个窗口。基于时间的滚动窗口定义一个时间间隔，包含在此时间间隔内的事件。图2-7显示了基于时间的滚动窗口，将事件收集到窗口中每10分钟触发一次计算。</simpara>
</listitem>
</itemizedlist>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0206.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0207.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<itemizedlist>
<listitem>
<simpara>滑动窗口将事件分配到固定大小的重叠的窗口中去。因此，事件可能属于多个桶。我们通过提供窗口的长度和滑动距离来定义滑动窗口。滑动距离定义了创建新窗口的间隔。基于滑动计数的窗口，图2-8的长度为四个事件，三个为滑动距离。</simpara>
</listitem>
</itemizedlist>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0208.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<itemizedlist>
<listitem>
<simpara>会话窗口在常见的真实场景中很有用，一些场景既不能使用滚动窗口也不能使用滑动窗口。考虑一个分析在线用户行为的应用程序。在应用程序里，我们想把源自同一时期的用户活动或会话事件分组在一起。会话由一系列相邻时间发生的事件组成，接下来有一段时间没有活动。例如，用户在App上浏览一系列的新闻，然后关掉App，那么浏览新闻这段时间的浏览事件就是一个会话。会话窗口事先没有定义窗口的长度，而是取决于数据的实际情况，滚动窗口和滑动窗口无法应用于这个场景。相反，我们需要将同一会话中的事件分配到同一个窗口中去，而不同的会话可能窗口长度不一样。会话窗口会定义一个间隙值来区分不同的会话。间隙值的意思是：用户一段时间内不活动，就认为用户的会话结束了。图2-9显示了一个会话窗口。</simpara>
</listitem>
</itemizedlist>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0209.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>到目前为止，所有窗口类型都是在整条流上去做窗口操作。但实际上你可能想要将一条流分流成多个逻辑流并定义并行窗口。
例如，如果我们正在接收来自不同传感器的测量结果，那么可能想要在做窗口计算之前按传感器ID对流进行分流操作。
在并行窗口中，每条流都独立于其他流，然后应用了窗口逻辑。图2-10显示了一个基于计数的长度为2的并行滚动窗口，根据事件颜色分流。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0210.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>在流处理中，窗口操作与两个主要概念密切相关：时间语义和状态管理。时间也许是流处理最重要的方面。即使低延迟是流处理的一个有吸引力的特性，它的真正价值不仅仅是快速分析。真实世界的系统，网络和通信渠道远非完美，流数据经常被推迟或无序(乱序)到达。理解如何在这种条件下提供准确和确定的结果是至关重要的。
更重要的是，流处理程序可以按原样处理事件制作的也应该能够处理相同的历史事件方式，从而实现离线分析甚至时间旅行分析。
当然，前提是我们的系统可以保存状态，因为可能有故障发生。到目前为止，我们看到的所有窗口类型在产生结果前都需要保存之前的数据。实际上，如果我们想计算任何指标，即使是简单的计数，我们也需要保存状态。考虑到流处理程序可能会运行几天，几个月甚至几年，我们需要确保状态可以在发生故障的情况下可靠地恢复。
并且即使程序崩溃，我们的系统也能保证计算出准确的结果。本章，我们将在流处理应用可能发生故障的语境下，深入探讨时间和状态的概念。</simpara>
</section>
</section>
<section xml:id="_时间语义">
<title>时间语义</title>
<simpara>在本节中，我们将介绍时间语义，并描述流中不同的时间概念。我们将讨论流处理器在乱序事件流的情况下如何提供准确的计算结果，以及我们如何处理历史事件流，如何在流中进行时间旅行。</simpara>
<section xml:id="_在流处理中一分钟代表什么">
<title>在流处理中一分钟代表什么？</title>
<simpara>在处理可能是无限的事件流（包含了连续到达的事件），时间成为流处理程序的核心方面。假设我们想要连续的计算结果，可能每分钟就要计算一次。在我们的流处理程序上下文中，一分钟的意思是什么？</simpara>
<simpara>考虑一个程序需要分析一款移动端的在线游戏的用户所产生的事件流。游戏中的用户分了组，而应用程序将收集每个小组的活动数据，基于小组中的成员多快达到了游戏设定的目标，然后在游戏中提供奖励。例如额外的生命和用户升级。例如，如果一个小组中的所有用户在一分钟之内都弹出了500个泡泡，他们将升一级。Alice是一个勤奋的玩家，她在每天早晨的通勤时间玩游戏。问题在于Alice住在柏林，并且乘地铁去上班。而柏林的地铁手机信号很差。我们设想一个这样的场景，Alice当她的手机连上网时，开始弹泡泡，然后游戏会将数据发送到我们编写的应用程序中，这时地铁突然进入了隧道，她的手机也断网了。Alice还在玩这个游戏，而产生的事件将会缓存在手机中。当地铁离开隧道，Alice的手机又在线了，而手机中缓存的游戏事件将发送到应用程序。我们的应用程序应该如何处理这些数据？在这个场景中一分钟的意思是什么？这个一分钟应该包含Alice离线的那段时间吗？下图展示了这个问题。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0211.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>在线手游是一个简单的场景，展示了应用程序的运算应该取决于事件实际发生的时间，而不是应用程序收到事件的时间。如果我们按照应用程序收到事件的时间来进行处理的话，最糟糕的后果就是，Alice和她的朋友们再也不玩这个游戏了。但是还有很多时间语义非常关键的应用程序，我们需要保证时间语义的正确性。如果我们只考虑我们在一分钟之内收到了多少数据，我们的结果会变化，因为结果取决于网络连接的速度或处理的速度。相反，定义一分钟之内的事件数量，这个一分钟应该是数据本身的时间。</simpara>
<simpara>在Alice的这个例子中，流处理程序可能会碰到两个不同的时间概念：处理时间和事件时间。我们将在接下来的部分，讨论这两个概念。</simpara>
</section>
<section xml:id="_处理时间">
<title>处理时间</title>
<simpara>处理时间是处理流的应用程序的机器的本地时钟的时间（墙上时钟）。处理时间的窗口包含了一个时间段内来到机器的所有事件。这个时间段指的是机器的墙上时钟。如下图所示，在Alice的这个例子中，处理时间窗口在Alice的手机离线的情况下，时间将会继续行走。但这个处理时间窗口将不会收集Alice的手机离线时产生的事件。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0212.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
</section>
<section xml:id="_事件时间">
<title>事件时间</title>
<simpara>事件时间是流中的事件实际发生的时间。事件时间基于流中的事件所包含的时间戳。通常情况下，在事件进入流处理程序前，事件数据就已经包含了时间戳。下图展示了事件时间窗口将会正确的将事件分发到窗口中去。可以如实反应事情是怎么发生的。即使事件可能存在延迟。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0213.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>事件时间使得计算结果的过程不需要依赖处理数据的速度。基于事件时间的操作是可以预测的，而计算结果也是确定的。无论流处理程序处理流数据的速度快或是慢，无论事件到达流处理程序的速度快或是慢，事件时间窗口的计算结果都是一样的。</simpara>
<simpara>可以处理迟到的事件只是我们使用事件时间所克服的一个挑战而已。普遍存在的事件乱序问题可以使用事件时间得到解决。考虑和Alice玩同样游戏的Bob，他恰好和Alice在同一趟地铁上。Alice和Bob虽然玩的游戏一样，但他们的手机信号是不同的运营商提供的。当Alice的手机没信号时，Bob的手机依然有信号，游戏数据可以正常发送出去。</simpara>
<simpara>如果使用事件时间，即使碰到了事件乱序到达的情况，我们也可以保证结果的正确性。还有，当我们在处理可以重播的流数据时，由于时间戳的确定性，我们可以快进过去。也就是说，我们可以重播一条流，然后分析历史数据，就好像流中的事件是实时发生一样。另外，我们可以快进历史数据来使我们的应用程序追上现在的事件，然后应用程序仍然是一个实时处理程序，而且业务逻辑不需要改变。</simpara>
</section>
<section xml:id="_水位线">
<title>水位线</title>
<simpara>在我们对事件时间窗口的讨论中，我们忽略了一个很重要的方面：我们应该怎样去决定何时触发事件时间窗口的计算？也就是说，在我们可以确定一个时间点之前的所有事件都已经到达之前，我们需要等待多久？我们如何知道事件是迟到的？在分布式系统无法准确预测行为的现实条件下，以及外部组件所引发的事件的延迟，以上问题并没有准确的答案。在本小节中，我们将会看到如何使用水位线来设置事件时间窗口的行为。</simpara>
<simpara>水位线是全局进度的度量标准。系统可以确信在一个时间点之后，不会有早于这个时间点发生的事件到来了。本质上，水位线提供了一个逻辑时钟，这个逻辑时钟告诉系统当前的事件时间。当一个运算符接收到含有时间T的水位线时，这个运算符会认为早于时间T的发生的事件已经全部都到达了。对于事件时间窗口和乱序事件的处理，水位线非常重要。运算符一旦接收到水位线，运算符会认为一段时间内发生的所有事件都已经观察到，可以触发针对这段时间内所有事件的计算了。</simpara>
<simpara>水位线提供了一种结果可信度和延时之间的妥协。激进的水位线设置可以保证低延迟，但结果的准确性不够。在这种情况下，迟到的事件有可能晚于水位线到达，我们需要编写一些代码来处理迟到事件。另一方面，如果水位线设置的过于宽松，计算的结果准确性会很高，但可能会增加流处理程序不必要的延时。</simpara>
<simpara>在很多真实世界的场景里面，系统无法获得足够的知识来完美的确定水位线。在手游这个场景中，我们无法得知一个用户离线时间会有多长，他们可能正在穿越一条隧道，可能正在乘飞机，可能永远不会再玩儿了。水位线无论是用户自定义的或者是自动生成的，在一个分布式系统中追踪全局的时间进度都不是很容易。所以仅仅依靠水位线可能并不是一个很好的主意。流处理系统还需要提供一些机制来处理迟到的元素（在水位线之后到达的事件）。根据应用场景，我们可能需要把迟到事件丢弃掉，或者写到日志里，或者使用迟到事件来更新之前已经计算好的结果。</simpara>
</section>
<section xml:id="_处理时间和事件时间">
<title>处理时间和事件时间</title>
<simpara>大家可能会有疑问，既然事件时间已经可以解决我们的所有问题，为什么我们还要对比这两个时间概念？真相是，处理时间在很多情况下依然很有用。处理时间窗口将会带来理论上最低的延迟。因为我们不需要考虑迟到事件以及乱序事件，所以一个窗口只需要简单的缓存窗口内的数据即可，一旦机器时间超过指定的处理时间窗口的结束时间，就会触发窗口的计算。所以对于一些处理速度比结果准确性更重要的流处理程序，处理时间就派上用场了。另一个应用场景是，当我们需要在真实的时间场景下，周期性的报告结果时，同时不考虑结果的准确性。一个例子就是一个实时监控的仪表盘，负责显示当事件到达时立即聚合的结果。最后，处理时间窗口可以提供流本身数据的忠实表达，对于一些案例可能是很必要的特性。例如我们可能对观察流和对每分钟事件的计数（检测可能存在的停电状况）很感兴趣。简单的说，处理时间提供了低延迟，同时结果也取决于处理速度，并且也不能保证确定性。另一方面，事件时间保证了结果的确定性，同时还可以使我们能够处理迟到的或者乱序的事件流。</simpara>
</section>
</section>
<section xml:id="_状态和持久化模型">
<title>状态和持久化模型</title>
<simpara>我们现在转向另一个对于流处理程序非常重要的话题：状态。在数据处理中，状态是普遍存在的。任何稍微复杂一点的计算，都涉及到状态。为了产生计算结果，一个函数在一段时间内的一定数量的事件上来累加状态（例如，聚合计算或者模式匹配）。有状态的运算符使用输入的事件以及内部保存的状态来计算得到输出。例如，一个滚动聚合运算符需要输出这个运算符所观察到的所有事件的累加和。这个运算符将会在内部保存当前观察到的所有事件的累加和，同时每输入一个事件就更新一次累加和的计算结果。相似的，当一个运算符检测到一个<literal>高温</literal>事件紧接着十分钟以内检测到一个<literal>烟雾</literal>事件时，将会报警。直到运算符观察到一个<literal>烟雾</literal>事件或者十分钟的时间段已经过去，这个运算符需要在内部状态中一直保存着<literal>高温</literal>事件。</simpara>
<simpara>当我们考虑一下使用批处理系统来分析一个无界数据集时，会发现状态的重要性显而易见。在现代流处理器兴起之前，处理无界数据集的一个通常做法是将输入的事件攒成微批，然后交由批处理器来处理。当一个任务结束时，计算结果将被持久化，而所有的运算符状态就丢失了。一旦一个任务在计算下一个微批次的数据时，这个任务是无法访问上一个任务的状态的（都丢掉了）。这个问题通常使用将状态代理到外部系统（例如数据库）的方法来解决。相反，在一个连续不间断运行的流处理任务中，事件的状态是一直存在的，我们可以将状态暴露出来作为编程模型中的一等公民。当然，我们的确可以使用外部系统来管理流的状态，即使这个解决方案会带来额外的延迟。</simpara>
<simpara>由于流处理运算符默认处理的是无界数据流。所以我们必须要注意不要让内部状态无限的增长。为了限制状态的大小，运算符通常情况下会保存一些之前所观察到的事件流的总结或者概要。这个总结可能是一个计数值，一个累加和，或者事件流的采样，窗口的缓存操作，或者是一个自定义的数据结构，这个数据结构用来保存数据流中感兴趣的一些特性。</simpara>
<simpara>我们可以想象的到，支持有状态的运算符可能会碰到一些实现上的挑战：</simpara>
<itemizedlist>
<listitem>
<simpara>状态管理：系统需要高效的管理状态，并保证针对状态的并发更新，不会产生竞争条件（race
condition）。</simpara>
</listitem>
<listitem>
<simpara>状态分区：并行会带来复杂性。因为计算结果同时取决于已经保存的状态和输入的事件流。幸运的是，大多数情况下，我们可以使用Key来对状态进行分区，然后独立的管理每一个分区。例如，当我们处理一组传感器的测量事件流时，我们可以使用分区的运算符状态来针对不同的传感器独立的保存状态。</simpara>
</listitem>
<listitem>
<simpara>状态恢复：第三个挑战是有状态的运算符如何保证状态可以恢复，即使出现任务失败的情况，计算也是正确的。</simpara>
</listitem>
</itemizedlist>
<simpara>下一节，我们将讨论任务失败和计算结果的保证。</simpara>
<section xml:id="_任务失败">
<title>任务失败</title>
<simpara>流任务中的运算符状态是很宝贵的，也需要抵御任务失败带来的问题。如果在任务失败的情况下，状态丢失的话，在任务恢复以后计算的结果将是不正确的。流任务会连续不断的运行很长时间，而状态可能已经收集了几天甚至几个月。在失败的情况下，重新处理所有的输入并重新生成一个丢失的状态，将会很浪费时间，开销也很大。</simpara>
<simpara>在本章开始时，我们看到如何将流的编程建模成数据流模型。在执行之前，流程序将会被翻译成物理层数据流图，物理层数据流图由连接的并行任务组成，而一个并行任务运行一些运算符逻辑，消费输入流数据，并为其他任务产生输出流数据。真实场景下，可能有数百个这样的任务并行运行在很多的物理机器上。在长时间的运行中，流任务中的任意一个任务在任意时间点都有可能失败。我们如何保证任务的失败能被正确的处理，以使任务能继续的运行下去呢？事实上，我们可能希望我们的流处理器不仅能在任务失败的情况下继续处理数据，还能保证计算结果的正确性以及运算符状态的安全。我们在本小节来讨论这些问题。</simpara>
<simpara><emphasis role="strong">什么是任务失败？</emphasis></simpara>
<simpara>对于流中的每一个事件，一个处理任务分为以下步骤：（1）接收事件，并将事件存储在本地的缓存中；（2）可能会更新内部状态；（3）产生输出记录。这些步骤都能失败，而系统必须对于在失败的场景下如何处理有清晰的定义。如果任务在第一步就失败了，事件会丢失吗？如果当更新内部状态的时候任务失败，那么内部状态会在任务恢复以后更新吗？在以上这些场景中，输出是确定性的吗？</simpara>
<simpara>在批处理场景下，所有的问题都不是问题。因为我们可以很方便的重新计算。所以不会有事件丢失，状态也可以得到完全恢复。在流的世界里，处理失败不是一个小问题。流系统在失败的情况下需要保证结果的准确性。接下来，我们需要看一下现代流处理系统所提供的一些保障，以及实现这些保障的机制。</simpara>
<simpara><emphasis role="strong">结果的保证</emphasis></simpara>
<simpara>当我们讨论保证计算的结果时，我们的意思是流处理器的内部状态需要保证一致性。也就是说我们关心的是应用程序的代码在故障恢复以后看到的状态值是什么。要注意保证应用程序状态的一致性并不是保证应用程序的输出结果的一致性。一旦输出结果被持久化，结果的准确性就很难保证了。除非持久化系统支持事务。</simpara>
<simpara><emphasis>AT-MOST-ONCE</emphasis></simpara>
<simpara>当任务故障时，最简单的做法是什么都不干，既不恢复丢失的状态，也不重播丢失的事件。At-most-once语义的含义是最多处理一次事件。换句话说，事件可以被丢弃掉，也没有任何操作来保证结果的准确性。这种类型的保证也叫<literal>没有保证</literal>，因为一个丢弃掉所有事件的系统其实也提供了这样的保障。没有保障听起来是一个糟糕的主意，但如果我们能接受近似的结果，并且希望尽可能低的延迟，那么这样也挺好。</simpara>
<simpara><emphasis>AT-LEAST-ONCE</emphasis></simpara>
<simpara>在大多数的真实应用场景，我们希望不丢失事件。这种类型的保障成为at-least-once，意思是所有的事件都得到了处理，而且一些事件还可能被处理多次。如果结果的正确性仅仅依赖于数据的完整性，那么重复处理是可以接受的。例如，判断一个事件是否在流中出现过，at-least-once这样的保证完全可以正确的实现。在最坏的情况下，我们多次遇到了这个事件。而如果我们要对一个特定的事件进行计数，计算结果就可能是错误的了。</simpara>
<simpara>为了保证在at-least-once语义的保证下，计算结果也能正确。我们还需要另一套系统来从数据源或者缓存中重新播放数据。持久化的事件日志系统将会把所有的事件写入到持久化存储中。所以如果任务发生故障，这些数据可以重新播放。还有一种方法可以获得同等的效果，就是使用结果承认机制。这种方法将会把每一条数据都保存在缓存中，直到数据的处理等到所有的任务的承认。一旦得到所有任务的承认，数据将被丢弃。</simpara>
<simpara><emphasis>EXACTLY-ONCE</emphasis></simpara>
<simpara>恰好处理一次是最严格的保证，也是最难实现的。恰好处理一次语义不仅仅意味着没有事件丢失，还意味着针对每一个数据，内部状态仅仅更新一次。本质上，恰好处理一次语义意味着我们的应用程序可以提供准确的结果，就好像从未发生过故障。</simpara>
<simpara>提供恰好处理一次语义的保证必须有至少处理一次语义的保证才行，同时还需要数据重放机制。另外，流处理器还需要保证内部状态的一致性。也就是说，在故障恢复以后，流处理器应该知道一个事件有没有在状态中更新。事务更新是达到这个目标的一种方法，但可能引入很大的性能问题。Flink使用了一种轻量级快照机制来保证恰好处理一次语义。</simpara>
<simpara><emphasis>端到端恰好处理一次</emphasis></simpara>
<simpara>目前我们看到的一致性保证都是由流处理器实现的，也就是说都是在Flink流处理器内部保证的。而在真实世界中，流处理应用除了流处理器以外还包含了数据源（例如Kafka）和持久化系统。端到端的一致性保证意味着结果的正确性贯穿了整个流处理应用的始终。每一个组件都保证了它自己的一致性。而整个端到端的一致性级别取决于所有组件中一致性最弱的组件。要注意的是，我们可以通过弱一致性来实现更强的一致性语义。例如，当任务的操作具有幂等性时，比如流的最大值或者最小值的计算。在这种场景下，我们可以通过最少处理一次这样的一致性来实现恰好处理一次这样的最高级别的一致性。</simpara>
</section>
</section>
</section>
<section xml:id="_flink运行架构">
<title>Flink运行架构</title>
<section xml:id="_系统架构">
<title>系统架构</title>
<simpara>Flink是一个用于有状态的并行数据流处理的分布式系统。它由多个进程构成，这些进程一般会分布运行在不同的机器上。对于分布式系统来说，面对的常见问题有：集群中资源的分配和管理、进程协调调度、持久化和高可用的数据存储，以及故障恢复。</simpara>
<simpara>对于这些分布式系统的经典问题，业内已有比较成熟的解决方案和服务。所以Flink并不会自己去处理所有的问题，而是利用了现有的集群架构和服务，这样它就可以把精力集中在核心工作——分布式数据流处理上了。Flink与一些集群资源管理工具有很好的集成，比如Apache
Mesos、YARN和Kubernetes；同时，也可以配置为独立（stand-alone）集群运行。Flink自己并不提供持久化的分布式存储，而是直接利用了已有的分布式文件系统（比如HDFS）或者对象存储（比如S3）。对于高可用的配置，Flink需要依靠Apache
ZooKeeper来完成。</simpara>
<simpara>在本节中，我们将介绍Flink的不同组件，以及在运行程序时它们如何相互作用。我们会讨论部署Flink应用程序的两种模式，并且了解每种模式下分发和执行任务的方式。最后，我们还会解释一下Flink的高可用性模式是如何工作的。</simpara>
<section xml:id="_flink运行时组件">
<title>Flink运行时组件</title>
<simpara>Flink运行时架构主要包括四个不同的组件，它们会在运行流处理应用程序时协同工作：作业管理器（JobManager）、资源管理器（ResourceManager）、任务管理器（TaskManager），以及分发器（Dispatcher）。因为Flink是用Java和Scala实现的，所以所有组件都会运行在Java虚拟机（JVMs）上。每个组件的职责如下：</simpara>
<itemizedlist>
<listitem>
<simpara>作业管理器（JobManager）是控制一个应用程序执行的主进程，也就是说，每个应用程序都会被一个不同的作业管理器所控制执行。作业管理器会先接收到要执行的应用程序。这个应用程序会包括：作业图（JobGraph）、逻辑数据流图（logical
dataflow
graph）和打包了所有的类、库和其它资源的JAR包。作业管理器会把JobGraph转换成一个物理层面的数据流图，这个图被叫做<literal>执行图</literal>（ExecutionGraph），包含了所有可以并发执行的任务。作业管理器会向资源管理器（ResourceManager）请求执行任务必要的资源，也就是任务管理器（TaskManager）上的插槽（slot）。一旦它获取到了足够的资源，就会将执行图分发到真正运行它们的TaskManager上。而在运行过程中，作业管理器会负责所有需要中央协调的操作，比如说检查点（checkpoints）的协调。</simpara>
</listitem>
<listitem>
<simpara>ResourceManager主要负责管理任务管理器（TaskManager）的插槽（slot），TaskManger插槽是Flink中定义的处理资源单元。Flink为不同的环境和资源管理工具提供了不同资源管理器（ResourceManager），比如YARN、Mesos、K8s，以及standalone部署。当作业管理器申请插槽资源时，ResourceManager会将有空闲插槽的TaskManager分配给作业管理器。如果ResourceManager没有足够的插槽来满足作业管理器的请求，它还可以向资源提供平台发起会话，以提供启动TaskManager进程的容器。另外，ResourceManager还负责终止空闲的TaskManager，释放计算资源。</simpara>
</listitem>
<listitem>
<simpara>任务管理器（TaskManager）是Flink中的工作进程。通常在Flink中会有多个TaskManager运行，每一个TaskManager都包含了一定数量的插槽（slots）。插槽的数量限制了TaskManager能够执行的任务数量。启动之后，TaskManager会向资源管理器注册它的插槽；收到资源管理器的指令后，TaskManager就会将一个或者多个插槽提供给作业管理器调用。作业管理器就可以向插槽分配任务（tasks）来执行了。在执行过程中，一个TaskManager可以跟其它运行同一应用程序的TaskManager交换数据。任务的执行和插槽的概念会在<literal>任务执行</literal>一节做具体讨论。</simpara>
</listitem>
<listitem>
<simpara>分发器（Dispatcher）可以跨作业运行，它为应用提交提供了REST接口。当一个应用被提交执行时，分发器就会启动并将应用移交给一个作业管理器。由于是REST接口，所以Dispatcher可以作为集群的一个HTTP接入点，这样就能够不受防火墙阻挡。Dispatcher也会启动一个Web
UI，用来方便地展示和监控作业执行的信息。Dispatcher在架构中可能并不是必需的，这取决于应用提交运行的方式。</simpara>
</listitem>
</itemizedlist>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0301.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<blockquote>
<simpara>上图是从一个较为高层级的视角，来看应用中各组件的交互协作。如果部署的集群环境不同（例如YARN，Mesos，Kubernetes，standalone等），其中一些步骤可以被省略，或是有些组件会运行在同一个JVM进程中。</simpara>
</blockquote>
</section>
<section xml:id="_应用部署">
<title>应用部署</title>
<simpara>Flink应用程序可以用以下两种不同的方式部署：</simpara>
<simpara><emphasis>框架（Framework）方式</emphasis></simpara>
<simpara>在这个模式下，Flink应用被打包成一个Jar文件，并由客户端提交给一个运行服务（running
service）。这个服务可以是一个Flink的Dispatcher，也可以是一个Flink的作业管理器，或是Yarn的ResourceManager。如果application被提交给一个作业管理器，则它会立即开始执行这个application。如果application被提交给了一个Dispatcher，或是Yarn
ResourceManager，则它会启动一个作业管理器，然后将application交给它，再由作业管理器开始执行此应用。</simpara>
<simpara><emphasis>库（Library）方式</emphasis></simpara>
<simpara>在这个模式下，Flink Application 会被打包在一个容器（container）
镜像里，例如一个Docker
镜像。此镜像包含了运行作业管理器和ResourceManager的代码。当一个容器从镜像启动后，它会自动启动ResourceManager和作业管理器，并提交打包好的应用。另一种方法是：将应用打包到镜像后，只用于部署TaskManager容器。从镜像启动的容器会自动启动一个TaskManager，然后连接ResourceManager并注册它的slots。这些镜像的启动以及失败重启，通常都会由一个外部的资源管理器管理（比如Kubernetes）。</simpara>
<simpara>框架模式遵循了传统的任务提交方式，从客户端提交到Flink运行服务。而在库模式下，没有运行的Flink服务。它是将Flink作为一个库，与应用程序一同打包到了一个容器镜像。这种部署方式在微服务架构中较为常见。我们会在<literal>运行管理流式应用程序</literal>一节对这个话题做详细讨论。</simpara>
</section>
<section xml:id="_任务执行">
<title>任务执行</title>
<simpara>一个TaskManager可以同时执行多个任务（tasks）。这些任务可以是同一个算子（operator）的子任务（数据并行），也可以是来自不同算子的（任务并行），甚至可以是另一个不同应用程序的（作业并行）。TaskManager提供了一定数量的处理插槽（processing
slots），用于控制可以并行执行的任务数。一个slot可以执行应用的一个分片，也就是应用中每一个算子的一个并行任务。图3-2展示了TaskManagers，slots，tasks以及operators之间的关系：</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0302.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>最左边是一个<literal>作业图</literal>（JobGraph），包含了5个算子——它是应用程序的非并行表示。其中算子A和C是数据源（source），E是输出端（sink）。C和E并行度为2，而其他的算子并行度为4。因为最高的并行度是4，所以应用需要至少四个slot来执行任务。现在有两个TaskManager，每个又各有两个slot，所以我们的需求是满足的。作业管理器将JobGraph转化为<literal>执行图</literal>（ExecutionGraph），并将任务分配到四个可用的slot上。对于有4个并行任务的算子，它的task会分配到每个slot上。而对于并行度为2的operator
C和E，它们的任务被分配到slot 1.1、2.1 以及 slot
1.2、2.2。将tasks调度到slots上，可以让多个tasks跑在同一个TaskManager内，也就可以是的tasks之间的数据交换更高效。然而将太多任务调度到同一个TaskManager上会导致TaskManager过载，继而影响效率。之后我们会在<literal>控制任务调度</literal>一节继续讨论如何控制任务的调度。</simpara>
<simpara>TaskManager在同一个JVM中以多线程的方式执行任务。线程较进程会更轻量级，但是线程之间并没有对任务进行严格隔离。所以，单个任务的异常行为有可能会导致整个TaskManager进程挂掉，当然也同时包括运行在此进程上的所有任务。通过为每个TaskManager配置单独的slot，就可以将应用在TaskManager上相互隔离开来。TaskManager内部有多线程并行的机制，而且在一台主机上可以部署多个TaskManager，所以Flink在资源配置上非常灵活，在部署应用时可以充分权衡性能和资源的隔离。我们将会在第九章对Flink集群的配置和搭建继续做详细讨论。</simpara>
</section>
<section xml:id="_高可用配置">
<title>高可用配置</title>
<simpara>流式应用程序一般被设计为7 x
24小时运行。所以很重要的一点是：即使出现了进程挂掉的情况，应用仍需要继续保持运行。为了从故障恢复，系统首先需要重启进程、然后重启应用并恢复它的状态。接下来，我们就来了解Flink如何重启失败的进程。</simpara>
<simpara><emphasis>TaskManager故障</emphasis></simpara>
<simpara>如前所述，Flink需要足够数目的slot，来执行一个应用的所有任务。假设一个Flink环境有4个TaskManager，每个提供2个插槽，那么流应用程序执行的最高并行度为8。如果其中一个TaskManager挂掉了，那么可用的slots会降到6。在这种情况下，作业管理器会请求ResourceManager提供更多的slots。如果此请求无法满足——例如应用跑在一个独立集群——那么作业管理器在有足够的slots之前，无法重启应用。应用的重启策略决定了作业管理器的重启频率，以及两次重启尝试之间的时间间隔。</simpara>
<simpara><emphasis>作业管理器故障</emphasis></simpara>
<simpara>比TaskManager故障更严重的问题是作业管理器故障。作业管理器控制整个流应用程序的执行，并维护执行中的元数据——例如指向已完成检查点的指针。若是对应的作业管理器挂掉，则流程序无法继续运行。所以这就导致在Flink应用中，作业管理器是单点故障。为了解决这个问题，Flink提供了高可用模式。在原先的作业管理器挂掉后，可以将一个作业的状态和元数据迁移到另一个作业管理器，并继续执行。</simpara>
<simpara>Flink的高可用模式基于Apache
ZooKeeper，我们知道，ZooKeeper是用来管理需要协调和共识的分布式服务的系统。Flink主要利用ZooKeeper来进行领导者（leader）的选举，并把它作为一个高可用和持久化的数据存储。当在高可用模式下运行时，作业管理器会将JobGraph以及所有需要的元数据（例如应用程序的jar文件），写入到一个远程的持久化存储系统中。而且，作业管理器会将指向存储位置的指针，写入到ZooKeeper的数据存储中。在执行一个应用的过程中，作业管理器会接收每个独立任务检查点的状态句柄（也就是存储位置）。当一个检查点完成时（所有任务已经成功地将它们的状态写入到远程存储），
作业管理器把状态句柄写入远程存储，并将指向这个远程存储的指针写入ZooKeeper。这样，一个作业管理器挂掉之后再恢复，所需要的所有数据信息已经都保存在了远程存储，而ZooKeeper里存有指向此存储位置的指针。图3-3描述了这个设计：</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0303.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>当一个作业管理器失败，所有属于这个应用的任务都会自动取消。一个新的作业管理器接管工作，会执行以下操作：</simpara>
<itemizedlist>
<listitem>
<simpara>从ZooKeeper请求存储位置（storage
location），从远端存储获取JobGraph，Jar文件，以及应用最近一次检查点（checkpoint）的状态句柄（state
handles）</simpara>
</listitem>
<listitem>
<simpara>从ResourceManager请求slots，用来继续运行应用</simpara>
</listitem>
<listitem>
<simpara>重启应用，并将所有任务的状态，重设为最近一次已完成的检查点</simpara>
</listitem>
</itemizedlist>
<simpara>如果我们是在容器环境里运行应用（如Kubernetes），故障的作业管理器或TaskManager
容器通常会由容器服务自动重启。当运行在YARN或Mesos之上时，作业管理器或TaskManager进程会由Flink的保留进程自动触发重启。而在standalone模式下，Flink并未提供重启故障进程的工具。所以，此模式下我们可以增加备用（standby）的
作业管理器和TaskManager，用于接管故障的进程。我们将会在<literal>高可用配置</literal>一节中做进一步讨论。</simpara>
</section>
</section>
<section xml:id="_flink中的数据传输">
<title>Flink中的数据传输</title>
<section xml:id="_基于信任度的流控制">
<title>基于信任度的流控制</title>
<simpara>通过网络连接来发送每条数据的效率很低，会导致很大的开销。为了充分利用网络连接的带宽，就需要进行缓冲了。在流处理的上下文中，缓冲的一个缺点是会增加延迟，因为数据需要在缓冲区中进行收集，而不是立即发送。</simpara>
<simpara>Flink实现了一个基于信任度的流量控制机制，其工作原理如下。接收任务授予发送任务一些<literal>信任度</literal>（credit），也就是为了接收其数据而保留的网络缓冲区数。当发送者收到一个信任度通知，它就会按照被授予的信任度，发送尽可能多的缓冲数据，并且同时发送目前积压数据的大小——也就是已填满并准备发送的网络缓冲的数量。接收者用保留的缓冲区处理发来的数据，并对发送者传来的积压量进行综合考量，为其所有连接的发送者确定下一个信用度授权的优先级。</simpara>
<simpara>基于信用度的流控制可以减少延迟，因为发送者可以在接收者有足够的资源接受数据时立即发送数据。此外，在数据倾斜的情况下，这样分配网络资源是一种很有效的机制，因为信用度是根据发送者积压数据量的规模授予的。因此，基于信用的流量控制是Flink实现高吞吐量和低延迟的重要组成部分。</simpara>
</section>
<section xml:id="_任务链">
<title>任务链</title>
<simpara>Flink采用了一种称为任务链的优化技术，可以在特定条件下减少本地通信的开销。为了满足任务链的要求，必须将两个或多个算子设为相同的并行度，并通过本地转发（local
forward）的方式进行连接。图3-5所示的算子管道满足这些要求。它由三个算子组成，这些算子的任务并行度都被设为2，并且通过本地转发方式相连接。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0305.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>图3-6展示了管道以任务链方式运行的过程。算子的函数被融合成了一个单一的任务，由一个线程执行。由函数生成的数据通过一个简单的方法调用移交给下一个函数；这样在函数之间直接传递数据，基本上没有序列化和通信成本。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0306.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>任务链可以显著降低本地任务之间的通信成本，但也有一些场景，在没有链接的情况下运行管道操作是有意义的。例如，如果任务链中某个函数执行的开销巨大，那就可以将一条长的任务链管道断开，或者将一条链断开为两个任务，从而可以将这个开销大的函数调度到不同的槽（slots）中。图3-7显示了在没有任务链的情况下相同管道操作的执行情况。所有函数都由独立的单个任务来评估，每个任务都在专有的线程中运行。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0307.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>任务链在Flink中默认会启用。在<literal>控制任务链</literal>一节中，我们展示了如何禁用应用程序的任务链，以及如何控制各个算子的链接行为。</simpara>
</section>
</section>
<section xml:id="_事件时间处理">
<title>事件时间处理</title>
<simpara>在<literal>时间语义</literal>一节，我们重点强调了时间语义在流处理应用中的重要性，并且解释了处理时间（processing
time）和事件时间（event
time）的不同。处理时间比较好理解，因为它是基于处理器本地时间的；但同时，它会带来比较混乱、不一致、并且不可重现的结果。相比之下，事件时间语义能够产生可重现且一致的结果，这也是许多流处理场景希望解决的一大难题。但是，与处理时间应用程序相比，事件时间应用程序会更复杂，需要额外的配置。另外，支持事件时间的流处理器，也比纯粹在处理时间中运行的系统内部更为复杂。</simpara>
<simpara>Flink为常见的事件时间处理操作提供了直观且易于使用的原语，同时暴露了表达性很强的API，用户可以使用自定义算子实现更高级的事件时间应用程序。很好地理解Flink的内部时间处理，对于实现这样的高级应用程序会有很大帮助，有时也是必需的。上一章介绍了Flink利用两个概念来支持事件时间语义：记录时间戳（timestamps）和水位线（watermarks）。接下来，我们将描述Flink如何在内部实现并处理时间戳和水位线，进而支持具有事件时间语义的流式应用程序。</simpara>
<section xml:id="_时间戳">
<title>时间戳</title>
<simpara>由Flink事件时间流应用程序处理的所有记录都必须伴有时间戳。时间戳将数据与特定时间点相关联，通常就是数据所表示的事件发生的时间点。而只要时间戳大致跟数据流保持一致，基本上随着数据流的前进而增大，应用程序就可以自由选择时间戳的含义。不过正如<literal>时间语义</literal>一节中所讨论的，在现实场景中，时间戳基本上都是乱序的，所以采用<literal>事件时间</literal>而非<literal>处理事件</literal>往往会显得更为重要。</simpara>
<simpara>当Flink以事件时间模式处理数据流时，它会根据数据记录的时间戳来处理基于时间的算子。例如，时间窗口算子根据相关时间戳将数据分配给不同的时间窗口。Flink将时间戳编码为16字节的长整型值，并将其作为元数据附加到数据记录中。它的内置运算符会将这个长整型值解释为一个具有毫秒精度的Unix时间戳，也就是1970-01-01-00:00:00.000以来的毫秒数。当然，如果用户进行了自定义，那么运算符可以有自己的解释，例如，可以将精度调整到微秒。</simpara>
</section>
<section xml:id="_水位线_2">
<title>水位线</title>
<simpara>除了时间戳，基于事件时间的Flink应用程序还必须支持水位线（watermark）。在基于事件时间的应用中，水位线用于生成每个任务的当前事件时间。基于时间的算子使用这个<literal>当前事件时间</literal>来触发计算和处理操作。例如，一个时间窗口任务（time-window
task）会在任务的事件时间超出窗口的关闭边界时，完成窗口计算，并输出计算结果。</simpara>
<simpara>在Flink中，水位线被实现为一条特殊的数据记录，它里面以长整型值保存了一个时间戳。水位线在带有时间戳的数据流中，跟随着其它数据一起流动，如图3-8所示。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0308.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>水位线有两个基本属性：</simpara>
<itemizedlist>
<listitem>
<simpara>必须单调递增，以确保任务的事件时间时钟在向前推进，而不是在后退。</simpara>
</listitem>
<listitem>
<simpara>它们与数据的时间戳相关。带有时间戳T的水位线表示，所有后续数据的时间戳都应该大于T。</simpara>
</listitem>
</itemizedlist>
<simpara>上面的第二个属性用于处理带有乱序时间戳的数据流，比如图3-8中时间戳3和5的数据。基于时间的算子任务会收集和处理数据（这些数据可能具有乱序的时间戳），并在事件时间时钟到达某个时刻时完成计算。这个时刻就表示数据收集的截止，具有之前时间戳的数据应该都已经到达、不再需要了；而其中的事件时间时钟，正是由当前接收到的水位线来指示的。如果任务再接收到的数据违反了watermark的这一属性，也就是时间戳小于以前接收到的水位线时，它所属的那部分计算可能已经完成了。这种数据被称为延迟数据（late
records）。Flink提供了处理延迟数据的不同方式，我们会在<literal>处理延迟数据</literal>一节中讨论。</simpara>
<simpara>水位线还有一个很有趣的特性，它允许应用程序自己来平衡结果的完整性和延迟。如果水位线与数据的时间戳非常接近，那么我们可以得到较低的处理延迟，因为任务在完成计算之前只会短暂地等待更多数据到达。而同时，结果的完整性可能会受到影响，因为相关数据可能因为迟到而被视为<literal>延迟数据</literal>，这样就不会包含在结果中。相反，非常保守的水位线提供了足够的时间去等待所有数据到达，这样会增加处理延迟，但提高了结果的完整性。</simpara>
</section>
<section xml:id="_watermark的传递和事件时间">
<title>watermark的传递和事件时间</title>
<simpara>在本节中，我们将讨论算子如何处理水位线。Flink把watermark作为一条特殊的数据来实现，它也会由算子任务接收和发送。任务会有一个内部的时间服务，它会维护定时器，并在收到watermark时触发。任务可以在计时器服务中注册定时器，以便在将来特定的时间点执行计算。例如，窗口算子为每个活动窗口注册一个定时器，当事件时间超过窗口的结束时间时，该计时器将清除窗口的状态。</simpara>
<simpara>当任务收到watermark时，将执行以下操作：</simpara>
<itemizedlist>
<listitem>
<simpara>任务根据watermark的时间戳更新其内部事件时钟。</simpara>
</listitem>
<listitem>
<simpara>任务的时间服务会将所有过期的计时器标识出来，它们的时间小于当前的事件时间。对于每个过期的计时器，任务调用一个回调函数，该函数可以执行计算并发送结果。</simpara>
</listitem>
<listitem>
<simpara>任务会发出一个带有更新后的事件时间的watermark。</simpara>
</listitem>
</itemizedlist>
<blockquote>
<simpara>Flink限制通过DataStream
API访问时间戳和watermark。函数不能读取或修改数据的时间戳和watermark，但底层的<literal>处理函数</literal>（process
functions）除外，它们可以读取当前处理数据的时间戳、请求算子的当前事件时间，还可以注册定时器。通常的函数都不会暴露这些可以设置时间戳、操作任务事件时间时钟、或者发出水位线的API。而基于时间的数据流算子任务则会配置发送出的数据的时间戳，以确保它们能够与已到达的水位线平齐。例如，窗口计算完成后，时间窗口的算子任务会将窗口的结束时间作为时间戳附加到将要发送出的结果数据上，然后再使用触发窗口计算的时间戳发出watermark。</simpara>
</blockquote>
<simpara>现在，让我们更详细地解释一下任务在接收到新的watermark时，如何继续发送watermark并更新其事件时钟。正如我们在<literal>数据并发和任务并发</literal>中所了解的，Flink将数据流拆分为多个分区，并通过单独的算子任务并行地处理每个分区。每个分区都是一个流，里面包含了带着时间戳的数据和watermark。一个算子与它前置或后续算子的连接方式有多种情况，所以它对应的任务可以从一个或多个<literal>输入分区</literal>接收数据和watermark，同时也可以将数据和watermark发送到一个或多个<literal>输出分区</literal>。接下来，我们将详细描述一个任务如何向多个输出任务发送watermark，以及如何通过接收到的watermark来驱动事件时间时钟前进。</simpara>
<simpara>任务为每个输入分区维护一个分区水位线（watermark）。当从一个分区接收到watermark时，它会比较新接收到的值和当前水位值，然后将相应的分区watermark更新为两者的最大值。然后，任务会比较所有分区watermark的大小，将其事件时钟更新为所有分区watermark的最小值。如果事件时间时钟前进了，任务就将处理所有被触发的定时器操作，并向所有连接的输出分区发送出相应的watermark，最终将新的事件时间广播给所有下游任务。</simpara>
<simpara>图3-9显示了具有四个输入分区和三个输出分区的任务如何接收watermark、更新分区watermark和事件时间时钟，以及向下游发出watermark。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0309.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>具有两个或多个输入流（如Union或CoFlatMap）的算子任务（参见<literal>多流转换</literal>一节）也会以所有分区watermark的最小值作为事件时间时钟。它们并不区分不同输入流的分区watermark，所以两个输入流的数据都是基于相同的事件时间时钟进行处理的。当然我们可以想到，如果应用程序的各个输入流的事件时间不一致，那么这种处理方式可能会导致问题。</simpara>
<simpara>Flink的水位处理和传递算法，确保了算子任务发出的时间戳和watermark是<literal>对齐</literal>的。不过它依赖一个条件，那就是所有分区都会提供不断增长的watermark。一旦一个分区不再推进水位线的上升，或者完全处于空闲状态、不再发送任何数据和watermark，任务的事件时间时钟就将停滞不前，任务的定时器也就无法触发了。对于基于时间的算子来说，它们需要依赖时钟的推进来执行计算和清除状态，这种情况显然就会有问题。如果任务没有定期从所有输入任务接收到新的watermark，那么基于时间的算子的处理延迟和状态空间的大小都会显著增加。</simpara>
<simpara>对于具有两个输入流而且watermark明显不同的算子，也会出现类似的情况。具有两个输入流的任务的事件时间时钟，将会同较慢的那条流的watermark保持一致，而通常较快流的数据或者中间结果会在state中缓冲，直到事件时间时钟达到这条流的watermark，才会允许处理它们。</simpara>
</section>
<section xml:id="_时间戳的分配和水位线的产生">
<title>时间戳的分配和水位线的产生</title>
<simpara>我们已经解释了什么是时间戳和水位线，以及它们是如何由Flink内部处理的；然而我们还没有讨论它们的产生。流应用程序接收到数据流时，通常就会先分配时间戳并生成水位线（watermark）。因为时间戳的选择是由不同的应用程序决定的，而且watermark取决于时间戳和流的特性，所以应用程序必须首先显式地分配时间戳并生成watermark。Flink流应用程序可以通过三种方式分配时间戳和生成watermark：</simpara>
<itemizedlist>
<listitem>
<simpara>在数据源（source）处分配：当数据流被摄入到应用程序中时，可以由<literal>源函数</literal>SourceFunction分配和生成时间戳和watermark。SourceFunction可以产生并发送一个数据流；数据会与相关的时间戳一起发送出去，而watermark可以作为一条特殊数据在任何时间点发出。如果SourceFunction（暂时）不再发出watermark，它可以声明自己处于<literal>空闲</literal>（idle）状态。Flink会在后续算子的水位计算中，把空闲的SourceFunction产生的流分区排除掉。source的这一空闲机制，可以用来解决前面提到的水位不再上升的问题。源函数（Source
Function）在<literal>实现自定义源函数</literal>一节中进行了更详细的讨论。</simpara>
</listitem>
<listitem>
<simpara>定期分配：在Flink中，DataStream
API提供一个名为AssignerWithPeriodicWatermarks的用户定义函数，它可以从每个数据中提取时间戳，并被定期调用以生成当前watermark。提取出的时间戳被分配给相应的数据，而生成的watermark也会添加到流中。这个函数将在<literal>分配时间戳和生成水位线</literal>一节中讨论。</simpara>
</listitem>
<listitem>
<simpara>间断分配：AssignerWithPunctuatedWatermarks是另一个用户定义的函数，它同样会从每个数据中提取一个时间戳。它可以用于生成特殊输入数据中的watermark。与AssignerWithPeriodicWatermarks相比，此函数可以（但不是必须）从每个记录中提取watermark。我们在<literal>分配时间戳和生成水位线</literal>一节中同样讨论了该函数。</simpara>
</listitem>
</itemizedlist>
<simpara>用户定义的时间戳分配函数并没有严格的限制，通常会放在尽可能靠近source算子的位置，因为当经过一些算子处理后，数据及其时间戳的顺序就更加难以解释了。所以尽管我们可以在流应用程序的中段覆盖已有的时间戳和watermark——Flink通过用户定义的函数提供了这种灵活性，但这显然并不是推荐的做法。</simpara>
</section>
</section>
<section xml:id="_状态管理">
<title>状态管理</title>
<simpara>在第2章中，我们已经知道大多数流应用程序都是有状态的。许多算子会不断地读取和更新状态，例如在窗口中收集的数据、读取输入源的位置，或者像机器学习模型那样的用户定制化的算子状态。
Flink用同样的方式处理所有的状态，无论是内置的还是用户自定义的算子。本节我们将会讨论Flink支持的不同类型的状态，并解释<literal>状态后端</literal>是如何存储和维护状态的。</simpara>
<simpara>一般来说，由一个任务维护，并且用来计算某个结果的所有数据，都属于这个任务的状态。你可以认为状态就是一个本地变量，可以被任务的业务逻辑访问。图3-10显示了任务与其状态之间的交互。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0310.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>任务会接收一些输入数据。在处理数据时，任务可以读取和更新状态，并根据输入数据和状态计算结果。最简单的例子，就是统计接收到多少条数据的任务。当任务收到新数据时，它会访问状态以获取当前的计数，然后让计数递增，更新状态并发送出新的计数。</simpara>
<simpara>应用程序里，读取和写入状态的逻辑一般都很简单直接，而有效可靠的状态管理会复杂一些。这包括如何处理很大的状态——可能会超过内存，并且保证在发生故障时不会丢失任何状态。幸运的是，Flink会帮我们处理这相关的所有问题，包括状态一致性、故障处理以及高效存储和访问，以便开发人员可以专注于应用程序的逻辑。</simpara>
<simpara>在Flink中，状态始终与特定算子相关联。为了使运行时的Flink了解算子的状态，算子需要预先注册其状态。总的说来，有两种类型的状态：算子状态（operator
state）和键控状态（keyed
state），它们有着不同的范围访问，我们将在下面展开讨论。</simpara>
<section xml:id="_算子状态">
<title>算子状态</title>
<simpara>算子状态的作用范围限定为算子任务。这意味着由同一并行任务所处理的所有数据都可以访问到相同的状态，状态对于同一任务而言是共享的。算子状态不能由相同或不同算子的另一个任务访问。图3-11显示了任务如何访问算子状态。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0311.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>Flink为算子状态提供三种基本数据结构：</simpara>
<itemizedlist>
<listitem>
<simpara>列表状态：将状态表示为一组数据的列表。</simpara>
</listitem>
<listitem>
<simpara>联合列表状态：也将状态表示为数据的列表。它与常规列表状态的区别在于，在发生故障时，或者从保存点（savepoint）启动应用程序时如何恢复。我们将在后面继续讨论。</simpara>
</listitem>
<listitem>
<simpara>广播状态：如果一个算子有多项任务，而它的每项任务状态又都相同，那么这种特殊情况最适合应用广播状态。在保存检查点和重新调整算子并行度时，会用到这个特性。这两部分内容将在本章后面讨论。</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="_键控状态">
<title>键控状态</title>
<simpara>顾名思义，键控状态是根据输入数据流中定义的键（key）来维护和访问的。Flink为每个键值维护一个状态实例，并将具有相同键的所有数据，都分区到同一个算子任务中，这个任务会维护和处理这个key对应的状态。当任务处理一条数据时，它会自动将状态的访问范围限定为当前数据的key。因此，具有相同key的所有数据都会访问相同的状态。图3-12显示了任务如何与键控状态进行交互。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0312.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>我们可以将键控状态看成是在算子所有并行任务上，对键进行分区（或分片）之后的一个键值映射（key-value
map）。Flink为键控状态提供不同的数据结构，用于确定map中每个key存储的值的类型。我们简单了解一下最常见的键控状态。</simpara>
<itemizedlist>
<listitem>
<simpara>值状态：为每个键存储一个任意类型的单个值。复杂数据结构也可以存储为值状态。</simpara>
</listitem>
<listitem>
<simpara>列表状态：为每个键存储一个值的列表。列表里的每个数据可以是任意类型。</simpara>
</listitem>
<listitem>
<simpara>映射状态：为每个键存储一个键值映射（map）。map的key和value可以是任意类型。</simpara>
</listitem>
</itemizedlist>
<simpara>状态的数据结构可以让Flink实现更有效的状态访问。我们将在<literal>在运行时上下文（RuntimeContext）中声明键控状态</literal>中做进一步讨论。</simpara>
</section>
<section xml:id="_状态后端">
<title>状态后端</title>
<simpara>每传入一条数据，有状态的算子任务都会读取和更新状态。由于有效的状态访问对于处理数据的低延迟至关重要，因此每个并行任务都会在本地维护其状态，以确保快速的状态访问。状态到底是如何被存储、访问以及维护的？这件事由一个可插入的组件决定，这个组件就叫做状态后端（state
backend）。状态后端主要负责两件事：本地的状态管理，以及将检查点（checkpoint）状态写入远程存储。</simpara>
<simpara>对于本地状态管理，状态后端会存储所有键控状态，并确保所有的访问都被正确地限定在当前键范围。Flink提供了默认的状态后端，会将键控状态作为内存中的对象进行管理，将它们存储在JVM堆上。另一种状态后端则会把状态对象进行序列化，并将它们放入RocksDB中，然后写入本地硬盘。第一种方式可以提供非常快速的状态访问，但它受内存大小的限制；而访问RocksDB状态后端存储的状态速度会较慢，但其状态可以增长到非常大。</simpara>
<simpara>状态检查点的写入也非常重要，这是因为Flink是一个分布式系统，而状态只能在本地维护。TaskManager进程（所有任务在其上运行）可能在任何时间点挂掉。因此，它的本地存储只能被认为是不稳定的。状态后端负责将任务的状态检查点写入远程的持久存储。写入检查点的远程存储可以是分布式文件系统，也可以是数据库。不同的状态后端在状态检查点的写入机制方面有所不同。例如，RocksDB状态后端支持增量的检查点，这对于非常大的状态来说，可以显著减少状态检查点写入的开销。</simpara>
<simpara>我们将在<literal>选择状态后端</literal>一节中更详细地讨论不同的状态后端及其优缺点。</simpara>
</section>
<section xml:id="_调整有状态算子的并行度">
<title>调整有状态算子的并行度</title>
<simpara>流应用程序的一个常见要求是，为了增大或较小输入数据的速率，需要灵活地调整算子的并行度。对于无状态算子而言，并行度的调整没有任何问题，但更改有状态算子的并行度显然就没那么简单了，因为它们的状态需要重新分区并分配给更多或更少的并行任务。
Flink支持四种模式来调整不同类型的状态。</simpara>
<simpara>具有键控状态的算子通过将键重新分区为更少或更多任务来缩放并行度。不过，并行度调整时任务之间会有一些必要的状态转移。为了提高效率，Flink并不会对单独的key做重新分配，而是用所谓的``键组’’（key
group）把键管理起来。键组是key的分区形式，同时也是Flink为任务分配key的方式。图3-13显示了如何在键组中重新分配键控状态。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0313.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>具有算子列表状态的算子，会通过重新分配列表中的数据项目来进行并行度缩放。从概念上讲，所有并行算子任务的列表项目会被收集起来，并将其均匀地重新分配给更少或更多的任务。如果列表条目少于算子的新并行度，则某些任务将以空状态开始。图3-14显示了算子列表状态的重新分配。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0314.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>具有算子联合列表状态的算子，会通过向每个任务广播状态的完整列表，来进行并行度的缩放。然后，任务可以选择要使用的状态项和要丢弃的状态项。图3-15显示了如何重新分配算子联合列表状态。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0315.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>具有算子广播状态的算子，通过将状态复制到新任务，来增大任务的并行度。这是没问题的，因为广播状态保证了所有任务都具有相同的状态。而对于缩小并行度的情况，我们可以直接取消剩余任务，因为状态是相同的，已经被复制并且不会丢失。图3-16显示了算子广播状态的重新分配。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0316.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
</section>
</section>
<section xml:id="_检查点保存点和状态恢复">
<title>检查点，保存点和状态恢复</title>
<simpara>Flink是一个分布式数据处理系统，因此必须有一套机制处理各种故障，比如被杀掉的进程，故障的机器和中断的网络连接。任务都是在本地维护状态的，所以Flink必须确保状态不会丢失，并且在发生故障时能够保持一致。</simpara>
<simpara>在本节中，我们将介绍Flink的检查点（checkpoint）和恢复机制，这保证了<literal>精确一次</literal>（exactly-once）的状态一致性。我们还会讨论Flink独特的保存点（savepoint）功能，这是一个<literal>瑞士军刀</literal>式的工具，可以解决许多操作数据流时面对的问题。</simpara>
<section xml:id="_一致的检查点">
<title>一致的检查点</title>
<simpara>Flink的恢复机制的核心，就是应用状态的一致检查点。有状态流应用的一致检查点，其实就是所有任务状态在某个时间点的一份拷贝，而这个时间点应该是所有任务都恰好处理完一个相同的输入数据的时候。这个过程可以通过一致检查点的一个简单算法步骤来解释。这个算法的步骤是：</simpara>
<itemizedlist>
<listitem>
<simpara>暂停所有输入流的摄取，也就是不再接收新数据的输入。</simpara>
</listitem>
<listitem>
<simpara>等待所有正在处理的数据计算完毕，这意味着结束时，所有任务都已经处理了所有输入数据。</simpara>
</listitem>
<listitem>
<simpara>通过将每个任务的状态复制到远程持久存储，来得到一个检查点。所有任务完成拷贝操作后，检查点就完成了。</simpara>
</listitem>
<listitem>
<simpara>恢复所有输入流的摄取。</simpara>
</listitem>
</itemizedlist>
<simpara>需要注意，Flink实现的并不是这种简单的机制。我们将在本节后面介绍Flink更精妙的检查点算法。</simpara>
<simpara>图3-17显示了一个简单应用中的一致检查点。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0317.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>上面的应用程序中具有单一的输入源（source）任务，输入数据就是一组不断增长的数字的流——1,2,3等。数字流被划分为偶数流和奇数流。求和算子（sum）的两个任务会分别实时计算当前所有偶数和奇数的总和。源任务会将其输入流的当前偏移量存储为状态，而求和任务则将当前的总和值存储为状态。在图3-17中，Flink在输入偏移量为5时，将检查点写入了远程存储，当前的总和为6和9。</simpara>
</section>
<section xml:id="_从一致检查点中恢复状态">
<title>从一致检查点中恢复状态</title>
<simpara>在执行流应用程序期间，Flink会定期检查状态的一致检查点。如果发生故障，Flink将会使用最近的检查点来一致恢复应用程序的状态，并重新启动处理流程。图3-18显示了恢复过程。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0318.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>应用程序从检查点的恢复分为三步：</simpara>
<itemizedlist>
<listitem>
<simpara>重新启动整个应用程序。</simpara>
</listitem>
<listitem>
<simpara>将所有的有状态任务的状态重置为最近一次的检查点。</simpara>
</listitem>
<listitem>
<simpara>恢复所有任务的处理。</simpara>
</listitem>
</itemizedlist>
<simpara>这种检查点的保存和恢复机制可以为应用程序状态提供<literal>精确一次</literal>（exactly-once）的一致性，因为所有算子都会保存检查点并恢复其所有状态，这样一来所有的输入流就都会被重置到检查点完成时的位置。至于数据源是否可以重置它的输入流，这取决于其实现方式和消费流数据的外部接口。例如，像Apache
Kafka这样的事件日志系统可以提供流上之前偏移位置的数据，所以我们可以将源重置到之前的偏移量，重新消费数据。而从套接字（socket）消费数据的流就不能被重置了，因为套接字的数据一旦被消费就会丢弃掉。因此，对于应用程序而言，只有当所有的输入流消费的都是可重置的数据源时，才能确保在<literal>精确一次</literal>的状态一致性下运行。</simpara>
<simpara>从检查点重新启动应用程序后，其内部状态与检查点完成时的状态完全相同。然后它就会开始消费并处理检查点和发生故障之间的所有数据。尽管这意味着Flink会对一些数据处理两次（在故障之前和之后），我们仍然可以说这个机制实现了精确一次的一致性语义，因为所有算子的状态都已被重置，而重置后的状态下还不曾看到这些数据。</simpara>
<simpara>我们必须指出，Flink的检查点保存和恢复机制仅仅可以重置流应用程序的内部状态。对于应用中的一些的输出（sink）算子，在恢复期间，某些结果数据可能会多次发送到下游系统，比如事件日志、文件系统或数据库。对于某些存储系统，Flink提供了具有精确一次输出功能的sink函数，比如，可以在检查点完成时提交发出的记录。另一种适用于许多存储系统的方法是幂等更新。在<literal>应用程序一致性保证</literal>一节中，我们还会详细讨论如何解决应用程序端到端的精确一次一致性问题。</simpara>
</section>
<section xml:id="_flink的检查点算法">
<title>Flink的检查点算法</title>
<simpara>Flink的恢复机制，基于它的一致性检查点。前面我们已经了解了从流应用中创建检查点的简单方法——先暂停应用，保存检查点，然后再恢复应用程序，这种方法很好理解，但它的理念是<literal>停止一切</literal>，这对于即使是中等延迟要求的应用程序而言也是不实用的。所以Flink没有这么简单粗暴，而是基于Chandy-Lamport算法实现了分布式快照的检查点保存。该算法并不会暂停整个应用程序，而是将检查点的保存与数据处理分离，这样就可以实现在其它任务做检查点状态保存状态时，让某些任务继续进行而不受影响。接下来我们将解释此算法的工作原理。</simpara>
<simpara>Flink的检查点算法用到了一种称为<literal>检查点分界线</literal>（checkpoint
barrier）的特殊数据形式。与水位线（watermark）类似，检查点分界线由source算子注入到常规的数据流中，它的位置是限定好的，不能超过其他数据，也不能被后面的数据超过。检查点分界线带有检查点ID，用来标识它所属的检查点；这样，这个分界线就将一条流逻辑上分成了两部分。分界线之前到来的数据导致的状态更改，都会被包含在当前分界线所属的检查点中；而基于分界线之后的数据导致的所有更改，就会被包含在之后的检查点中。</simpara>
<simpara>我们用一个简单的流应用程序作为示例，来一步一步解释这个算法。该应用程序有两个源（source）任务，每个任务都消费一个增长的数字流。源任务的输出被划分为两部分：偶数和奇数的流。每个分区由一个任务处理，该任务计算所有收到的数字的总和，并将更新的总和转发给输出（sink）任务。这个应用程序的结构如图3-19所示。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0319.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>作业管理器会向每个数据源（source）任务发送一条带有新检查点ID的消息，通过这种方式来启动检查点，如图3-20所示。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0320.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>当source任务收到消息时，它会暂停发出新的数据，在状态后端触发本地状态的检查点保存，并向所有传出的流分区广播带着检查点ID的分界线（barriers）。状态后端在状态检查点完成后会通知任务，而任务会向作业管理器确认检查点完成。在发出所有分界线后，source任务就可以继续常规操作，发出新的数据了。通过将分界线注入到输出流中，源函数（source
function）定义了检查点在流中所处的位置。图3-21显示了两个源任务将本地状态保存到检查点，并发出检查点分界线之后的流应用程序。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0321.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>源任务发出的检查点分界线（barrier），将被传递给所连接的任务。与水位线（watermark）类似，barrier会被广播到所有连接的并行任务，以确保每个任务从它的每个输入流中都能接收到。当任务收到一个新检查点的barrier时，它会等待这个检查点的所有输入分区的barrier到达。在等待的过程中，任务并不会闲着，而是会继续处理尚未提供barrier的流分区中的数据。对于那些barrier已经到达的分区，如果继续有新的数据到达，它们就不会被立即处理，而是先缓存起来。这个等待所有分界线到达的过程，称为``分界线对齐’’（barrier
alignment），如图3-22所示。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0322.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>当任务从所有输入分区都收到barrier时，它就会在状态后端启动一个检查点的保存，并继续向所有下游连接的任务广播检查点分界线，如图3-23所示。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0323.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>所有的检查点barrier都发出后，任务就开始处理之前缓冲的数据。在处理并发出所有缓冲数据之后，任务就可以继续正常处理输入流了。图3-24显示了此时的应用程序。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0324.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>最终，检查点分界线会到达输出（sink）任务。当sink任务接收到barrier时，它也会先执行``分界线对齐’’，然后将自己的状态保存到检查点，并向作业管理器确认已接收到barrier。一旦从应用程序的所有任务收到一个检查点的确认信息，作业管理器就会将这个检查点记录为已完成。图3-25显示了检查点算法的最后一步。这样，当发生故障时，我们就可以用已完成的检查点恢复应用程序了。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0325.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
</section>
<section xml:id="_检查点的性能影响">
<title>检查点的性能影响</title>
<simpara>Flink的检查点算法可以在不停止整个应用程序的情况下，生成一致的分布式检查点。但是，它可能会增加应用程序的处理延迟。Flink对此有一些调整措施，可以在某些场景下显得对性能的影响没那么大。</simpara>
<simpara>当任务将其状态保存到检查点时，它其实处于一个阻塞状态，而此时新的输入会被缓存起来。由于状态可能变得非常大，而且检查点需要通过网络将数据写入远程存储系统，检查点的写入很容易就会花费几秒到几分钟的时间——这对于要求低延迟的应用程序而言，显然是不可接受的。在Flink的设计中，真正负责执行检查点写入的，其实是状态后端。具体怎样复制任务的状态，取决于状态后端的实现方式。例如，文件系统（FileSystem）状态后端和RocksDB状态后端都支持了异步（asynchronous）检查点。触发检查点操作时，状态后端会先创建状态的本地副本。本地拷贝完成后，任务就将继续常规的数据处理，这往往并不会花费太多时间。一个后台线程会将本地快照异步复制到远程存储，并在完成检查点后再回来通知任务。异步检查点的机制，显著减少了任务继续处理数据之前的等待时间。此外，RocksDB状态后端还实现了增量的检查点，这样可以大大减少要传输的数据量。</simpara>
<simpara>为了减少检查点算法对处理延迟的影响，另一种技术是调整分界线对齐的步骤。对于需要非常低的延迟、并且可以容忍<literal>至少一次</literal>（at-least-once）状态保证的应用程序，Flink可以将检查点算法配置为，在等待barrier对齐期间处理所有到达的数据，而不是把barrier已经到达的那些分区的数据缓存起来。当检查点的所有barrier到达，算子任务就会将状态写入检查点——当然，现在的状态中，就可能包括了一些<literal>提前</literal>的更改，这些更改由本该属于下一个检查点的数据到来时触发。如果发生故障，从检查点恢复时，就将再次处理这些数据：这意味着检查点现在提供的是<literal>至少一次</literal>（at-least-once）而不是<literal>精确一次</literal>（exactly-once）的一致性保证。</simpara>
</section>
<section xml:id="_保存点">
<title>保存点</title>
<simpara>Flink的恢复算法是基于状态检查点的。Flink根据可配置的策略，定期保存并自动丢弃检查点。检查点的目的是确保在发生故障时可以重新启动应用程序，所以当应用程序被显式地撤销（cancel）时，检查点会被删除掉。除此之外，应用程序状态的一致性快照还可用于除故障恢复之外的更多功能。</simpara>
<simpara>Flink中一个最有价值，也是最独特的功能是保存点（savepoints）。原则上，创建保存点使用的算法与检查点完全相同，因此保存点可以认为就是具有一些额外元数据的检查点。Flink不会自动创建保存点，因此用户（或者外部调度程序）必须明确地触发创建操作。同样，Flink也不会自动清理保存点。第10章将会具体介绍如何触发和处理保存点。</simpara>
<simpara><emphasis role="strong">使用保存点</emphasis></simpara>
<simpara>有了应用程序和与之兼容的保存点，我们就可以从保存点启动应用程序了。这会将应用程序的状态初始化为保存点的状态，并从保存点创建时的状态开始运行应用程序。虽然看起来这种行为似乎与用检查点从故障中恢复应用程序完全相同，但实际上故障恢复只是一种特殊情况，它只是在相同的集群上以相同的配置启动相同的应用程序。而从保存点启动应用程序会更加灵活，这就可以让我们做更多事情了。</simpara>
<itemizedlist>
<listitem>
<simpara>可以从保存点启动不同但兼容的应用程序。这样一来，我们就可以及时修复应用程序中的逻辑bug，并让流式应用的源尽可能多地提供之前发生的事件，然后重新处理，以便修复之前的计算结果。修改后的应用程序还可用于运行A
/
B测试，或者具有不同业务逻辑的假设场景。这里要注意，应用程序和保存点必须兼容才可以这么做——也就是说，应用程序必须能够加载保存点的状态。</simpara>
</listitem>
<listitem>
<simpara>可以使用不同的并行度来启动相同的应用程序，可以将应用程序的并行度增大或减小。</simpara>
</listitem>
<listitem>
<simpara>可以在不同的集群上启动同样的应用程序。这非常有意义，意味着我们可以将应用程序迁移到较新的Flink版本或不同的集群上去。</simpara>
</listitem>
<listitem>
<simpara>可以使用保存点暂停应用程序，稍后再恢复。这样做的意义在于，可以为更高优先级的应用程序释放集群资源，或者在输入数据不连续生成时释放集群资源。</simpara>
</listitem>
<listitem>
<simpara>还可以将保存点设置为某一版本，并归档（archive）存储应用程序的状态。</simpara>
</listitem>
</itemizedlist>
<simpara>保存点是非常强大的功能，所以许多用户会定期创建保存点以便能够及时退回之前的状态。我们见到的各种场景中，保存点一个最有趣的应用是不断将流应用程序迁移到更便宜的数据中心上去。</simpara>
<simpara><emphasis role="strong">从保存点启动应用程序</emphasis></simpara>
<simpara>前面提到的保存点的所有用例，都遵循相同的模式。那就是首先创建正在运行的应用程序的保存点，然后在一个新启动的应用程序中用它来恢复状态。之前我们已经知道，保存点的创建和检查点非常相似，而接下来我们就将介绍对于一个从保存点启动的应用程序，Flink如何初始化其状态。</simpara>
<simpara>应用程序由多个算子组成。每个算子可以定义一个或多个键控状态和算子状态。算子由一个或多个算子任务并行执行。因此，一个典型的应用程序会包含多个状态，这些状态分布在多个算子任务中，这些任务可以运行在不同的TaskManager进程上。</simpara>
<simpara>图3-26显示了一个具有三个算子的应用程序，每个算子执行两个算子任务。一个算子（OP-1）具有单一的算子状态（OS-1），而另一个算子（OP-2）具有两个键控状态（KS-1和KS-2）。当保存点创建时，会将所有任务的状态复制到持久化的存储位置。</simpara>
<simpara>保存点中的状态拷贝会以算子标识符（operator ID）和状态名称（state
name）组织起来。算子ID和状态名称必须能够将保存点的状态数据，映射到一个正在启动的应用程序的算子状态。从保存点启动应用程序时，Flink会将保存点的数据重新分配给相应的算子任务。</simpara>
<blockquote>
<simpara>请注意，保存点不包含有关算子任务的信息。这是因为当应用程序以不同的并行度启动时，任务数量可能会更改。</simpara>
</blockquote>
<simpara>如果我们要从保存点启动一个修改过的应用程序，那么保存点中的状态只能映射到符合标准的应用程序——它里面的算子必须具有相应的ID和状态名称。默认情况下，Flink会自动分配唯一的算子ID。然而，一个算子的ID，是基于它之前算子的ID确定性地生成的。因此，算子的ID会在其前序算子改变时改变，比如，当我们添加了新的或移除掉一个算子时，前序算子ID改变，当前算子ID就会变化。所以对于具有默认算子ID的应用程序而言，如果想在不丢失状态的前提下升级，就会受到极大的限制。因此，我们强烈建议在程序中为算子手动分配唯一ID，而不是依靠Flink的默认分配。我们将在<literal>指定唯一的算子标识符</literal>一节中详细说明如何分配算子标识符。</simpara>
</section>
</section>
</section>
<section xml:id="_flink_datastream_api">
<title>Flink DataStream API</title>
<simpara>本章介绍了Flink DataStream
API的基本知识。我们展示了典型的Flink流处理程序的结构和组成部分，还讨论了Flink的类型系统以及支持的数据类型，还展示了数据和分区转换操作。窗口操作符，基于时间语义的转换操作，有状态的操作符，以及和外部系统的连接器将在接下来的章节进行介绍。阅读完这一章后，我们将会知道如何去实现一个具有基本功能的流处理程序。我们的示例程序采用Java语言，因为Java语言比较常用。</simpara>
<section xml:id="_你好flink">
<title>你好，Flink！</title>
<simpara>让我们写一个简单的例子来获得使用DataStream
API编写流处理应用程序的粗浅印象。我们将使用这个简单的示例来展示一个Flink程序的基本结构，以及介绍一些DataStream
API的重要特性。我们的示例程序摄取了一条（来自多个传感器的）温度测量数据流。</simpara>
<simpara>首先让我们看一下表示传感器读数的数据结构：</simpara>
<programlisting language="java" linenumbering="unnumbered">public class SensorReading {

    public String id;
    public long timestamp;
    public double temperature;

    public SensorReading() { }

    public SensorReading(String id, long timestamp, double temperature) {
        this.id = id;
        this.timestamp = timestamp;
        this.temperature = temperature;
    }

    public String toString() {
        return "(" + this.id + ", " + this.timestamp + ", " + this.temperature + ")";
    }
}</programlisting>
<simpara>示例程序将温度从华氏温度读数转换成摄氏温度读数，然后针对每一个传感器，每5秒钟计算一次平均温度纸。</simpara>
<programlisting language="java" linenumbering="unnumbered">public class AverageSensorReadings {
  public static void main(String[] args) throws Exception {
    final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

    DataStream&lt;SensorReading&gt; sensorData = env.addSource(new SensorSource());

    DataStream&lt;T&gt; avgTemp = sensorData
      .map(r -&gt; {
        Double celsius = (r.temperature - 32) * (5.0 / 9.0);
        return SensorReading(r.id, r.timestamp, celsius);
      })
      .keyBy(r -&gt; r.id)
      .window(TumblingProcessingTimeWindows(Time.seconds(5)))
      .apply(new TemperatureAverager());

    avgTemp.print();

    env.execute("Compute average sensor temperature");
  }
}</programlisting>
<simpara>你可能已经注意到Flink程序的定义和提交执行使用的就是正常的Java的方法。大多数情况下，这些代码都写在一个静态main方法中。在我们的例子中，我们定义了AverageSensorReadings对象，然后将大多数的应用程序逻辑放在了main()中。</simpara>
<simpara>Flink流处理程序的结构如下：</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>创建Flink程序执行环境。</simpara>
</listitem>
<listitem>
<simpara>从数据源读取一条或者多条流数据</simpara>
</listitem>
<listitem>
<simpara>使用流转换算子实现业务逻辑</simpara>
</listitem>
<listitem>
<simpara>将计算结果输出到一个或者多个外部设备（可选）</simpara>
</listitem>
<listitem>
<simpara>执行程序</simpara>
</listitem>
</orderedlist>
<simpara>接下来我们详细的学习一下这些部分。</simpara>
</section>
<section xml:id="_搭建执行环境">
<title>搭建执行环境</title>
<simpara>编写Flink程序的第一件事情就是搭建执行环境。执行环境决定了程序是运行在单机上还是集群上。在DataStream
API中，程序的执行环境是由StreamExecutionEnvironment设置的。在我们的例子中，我们通过调用静态getExecutionEnvironment()方法来获取执行环境。这个方法根据调用方法的上下文，返回一个本地的或者远程的环境。如果这个方法是一个客户端提交到远程集群的代码调用的，那么这个方法将会返回一个远程的执行环境。否则，将返回本地执行环境。</simpara>
<simpara>也可以用下面的方法来显式的创建本地或者远程执行环境：</simpara>
<programlisting language="java" linenumbering="unnumbered">StreamExecutionEnvironment localEnv = StreamExecutionEnvironment
  .createLocalEnvironment();

StreamExecutionEnvironment remoteEnv = StreamExecutionEnvironment
  .createRemoteEnvironment(
    "host", // hostname of JobManager
    1234, // port of JobManager process
    "path/to/jarFile.jar"
  ); // JAR file to ship to the JobManager</programlisting>
<simpara>执行环境提供了很多配置选项，例如：设置程序的并行度和程序是否开启容错机制。</simpara>
</section>
<section xml:id="_读取输入流">
<title>读取输入流</title>
<simpara>一旦执行环境设置好，就该写业务逻辑了。StreamExecutionEnvironment提供了创建数据源的方法，这些方法可以从数据流中将数据摄取到程序中。数据流可以来自消息队列或者文件系统，也可能是实时产生的（例如socket）。</simpara>
<simpara>在我们的例子里面，我们这样写：</simpara>
<programlisting language="java" linenumbering="unnumbered">DataStream&lt;SensorReading&gt; sensorData = env
  .addSource(new SensorSource());</programlisting>
<simpara>这样就可以连接到传感器测量数据的数据源并创建一个类型为SensorReading的DataStream了。Flink支持很多数据类型，我们将在接下来的章节里面讲解。在我们的例子里面，我们的数据类型是一个定义好的Java
POJO类。SensorReading样例类包含了传感器ID，数据的测量时间戳，以及测量温度值。assignTimestampsAndWatermarks(new
SensorTimeAssigner)方法指定了如何设置事件时间语义的时间戳和水位线。有关SensorTimeAssigner我们后面再讲。</simpara>
</section>
<section xml:id="_转换算子的使用">
<title>转换算子的使用</title>
<simpara>一旦我们有一条DataStream，我们就可以在这条数据流上面使用转换算子了。转换算子有很多种。一些转换算子可以产生一条新的DataStream，当然这个DataStream的类型可能是新类型。还有一些转换算子不会改变原有DataStream的数据，但会将数据流分区或者分组。业务逻辑就是由转换算子串起来组合而成的。</simpara>
<simpara>在我们的例子中，我们首先使用map()转换算子将传感器的温度值转换成了摄氏温度单位。然后，我们使用keyBy()转换算子将传感器读数流按照传感器ID进行分区。接下来，我们定义了一个window()算子，这个算子将每个传感器ID所对应的分区的传感器读数分配到了5秒钟的滚动窗口中。</simpara>
<programlisting language="java" linenumbering="unnumbered">DataStream&lt;T&gt; avgTemp = sensorData
  .map(r -&gt; {
    Double celsius = (r.temperature -32) * (5.0 / 9.0);
    return SensorReading(r.id, r.timestamp, celsius);
  })
  .keyBy(r -&gt; r.id)
  .window(TumblingProcessingTimeWindows(Time.seconds(5)))
  .apply(new TemperatureAverager());</programlisting>
<simpara>窗口转换算子将在<literal>窗口操作符</literal>一章中讲解。最后，我们使用了一个UDF函数来计算每个窗口的温度的平均值。我们稍后将会讨论UDF函数的实现。</simpara>
</section>
<section xml:id="_输出结果">
<title>输出结果</title>
<simpara>流处理程序经常将它们的计算结果发送到一些外部系统中去，例如：Apache
Kafka，文件系统，或者数据库中。Flink提供了一个维护的很好的sink算子的集合，这些sink算子可以用来将数据写入到不同的系统中去。我们也可以实现自己的sink算子。也有一些Flink程序并不会向第三方外部系统发送数据，而是将数据存储到Flink系统内部，然后可以使用Flink的可查询状态的特性来查询数据。</simpara>
<simpara>在我们的例子中，计算结果是一个`DataStream&lt;SensorReading&gt;`数据记录。每一条数据记录包含了一个传感器在5秒钟的周期里面的平均温度。计算结果组成的数据流将会调用print()将计算结果写到标准输出。</simpara>
<programlisting language="java" linenumbering="unnumbered">avgTemp.print();</programlisting>
<blockquote>
<simpara>要注意一点，流的Sink算子的选择将会影响应用程序端到端(end-to-end)的一致性，具体就是应用程序的计算提供的到底是at-least-once还是exactly-once的一致性语义。应用程序端到端的一致性依赖于所选择的流的Sink算子和Flink的检查点算法的集成使用。</simpara>
</blockquote>
</section>
<section xml:id="_执行">
<title>执行</title>
<simpara>当应用程序完全写好时，我们可以调用StreamExecutionEnvironment.execute()来执行应用程序。在我们的例子中就是我们的最后一行调用：</simpara>
<programlisting language="java" linenumbering="unnumbered">env.execute("Compute average sensor temperature");</programlisting>
<simpara>Flink程序是惰性执行的。也就是说创建数据源和转换算子的API调用并不会立刻触发任何数据处理逻辑。API调用仅仅是在执行环境中构建了一个执行计划，这个执行计划包含了执行环境创建的数据源和所有的将要用在数据源上的转换算子。只有当execute()被调用时，系统才会触发程序的执行。</simpara>
<simpara>构建好的执行计划将被翻译成一个JobGraph并提交到JobManager上面去执行。根据执行环境的种类，一个JobManager将会运行在一个本地线程中（如果是本地执行环境的化）或者JobGraph将会被发送到一个远程的JobManager上面去。如果JobManager远程运行，那么JobGraph必须和一个包含有所有类和应用程序的依赖的JAR包一起发送到远程JobManager。</simpara>
</section>
<section xml:id="_自定义产生传感器读数的数据源">
<title>自定义产生传感器读数的数据源</title>
<programlisting language="java" linenumbering="unnumbered">public class SensorSource extends RichParallelSourceFunction&lt;SensorReading&gt; {

    private boolean running = true;

    @Override
    public void run(SourceContext&lt;SensorReading&gt; srcCtx) throws Exception {

        Random rand = new Random();

        String[] sensorIds = new String[10];
        double[] curFTemp = new double[10];
        for (int i = 0; i &lt; 10; i++) {
            sensorIds[i] = "sensor_" + i;
            curFTemp[i] = 65 + (rand.nextGaussian() * 20);
        }

        while (running) {
            long curTime = Calendar.getInstance().getTimeInMillis();
            for (int i = 0; i &lt; 10; i++) {
                curFTemp[i] += rand.nextGaussian() * 0.5;
                srcCtx.collect(new SensorReading(sensorIds[i], curTime, curFTemp[i]));
            }

            Thread.sleep(100);
        }
    }

    @Override
    public void cancel() {
        this.running = false;
    }
}</programlisting>
<simpara>使用方法</simpara>
<programlisting language="java" linenumbering="unnumbered">DataStream&lt;SensorReading&gt; sensorData = env.addSource(new SensorSource());</programlisting>
</section>
<section xml:id="_转换算子_2">
<title>转换算子</title>
<simpara>在这一小节我们将大概看一下DataStream
API的基本转换算子。与时间有关的操作符（例如窗口操作符和其他特殊的转换算子）将会在后面的章节叙述。一个流的转换操作将会应用在一个或者多个流上面，这些转换操作将流转换成一个或者多个输出流。编写一个DataStream
API简单来说就是将这些转换算子组合在一起来构建一个数据流图，这个数据流图就实现了我们的业务逻辑。</simpara>
<simpara>大部分的流转换操作都基于用户自定义函数UDF。UDF函数打包了一些业务逻辑并定义了输入流的元素如何转换成输出流的元素。像MapFunction这样的函数，将会被定义为类，这个类实现了Flink针对特定的转换操作暴露出来的接口。</simpara>
<programlisting language="java" linenumbering="unnumbered">DataStream&lt;String&gt; sensorIds = filteredReadings.map(r -&gt; r.id);</programlisting>
<simpara>函数接口定义了需要由用户实现的转换方法，例如上面例子中的map()方法。</simpara>
<simpara>大部分函数接口被设计为Single Abstract
Method（单独抽象方法）接口，并且接口可以使用Java 8匿名函数来实现。Java
DataStream API也内置了对匿名函数的支持。当讲解DataStream
API的转换算子时，我们展示了针对所有函数类的接口，但为了简洁，大部分接口的实现使用匿名函数而不是函数类的方式。</simpara>
<simpara>DataStream
API针对大多数数据转换操作提供了转换算子。如果你很熟悉批处理API、函数式编程语言或者SQL，那么你将会发现这些API很容易学习。我们会将DataStream
API的转换算子分成四类：</simpara>
<itemizedlist>
<listitem>
<simpara>基本转换算子：将会作用在数据流中的每一条单独的数据上。</simpara>
</listitem>
<listitem>
<simpara>KeyedStream转换算子：在数据有key的情况下，对数据应用转换算子。</simpara>
</listitem>
<listitem>
<simpara>多流转换算子：合并多条流为一条流或者将一条流分割为多条流。</simpara>
</listitem>
<listitem>
<simpara>分布式转换算子：将重新组织流里面的事件。</simpara>
</listitem>
</itemizedlist>
<section xml:id="_基本转换算子">
<title>基本转换算子</title>
<simpara>基本转换算子会针对流中的每一个单独的事件做处理，也就是说每一个输入数据会产生一个输出数据。单值转换，数据的分割，数据的过滤，都是基本转换操作的典型例子。我们将解释这些算子的语义并提供示例代码。</simpara>
<simpara><emphasis>MAP</emphasis></simpara>
<simpara>map算子通过调用DataStream.map()来指定。map算子的使用将会产生一条新的数据流。它会将每一个输入的事件传送到一个用户自定义的mapper，这个mapper只返回一个输出事件，这个输出事件和输入事件的类型可能不一样。图5-1展示了一个map算子，这个map将每一个正方形转化成了圆形。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0501.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>MapFunction的类型与输入事件和输出事件的类型相关，可以通过实现MapFunction接口来定义。接口包含map()函数，这个函数将一个输入事件恰好转换为一个输出事件。</simpara>
<programlisting language="java" linenumbering="unnumbered">// T: the type of input elements
// O: the type of output elements
MapFunction[T, O]
    &gt; map(T): O</programlisting>
<simpara>下面的代码实现了将SensorReading中的id字段抽取出来的功能。</simpara>
<programlisting language="java" linenumbering="unnumbered">DataStream&lt;SensorReading&gt; readings = ...
DataStream&lt;String&gt; sensorIds = readings.map(new IdExtractor());

public static class IdExtractor implements MapFunction&lt;SensorReading, String&gt; {
    @Override
    public String map(SensorReading r) throws Exception {
        return r.id;
    }
}</programlisting>
<simpara>当然我们更推荐匿名函数的写法。</simpara>
<programlisting language="java" linenumbering="unnumbered">DataStream&lt;String&gt; sensorIds = filteredReadings.map(r -&gt; r.id);</programlisting>
<simpara><emphasis>FILTER</emphasis></simpara>
<simpara>filter转换算子通过在每个输入事件上对一个布尔条件进行求值来过滤掉一些元素，然后将剩下的元素继续发送。一个true的求值结果将会把输入事件保留下来并发送到输出，而如果求值结果为false，则输入事件会被抛弃掉。我们通过调用DataStream.filter()来指定流的filter算子，filter操作将产生一条新的流，其类型和输入流中的事件类型是一样的。图5-2展示了只产生白色方框的filter操作。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0502.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>布尔条件可以使用函数、FilterFunction接口或者匿名函数来实现。FilterFunction中的泛型是输入事件的类型。定义的filter()方法会作用在每一个输入元素上面，并返回一个布尔值。</simpara>
<programlisting language="java" linenumbering="unnumbered">// T: the type of elements
FilterFunction[T]
    &gt; filter(T): Boolean</programlisting>
<simpara>下面的例子展示了如何使用filter来从传感器数据中过滤掉温度值小于25华氏温度的读数。</simpara>
<programlisting language="java" linenumbering="unnumbered">DataStream&lt;SensorReading&gt; filteredReadings = readings.filter(r -&gt; r.temperature &gt;= 25);</programlisting>
<simpara><emphasis>FLATMAP</emphasis></simpara>
<simpara>flatMap算子和map算子很类似，不同之处在于针对每一个输入事件flatMap可以生成0个、1个或者多个输出元素。事实上，flatMap转换算子是filter和map的泛化。所以flatMap可以实现map和filter算子的功能。图5-3展示了flatMap如何根据输入事件的颜色来做不同的处理。如果输入事件是白色方框，则直接输出。输入元素是黑框，则复制输入。灰色方框会被过滤掉。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0503.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>flatMap算子将会应用在每一个输入事件上面。对应的FlatMapFunction定义了flatMap()方法，这个方法返回0个、1个或者多个事件到一个Collector集合中，作为输出结果。</simpara>
<programlisting language="java" linenumbering="unnumbered">// T: the type of input elements
// O: the type of output elements
FlatMapFunction[T, O]
    &gt; flatMap(T, Collector[O]): Unit</programlisting>
<simpara>下面的例子展示了在数据分析教程中经常用到的例子，我们用flatMap来实现。使用_来切割传感器ID，比如sensor_1。</simpara>
<programlisting language="java" linenumbering="unnumbered">public static class IdSplitter implements FlatMapFunction&lt;String, String&gt; {
    @Override
    public void flatMap(String id, Collector&lt;String&gt; out) {

        String[] splits = id.split("_");

        for (String split : splits) {
            out.collect(split);
        }
    }
}</programlisting>
<simpara>匿名函数写法：</simpara>
<programlisting language="java" linenumbering="unnumbered">DataStream&lt;String&gt; splitIds = sensorIds
    .flatMap((FlatMapFunction&lt;String, String&gt;)
            (id, out) -&gt; { for (String s: id.split("_")) { out.collect(s);}})
    // provide result type because Java cannot infer return type of lambda function
    // 提供结果的类型，因为Java无法推断匿名函数的返回值类型
    .returns(Types.STRING);</programlisting>
</section>
<section xml:id="_键控流转换算子">
<title>键控流转换算子</title>
<simpara>很多流处理程序的一个基本要求就是要能对数据进行分组，分组后的数据共享某一个相同的属性。DataStream
API提供了一个叫做KeyedStream的抽象，此抽象会从逻辑上对DataStream进行分区，分区后的数据拥有同样的Key值，分区后的流互不相关。</simpara>
<simpara>针对KeyedStream的状态转换操作可以读取数据或者写入数据到当前事件Key所对应的状态中。这表明拥有同样Key的所有事件都可以访问同样的状态，也就是说所以这些事件可以一起处理。</simpara>
<blockquote>
<simpara>要小心使用状态转换操作和基于Key的聚合操作。如果Key的值越来越多，例如：Key是订单ID，我们必须及时清空Key所对应的状态，以免引起内存方面的问题。稍后我们会详细讲解。</simpara>
</blockquote>
<simpara>KeyedStream可以使用map，flatMap和filter算子来处理。接下来我们会使用keyBy算子来将DataStream转换成KeyedStream，并讲解基于key的转换操作：滚动聚合和reduce算子。</simpara>
<simpara><emphasis>KEYBY</emphasis></simpara>
<simpara>keyBy通过指定key来将DataStream转换成KeyedStream。基于不同的key，流中的事件将被分配到不同的分区中去。所有具有相同key的事件将会在接下来的操作符的同一个子任务槽中进行处理。拥有不同key的事件可以在同一个任务中处理。但是算子只能访问当前事件的key所对应的状态。</simpara>
<simpara>如图5-4所示，把输入事件的颜色作为key，黑色的事件输出到了一个分区，其他颜色输出到了另一个分区。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0504.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>keyBy()方法接收一个参数，这个参数指定了key或者keys，有很多不同的方法来指定key。我们将在后面讲解。下面的代码声明了id这个字段为SensorReading流的key。</simpara>
<programlisting language="java" linenumbering="unnumbered">KeyedStream&lt;SensorReading, String&gt; keyed = readings.keyBy(r -&gt; r.id);</programlisting>
<simpara>匿名函数r &#8594; r.id抽取了传感器读数SensorReading的id值。</simpara>
<simpara><emphasis>滚动聚合</emphasis></simpara>
<simpara>滚动聚合算子由KeyedStream调用，并生成一个聚合以后的DataStream，例如：sum，minimum，maximum。一个滚动聚合算子会为每一个观察到的key保存一个聚合的值。针对每一个输入事件，算子将会更新保存的聚合结果，并发送一个带有更新后的值的事件到下游算子。滚动聚合不需要用户自定义函数，但需要接受一个参数，这个参数指定了在哪一个字段上面做聚合操作。DataStream
API提供了以下滚动聚合方法。</simpara>
<blockquote>
<simpara>滚动聚合算子只能用在滚动窗口，不能用在滑动窗口。</simpara>
</blockquote>
<itemizedlist>
<listitem>
<simpara>sum()：在输入流上对指定的字段做滚动相加操作。</simpara>
</listitem>
<listitem>
<simpara>min()：在输入流上对指定的字段求最小值。</simpara>
</listitem>
<listitem>
<simpara>max()：在输入流上对指定的字段求最大值。</simpara>
</listitem>
<listitem>
<simpara>minBy()：在输入流上针对指定字段求最小值，并返回包含当前观察到的最小值的事件。</simpara>
</listitem>
<listitem>
<simpara>maxBy()：在输入流上针对指定字段求最大值，并返回包含当前观察到的最大值的事件。</simpara>
</listitem>
</itemizedlist>
<simpara>滚动聚合算子无法组合起来使用，每次计算只能使用一个单独的滚动聚合算子。</simpara>
<simpara>下面的例子根据第一个字段来对类型为`Tuple3&lt;Integer, Integer, Integer&gt;`的流做分流操作，然后针对第二个字段做滚动求和操作。</simpara>
<programlisting language="java" linenumbering="unnumbered">DataStreamSource&lt;Tuple3&lt;Integer, Integer, Integer&gt;&gt; inputStream = env
    .fromElements(
        Tuple3.of(1, 2, 2),
        Tuple3.of(2, 3, 1),
        Tuple3.of(2, 2, 4),
        Tuple3.of(1, 5, 3)
    );

DataStream&lt;Tuple3&lt;Integer, Integer, Integer&gt;&gt; resultStream = inputStream
  .keyBy(0) // key on first field of the tuple
  .sum(1);   // sum the second field of the tuple in place</programlisting>
<simpara>在这个例子里面，输入流根据第一个字段来分流，然后在第二个字段上做计算。对于key
1，输出结果是(1,2,2),(1,7,2)。对于key
2，输出结果是(2,3,1),(2,5,1)。第一个字段是key，第二个字段是求和的数值，第三个字段未定义。</simpara>
<blockquote>
<simpara>滚动聚合操作会对每一个key都保存一个状态。因为状态从来不会被清空，所以我们在使用滚动聚合算子时只能使用在含有有限个key的流上面。</simpara>
</blockquote>
<simpara><emphasis>REDUCE</emphasis></simpara>
<simpara>reduce算子是滚动聚合的泛化实现。它将一个ReduceFunction应用到了一个KeyedStream上面去。reduce算子将会把每一个输入事件和当前已经reduce出来的值做聚合计算。reduce操作不会改变流的事件类型。输出流数据类型和输入流数据类型是一样的。</simpara>
<simpara>reduce函数可以通过实现接口ReduceFunction来创建一个类。ReduceFunction接口定义了reduce()方法，此方法接收两个输入事件，输出一个相同类型的事件。</simpara>
<programlisting language="java" linenumbering="unnumbered">// T: the element type
ReduceFunction[T]
    &gt; reduce(T, T): T</programlisting>
<simpara>下面的例子，流根据传感器ID分流，然后计算每个传感器的当前最大温度值。</simpara>
<programlisting language="java" linenumbering="unnumbered">DataStream&lt;SensorReading&gt; maxTempPerSensor = keyed
        .reduce((r1, r2) -&gt; {
            if (r1.temperature &gt; r2.temperature) {
                return r1;
            } else {
                return r2;
            }
        });</programlisting>
<blockquote>
<simpara>reduce作为滚动聚合的泛化实现，同样也要针对每一个key保存状态。因为状态从来不会清空，所以我们需要将reduce算子应用在一个有限key的流上。</simpara>
</blockquote>
</section>
<section xml:id="_多流转换算子">
<title>多流转换算子</title>
<simpara>许多应用需要摄入多个流并将流合并处理，还可能需要将一条流分割成多条流然后针对每一条流应用不同的业务逻辑。接下来，我们将讨论DataStream
API中提供的能够处理多条输入流或者发送多条输出流的操作算子。</simpara>
<simpara><emphasis>UNION</emphasis></simpara>
<simpara>DataStream.union()方法将两条或者多条DataStream合并成一条具有与输入流相同类型的输出DataStream。接下来的转换算子将会处理输入流中的所有元素。图5-5展示了union操作符如何将黑色和白色的事件流合并成一个单一输出流。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0505.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>事件合流的方式为FIFO方式。操作符并不会产生一个特定顺序的事件流。union操作符也不会进行去重。每一个输入事件都被发送到了下一个操作符。</simpara>
<simpara>下面的例子展示了如何将三条类型为SensorReading的数据流合并成一条流。</simpara>
<programlisting language="java" linenumbering="unnumbered">DataStream&lt;SensorReading&gt; parisStream = ...
DataStream&lt;SensorReading&gt; tokyoStream = ...
DataStream&lt;SensorReading&gt; rioStream = ...
DataStream&lt;SensorReading&gt; allCities = parisStream
  .union(tokyoStream, rioStream);</programlisting>
<simpara><emphasis>CONNECT, COMAP和COFLATMAP</emphasis></simpara>
<simpara>联合两条流的事件是非常常见的流处理需求。例如监控一片森林然后发出高危的火警警报。报警的Application接收两条流，一条是温度传感器传回来的数据，一条是烟雾传感器传回来的数据。当两条流都超过各自的阈值时，报警。</simpara>
<simpara>DataStream
API提供了connect操作来支持以上的应用场景。DataStream.connect()方法接收一条DataStream，然后返回一个ConnectedStreams类型的对象，这个对象表示了两条连接的流。</simpara>
<programlisting language="java" linenumbering="unnumbered">// first stream
DataStream&lt;Integer&gt; first = ...
// second stream
DataStream&lt;String&gt; second = ...

// connect streams
ConnectedStreams&lt;Integer, String&gt; connected = first.connect(second);</programlisting>
<simpara>ConnectedStreams提供了map()和flatMap()方法，分别需要接收类型为CoMapFunction和CoFlatMapFunction的参数。</simpara>
<simpara>以上两个函数里面的泛型是第一条流的事件类型和第二条流的事件类型，以及输出流的事件类型。还定义了两个方法，每一个方法针对一条流来调用。map1()和flatMap1()会调用在第一条流的元素上面，map2()和flatMap2()会调用在第二条流的元素上面。</simpara>
<programlisting language="java" linenumbering="unnumbered">// IN1: 第一条流的事件类型
// IN2: 第二条流的事件类型
// OUT: 输出流的事件类型
CoMapFunction[IN1, IN2, OUT]
    &gt; map1(IN1): OUT
    &gt; map2(IN2): OUT

CoFlatMapFunction[IN1, IN2, OUT]
    &gt; flatMap1(IN1, Collector[OUT]): Unit
    &gt; flatMap2(IN2, Collector[OUT]): Unit</programlisting>
<blockquote>
<simpara>函数无法选择读某一条流。我们是无法控制函数中的两个方法的调用顺序的。当一条流中的元素到来时，将会调用相对应的方法。</simpara>
</blockquote>
<simpara>对两条流做连接查询通常需要这两条流基于某些条件被确定性的路由到操作符中相同的并行实例里面去。在默认情况下，connect()操作将不会对两条流的事件建立任何关系，所以两条流的事件将会随机的被发送到下游的算子实例里面去。这样的行为会产生不确定性的计算结果，显然不是我们想要的。为了针对ConnectedStreams进行确定性的转换操作，connect()方法可以和keyBy()或者broadcast()组合起来使用。我们首先看一下keyBy()的示例。</simpara>
<programlisting language="java" linenumbering="unnumbered">DataStream&lt;Tuple2&lt;Integer, Long&gt;&gt; one = ...
DataStream&lt;Tuple2&lt;Integer, String&gt;&gt; two = ...

// keyBy two connected streams
ConnectedStreams&lt;Tuple2&lt;Integer, Long&gt;, Tuple2&lt;Integer, String&gt;&gt; keyedConnect1 = one
  .connect(two)
  .keyBy(0, 0); // key both input streams on first attribute

// alternative: connect two keyed streams
ConnectedStreams&lt;Tuple2&lt;Integer, Long&gt;, Tuple2&lt;Integer, String&gt;&gt; keyedConnect2 = one
  .keyBy(0)
  .connect(two.keyBy(0));</programlisting>
<simpara>无论使用keyBy()算子操作ConnectedStreams还是使用connect()算子连接两条KeyedStreams，connect()算子会将两条流的含有相同Key的所有事件都发送到相同的算子实例。两条流的key必须是一样的类型和值，就像SQL中的JOIN。在connected和keyed
stream上面执行的算子有访问keyed state的权限。</simpara>
<simpara>下面的例子展示了如何连接一条DataStream和广播过的流。</simpara>
<programlisting language="java" linenumbering="unnumbered">DataStream&lt;Tuple2&lt;Integer, Long&gt;&gt; one = ...
DataStream&lt;Tuple2&lt;Integer, String&gt;&gt; two = ...

// connect streams with broadcast
ConnectedStreams&lt;Tuple2&lt;Integer, Long&gt;, Tuple2&lt;Integer, String&gt;&gt; keyedConnect = first
  // broadcast second input stream
  .connect(second.broadcast());</programlisting>
<simpara>一条被广播过的流中的所有元素将会被复制然后发送到下游算子的所有并行实例中去。未被广播过的流仅仅向前发送。所以两条流的元素显然会被连接处理。</simpara>
<simpara><emphasis role="strong">警告类</emphasis></simpara>
<programlisting language="java" linenumbering="unnumbered">public class Alert {

    public String message;
    public long timestamp;

    public Alert() { }

    public Alert(String message, long timestamp) {
        this.message = message;
        this.timestamp = timestamp;
    }

    public String toString() {
        return "(" + message + ", " + timestamp + ")";
    }
}</programlisting>
<simpara><emphasis role="strong">烟雾传感器读数类</emphasis></simpara>
<programlisting language="java" linenumbering="unnumbered">public enum SmokeLevel {
    LOW,
    HIGH
}</programlisting>
<simpara><emphasis role="strong">产生烟雾传感器读数的自定义数据源</emphasis></simpara>
<programlisting language="java" linenumbering="unnumbered">public class SmokeLevelSource implements SourceFunction&lt;SmokeLevel&gt; {

    private boolean running = true;

    @Override
    public void run(SourceContext&lt;SmokeLevel&gt; srcCtx) throws Exception {

        Random rand = new Random();

        while (running) {

            if (rand.nextGaussian() &gt; 0.8) {
                srcCtx.collect(SmokeLevel.HIGH);
            } else {
                srcCtx.collect(SmokeLevel.LOW);
            }

            Thread.sleep(1000);
        }
    }

    @Override
    public void cancel() {
        this.running = false;
    }
}</programlisting>
<simpara>监控一片森林然后发出高危的火警警报。报警的Application接收两条流，一条是温度传感器传回来的数据，一条是烟雾传感器传回来的数据。当两条流都超过各自的阈值时，报警。</simpara>
<programlisting language="java" linenumbering="unnumbered">public class MultiStreamTransformations {

    public static void main(String[] args) throws Exception {

        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        DataStream&lt;SensorReading&gt; tempReadings = env
                .addSource(new SensorSource());

        DataStream&lt;SmokeLevel&gt; smokeReadings = env
                .addSource(new SmokeLevelSource())
                .setParallelism(1);

        KeyedStream&lt;SensorReading, String&gt; keyedTempReadings = tempReadings
                .keyBy(r -&gt; r.id);

        DataStream&lt;Alert&gt; alerts = keyedTempReadings
                .connect(smokeReadings.broadcast())
                .flatMap(new RaiseAlertFlatMap());

        alerts.print();

        env.execute("Multi-Stream Transformations Example");
    }

    public static class RaiseAlertFlatMap implements CoFlatMapFunction&lt;SensorReading, SmokeLevel, Alert&gt; {

        private SmokeLevel smokeLevel = SmokeLevel.LOW;

        @Override
        public void flatMap1(SensorReading tempReading, Collector&lt;Alert&gt; out) throws Exception {
            // high chance of fire =&gt; true
            if (this.smokeLevel == SmokeLevel.HIGH &amp;&amp; tempReading.temperature &gt; 100) {
                out.collect(new Alert("Risk of fire! " + tempReading, tempReading.timestamp));
            }
        }

        @Override
        public void flatMap2(SmokeLevel smokeLevel, Collector&lt;Alert&gt; out) {
            // update smoke level
            this.smokeLevel = smokeLevel;
        }
    }
}</programlisting>
</section>
<section xml:id="_分布式转换算子">
<title>分布式转换算子</title>
<simpara>分区操作对应于我们之前讲过的<literal>数据交换策略</literal>这一节。这些操作定义了事件如何分配到不同的任务中去。当我们使用DataStream
API来编写程序时，系统将自动的选择数据分区策略，然后根据操作符的语义和设置的并行度将数据路由到正确的地方去。有些时候，我们需要在应用程序的层面控制分区策略，或者自定义分区策略。例如，如果我们知道会发生数据倾斜，那么我们想要针对数据流做负载均衡，将数据流平均发送到接下来的操作符中去。又或者，应用程序的业务逻辑可能需要一个算子所有的并行任务都需要接收同样的数据。再或者，我们需要自定义分区策略的时候。在这一小节，我们将展示DataStream的一些方法，可以使我们来控制或者自定义数据分区策略。</simpara>
<blockquote>
<simpara>keyBy()方法不同于分布式转换算子。所有的分布式转换算子将产生DataStream数据类型。而keyBy()产生的类型是KeyedStream，它拥有自己的keyed
state。</simpara>
</blockquote>
<simpara><emphasis>Random</emphasis></simpara>
<simpara>随机数据交换由DataStream.shuffle()方法实现。shuffle方法将数据随机的分配到下游算子的并行任务中去。</simpara>
<simpara><emphasis>Round-Robin</emphasis></simpara>
<simpara>rebalance()方法使用Round-Robin负载均衡算法将输入流平均分配到随后的并行运行的任务中去。图5-7为round-robin分布式转换算子的示意图。</simpara>
<simpara><emphasis>Rescale</emphasis></simpara>
<simpara>rescale()方法使用的也是round-robin算法，但只会将数据发送到接下来的并行运行的任务中的一部分任务中。本质上，当发送者任务数量和接收者任务数量不一样时，rescale分区策略提供了一种轻量级的负载均衡策略。如果接收者任务的数量是发送者任务的数量的倍数时，rescale操作将会效率更高。</simpara>
<simpara>rebalance()和rescale()的根本区别在于任务之间连接的机制不同。
rebalance()将会针对所有发送者任务和所有接收者任务之间建立通信通道，而rescale()仅仅针对每一个任务和下游算子的一部分子并行任务之间建立通信通道。rescale的示意图为图5-7。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0507.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara><emphasis>Broadcast</emphasis></simpara>
<simpara>broadcast()方法将输入流的所有数据复制并发送到下游算子的所有并行任务中去。</simpara>
<simpara><emphasis>Global</emphasis></simpara>
<simpara>global()方法将所有的输入流数据都发送到下游算子的第一个并行任务中去。这个操作需要很谨慎，因为将所有数据发送到同一个task，将会对应用程序造成很大的压力。</simpara>
<simpara><emphasis>Custom</emphasis></simpara>
<simpara>当Flink提供的分区策略都不适用时，我们可以使用partitionCustom()方法来自定义分区策略。这个方法接收一个Partitioner对象，这个对象需要实现分区逻辑以及定义针对流的哪一个字段或者key来进行分区。</simpara>
</section>
</section>
<section xml:id="_设置并行度">
<title>设置并行度</title>
<simpara>Flink应用程序在一个像集群这样的分布式环境中并行执行。当一个数据流程序提交到作业管理器执行时，系统将会创建一个数据流图，然后准备执行需要的操作符。每一个操作符将会并行化到一个或者多个任务中去。每个算子的并行任务都会处理这个算子的输入流中的一份子集。一个算子并行任务的个数叫做算子的并行度。它决定了算子执行的并行化程度，以及这个算子能处理多少数据量。</simpara>
<simpara>算子的并行度可以在执行环境这个层级来控制，也可以针对每个不同的算子设置不同的并行度。默认情况下，应用程序中所有算子的并行度都将设置为执行环境的并行度。执行环境的并行度（也就是所有算子的默认并行度）将在程序开始运行时自动初始化。如果应用程序在本地执行环境中运行，并行度将被设置为CPU的核数。当我们把应用程序提交到一个处于运行中的Flink集群时，执行环境的并行度将被设置为集群默认的并行度，除非我们在客户端提交应用程序时显式的设置好并行度。</simpara>
<simpara>通常情况下，将算子的并行度定义为和执行环境并行度相关的数值会是个好主意。这允许我们通过在客户端调整应用程序的并行度就可以将程序水平扩展了。我们可以使用以下代码来访问执行环境的默认并行度。</simpara>
<simpara>我们还可以重写执行环境的默认并行度，但这样的话我们将再也不能通过客户端来控制应用程序的并行度了。</simpara>
<simpara>算子默认的并行度也可以通过重写来明确指定。在下面的例子里面，数据源的操作符将会按照环境默认的并行度来并行执行，map操作符的并行度将会是默认并行度的2倍，sink操作符的并行度为2。</simpara>
<programlisting language="java" linenumbering="unnumbered">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
int defaultP = env.getParallelism();
env
  .addSource(new CustomSource())
  .map(new MyMapper())
  .setParallelism(defaultP * 2)
  .print()
  .setParallelism(2);</programlisting>
<simpara>当我们通过客户端将应用程序的并行度设置为16并提交执行时，source操作符的并行度为16，mapper并行度为32，sink并行度为2。如果我们在本地环境运行应用程序的话，例如在IDE中运行，机器是8核，那么source任务将会并行执行在8个任务上面，mapper运行在16个任务上面，sink运行在2个任务上面。</simpara>
<blockquote>
<simpara>并行度是动态概念，任务槽数量是静态概念。并行度&#8656;任务槽数量。一个任务槽最多运行一个并行度。</simpara>
</blockquote>
</section>
<section xml:id="_类型">
<title>类型</title>
<simpara>Flink程序所处理的流中的事件一般是对象类型。操作符接收对象输出对象。所以Flink的内部机制需要能够处理事件的类型。在网络中传输数据，或者将数据写入到状态后端、检查点和保存点中，都需要我们对数据进行序列化和反序列化。为了高效的进行此类操作，Flink需要流中事件类型的详细信息。Flink使用了Type
Information的概念来表达数据类型，这样就能针对不同的数据类型产生特定的序列化器，反序列化器和比较操作符。</simpara>
<simpara>Flink也能够通过分析输入数据和输出数据来自动获取数据的类型信息以及序列化器和反序列化器。尽管如此，在一些特定的情况下，例如匿名函数或者使用泛型的情况下，我们需要明确的提供数据的类型信息，来提高我们程序的性能。</simpara>
<simpara>在这一节中，我们将讨论Flink支持的类型，以及如何为数据类型创建相应的类型信息，还有就是在Flink无法推断函数返回类型的情况下，如何帮助Flink的类型系统去做类型推断。</simpara>
<section xml:id="_支持的数据类型">
<title>支持的数据类型</title>
<simpara>Flink支持Java提供的所有普通数据类型。最常用的数据类型可以做以下分类：</simpara>
<itemizedlist>
<listitem>
<simpara>Primitives（原始数据类型）</simpara>
</listitem>
<listitem>
<simpara>Flink专门为Java实现的Tuples（元组）</simpara>
</listitem>
<listitem>
<simpara>POJO类型</simpara>
</listitem>
<listitem>
<simpara>一些特殊的类型</simpara>
</listitem>
</itemizedlist>
<simpara>接下来让我们一探究竟。</simpara>
<simpara><emphasis>Primitives</emphasis></simpara>
<simpara>Java提供的所有原始数据类型都支持，例如Integer，String，Double等等。下面举一个例子：</simpara>
<programlisting language="java" linenumbering="unnumbered">DataStream&lt;Long&gt; numbers = env.fromElements(1L, 2L, 3L, 4L);
numbers.map(n -&gt; n + 1);</programlisting>
<simpara><emphasis>Tuples</emphasis></simpara>
<simpara>元组是一种组合数据类型，由固定数量的元素组成。</simpara>
<simpara>Flink为Java的Tuple提供了高效的实现。Flink实现的Java
Tuple最多可以有25个元素，根据元素数量的不同，Tuple都被实现成了不同的类：Tuple1，Tuple2，一直到Tuple25。Tuple类是强类型。</simpara>
<programlisting language="java" linenumbering="unnumbered">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; persons = env
  .fromElements(
    Tuple2.of("Adam", 17),
    Tuple2.of("Sarah", 23)
  );

persons.filter(p -&gt; p.f1 &gt; 18);</programlisting>
<simpara>Tuple的元素可以通过它们的public属性访问——f0，f1，f2等等。或者使用getField(int
pos)方法来访问，元素下标从0开始：</simpara>
<programlisting language="java" linenumbering="unnumbered">import org.apache.flink.api.java.tuple.Tuple2;

Tuple2&lt;String, Integer&gt; personTuple = Tuple2.of("Alex", 42);
Integer age = personTuple.getField(1); // age = 42</programlisting>
<simpara>Flink为Java实现的Tuple是可变数据结构，所以Tuple中的元素可以重新进行赋值。重复利用Java的Tuple可以减轻垃圾收集的压力。举个例子：</simpara>
<programlisting language="java" linenumbering="unnumbered">personTuple.f1 = 42; // set the 2nd field to 42
personTuple.setField(43, 1); // set the 2nd field to 43</programlisting>
<simpara><emphasis>POJO</emphasis></simpara>
<simpara>POJO类的定义：</simpara>
<itemizedlist>
<listitem>
<simpara>公有类</simpara>
</listitem>
<listitem>
<simpara>无参数的公有构造器</simpara>
</listitem>
<listitem>
<simpara>所有的字段都是公有的，可以通过getters和setters访问。</simpara>
</listitem>
<listitem>
<simpara>所有字段的数据类型都必须是Flink支持的数据类型。</simpara>
</listitem>
</itemizedlist>
<simpara>举个例子：</simpara>
<programlisting language="java" linenumbering="unnumbered">public class Person {
  public String name;
  public int age;

  public Person() {}

  public Person(String name, int age) {
    this.name = name;
    this.age = age;
  }
}

DataStream&lt;Person&gt; persons = env.fromElements(
  new Person("Alex", 42),
  new Person("Wendy", 23)
);</programlisting>
<simpara><emphasis>其他数据类型</emphasis></simpara>
<itemizedlist>
<listitem>
<simpara>Array, ArrayList, HashMap, Enum</simpara>
</listitem>
<listitem>
<simpara>Hadoop Writable types</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="_为数据类型创建类型信息">
<title>为数据类型创建类型信息</title>
<simpara>Flink类型系统的核心类是TypeInformation。它为系统在产生序列化器和比较操作符时，提供了必要的类型信息。例如，如果我们想使用某个key来做联结查询或者分组操作，TypeInformation可以让Flink做更严格的类型检查。</simpara>
<simpara>Flink针对Java提供了类来产生类型信息。在Java中，类是</simpara>
<programlisting language="java" linenumbering="unnumbered">org.apache.flink.api.common.typeinfo.Types</programlisting>
<simpara>举个例子：</simpara>
<programlisting language="java" linenumbering="unnumbered">TypeInformation&lt;Integer&gt; intType = Types.INT;

TypeInformation&lt;Tuple2&lt;Long, String&gt;&gt; tupleType = Types
  .TUPLE(Types.LONG, Types.STRING);

TypeInformation&lt;Person&gt; personType = Types
  .POJO(Person.class);</programlisting>
</section>
</section>
<section xml:id="_富函数">
<title>富函数</title>
<simpara>我们经常会有这样的需求：在函数处理数据之前，需要做一些初始化的工作；或者需要在处理数据时可以获得函数执行上下文的一些信息；以及在处理完数据时做一些清理工作。而DataStream
API就提供了这样的机制。</simpara>
<simpara>DataStream
API提供的所有转换操作函数，都拥有它们的<literal>富</literal>版本，并且我们在使用常规函数或者匿名函数的地方来使用富函数。例如下面就是富函数的一些例子，可以看出，只需要在常规函数的前面加上Rich前缀就是富函数了。</simpara>
<itemizedlist>
<listitem>
<simpara>RichMapFunction</simpara>
</listitem>
<listitem>
<simpara>RichFlatMapFunction</simpara>
</listitem>
<listitem>
<simpara>RichFilterFunction</simpara>
</listitem>
<listitem>
<simpara>…</simpara>
</listitem>
</itemizedlist>
<simpara>当我们使用富函数时，我们可以实现两个额外的方法：</simpara>
<itemizedlist>
<listitem>
<simpara>open()方法是rich
function的初始化方法，当一个算子例如map或者filter被调用之前open()会被调用。open()函数通常用来做一些只需要做一次即可的初始化工作。</simpara>
</listitem>
<listitem>
<simpara>close()方法是生命周期中的最后一个调用的方法，通常用来做一些清理工作。</simpara>
</listitem>
</itemizedlist>
<simpara>另外，getRuntimeContext()方法提供了函数的RuntimeContext的一些信息，例如函数执行的并行度，当前子任务的索引，当前子任务的名字。同时还它还包含了访问分区状态的方法。下面看一个例子：</simpara>
<programlisting language="java" linenumbering="unnumbered">public static class MyFlatMap extends RichFlatMapFunction&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt; {
  private int subTaskIndex = 0;

  @Override
  public void open(Configuration configuration) {
    int subTaskIndex = getRuntimeContext.getIndexOfThisSubtask;
    // 做一些初始化工作
    // 例如建立一个和HDFS的连接
  }

  @Override
  public void flatMap(Integer in, Collector&lt;Tuple2&lt;Integer, Integer&gt;&gt; out) {
    if (in % 2 == subTaskIndex) {
      out.collect((subTaskIndex, in));
    }
  }

  @Override
  public void close() {
    // 清理工作，断开和HDFS的连接。
  }
}</programlisting>
</section>
</section>
<section xml:id="_基于时间和窗口的操作符">
<title>基于时间和窗口的操作符</title>
<simpara>在本章，我们将要学习DataStream
API中处理时间和基于时间的操作符，例如窗口操作符。</simpara>
<simpara>首先，我们会学习如何定义时间属性，时间戳和水位线。然后我们将会学习底层操作process
function，它可以让我们访问时间戳和水位线，以及注册定时器事件。接下来，我们将会使用Flink的window
API，它提供了通常使用的各种窗口类型的内置实现。我们将会学到如何进行用户自定义窗口操作符，以及窗口的核心功能：assigners（分配器）、triggers（触发器）和evictors（清理器）。最后，我们将讨论如何基于时间来做流的联结查询，以及处理迟到事件的策略。</simpara>
<section xml:id="_设置时间属性">
<title>设置时间属性</title>
<simpara>如果我们想要在分布式流处理应用程序中定义有关时间的操作，彻底理解时间的语义是非常重要的。当我们指定了一个窗口去收集某1分钟内的数据时，这个长度为1分钟的桶中，到底应该包含哪些数据？</simpara>
<simpara><emphasis>ProcessingTime</emphasis></simpara>
<blockquote>
<simpara>机器时间在分布式系统中又叫做<literal>墙上时钟</literal>。</simpara>
</blockquote>
<simpara>当操作符执行时，此操作符看到的时间是操作符所在机器的机器时间。Processing-time
window的触发取决于机器时间，窗口包含的元素也是那个机器时间段内到达的元素。通常情况下，窗口操作符使用processing
time会导致不确定的结果，因为基于机器时间的窗口中收集的元素取决于元素到达的速度快慢。使用processing
time会为程序提供极低的延迟，因为无需等待水位线的到达。</simpara>
<blockquote>
<simpara>如果要追求极限的低延迟，请使用ProcessingTime。</simpara>
</blockquote>
<simpara><emphasis>EventTime</emphasis></simpara>
<simpara>当操作符执行时，操作符看的当前时间是由流中元素所携带的信息决定的。流中的每一个元素都必须包含时间戳信息。而系统的逻辑时钟由水位线(Watermark)定义。我们之前学习过，时间戳要么在事件进入流处理程序之前已经存在，要么就需要在程序的数据源（source）处进行分配。当水位线宣布特定时间段的数据都已经到达，事件时间窗口将会被触发计算。即使数据到达的顺序是乱序的，事件时间窗口的计算结果也将是确定性的。窗口的计算结果并不取决于元素到达的快与慢。</simpara>
<blockquote>
<simpara>当水位线超过事件时间窗口的结束时间时，窗口将会闭合，不再接收数据，并触发计算。</simpara>
</blockquote>
<simpara><emphasis>IngestionTime</emphasis></simpara>
<simpara>当事件进入source操作符时，source操作符所在机器的机器时间，就是此事件的<literal>摄入时间</literal>（IngestionTime），并同时产生水位线。IngestionTime相当于EventTime和ProcessingTime的混合体。一个事件的IngestionTime其实就是它进入流处理器中的时间。</simpara>
<blockquote>
<simpara>IngestionTime没什么价值，既有EventTime的执行效率（比较低），有没有EventTime计算结果的准确性。</simpara>
</blockquote>
<simpara>Flink 1.12通过指定不同类型的窗口来使用不同的时间属性。</simpara>
<section xml:id="_处理时间设定">
<title>处理时间设定</title>
<simpara>我们写Flink程序一般遵循以下步骤：</simpara>
<itemizedlist>
<listitem>
<simpara>读取数据源</simpara>
</listitem>
<listitem>
<simpara>分流</simpara>
</listitem>
<listitem>
<simpara>开窗</simpara>
</listitem>
<listitem>
<simpara>聚合</simpara>
</listitem>
<listitem>
<simpara>输出结果</simpara>
</listitem>
</itemizedlist>
<simpara><emphasis>滚动窗口</emphasis></simpara>
<programlisting language="java" linenumbering="unnumbered">.keyBy(...)
.window(TumblingProcessingTimeWindows.of(Time.seconds(5)))
.aggregate(...)</programlisting>
<simpara><emphasis>滑动窗口</emphasis></simpara>
<programlisting language="java" linenumbering="unnumbered">.keyBy(...)
.window(SlidingProcessingTimeWindows.of(Time.seconds(10), Time.seconds(5)))
.aggregate(...)</programlisting>
<simpara><emphasis>会话窗口</emphasis></simpara>
<programlisting language="java" linenumbering="unnumbered">.keyBy(...)
.window(ProcessingTimeSessionWindows.withGap(Time.seconds(10)))
.aggregate(...)</programlisting>
</section>
<section xml:id="_事件时间设定和水位线的产生">
<title>事件时间设定和水位线的产生</title>
<simpara><emphasis>滚动窗口</emphasis></simpara>
<programlisting language="java" linenumbering="unnumbered">.keyBy(...)
.window(TumblingEventTimeWindows.of(Time.seconds(5)))
.aggregate(...)</programlisting>
<simpara><emphasis>滑动窗口</emphasis></simpara>
<programlisting language="java" linenumbering="unnumbered">.keyBy(...)
.window(SlidingEventTimeWindows.of(Time.seconds(10), Time.seconds(5)))
.aggregate(...)</programlisting>
<simpara><emphasis>会话窗口</emphasis></simpara>
<programlisting language="java" linenumbering="unnumbered">.keyBy(...)
.window(EventTimeSessionWindows.withGap(Time.seconds(10)))
.aggregate(...)</programlisting>
<simpara>如果使用事件时间，那么流中的事件必须包含这个事件真正发生的时间。使用了事件时间的流必须携带水位线。</simpara>
<simpara>时间戳和水位线的单位是毫秒，记时从1970-01-01T00:00:00Z开始。到达某个操作符的水位线就会告知这个操作符：小于等于水位线中携带的时间戳的事件都已经到达这个操作符了。时间戳和水位线可以由SourceFunction产生，或者由用户自定义的时间戳分配器和水位线产生器来生成。</simpara>
<simpara>Flink暴露了TimestampAssigner接口供我们实现，使我们可以自定义如何从事件数据中抽取时间戳。一般来说，时间戳分配器需要在source操作符后马上进行调用。</simpara>
<blockquote>
<simpara>因为时间戳分配器看到的元素的顺序应该和source操作符产生数据的顺序是一样的，否则就乱了。这就是为什么我们经常将source操作符的并行度设置为1的原因。</simpara>
</blockquote>
<simpara>也就是说，任何分区操作都会将元素的顺序打乱，例如：并行度改变，keyBy()操作等等。</simpara>
<simpara>所以最佳实践是：在尽量接近数据源source操作符的地方分配时间戳和产生水位线，甚至最好在SourceFunction中分配时间戳和产生水位线。当然在分配时间戳和产生水位线之前可以对流进行map和filter操作是没问题的，也就是说必须是窄依赖。</simpara>
<simpara>以下这种写法是可以的。</simpara>
<programlisting language="java" linenumbering="unnumbered">DataStream&lt;T&gt; stream = env
  .addSource(...)
  .map(...)
  .filter(...)
  .assignTimestampsAndWatermarks(...)</programlisting>
<simpara>我们来看一个例子，这个例子的最大延迟时间是5秒钟。</simpara>
<programlisting language="java" linenumbering="unnumbered">env
  .addSource(new SensorSource())
  .assignTimestampsAndWatermarks(
    WatermarkStrategy
      .&lt;SensorReading&gt;forBoundedOutOfOrderness(Duration.ofSeconds(5))
      .withTimestampAssigner(new SerializableTimestampAssigner&lt;SensorReading&gt;() {
        @Override
        public long extractTimestamp(SensorReading element, long recordTimestamp) {
          return element.timestamp;
        }
      }))
    .keyBy(r -&gt; r.id)
    .window(TumblingEventTimeWindows.of(Time.seconds(5)))
    .aggregate(...)</programlisting>
<simpara>如果我们已经知道到来的事件的时间戳是升序的，也就是最大延迟时间为0，那么我们可以使用如下写法：</simpara>
<programlisting language="java" linenumbering="unnumbered">env
  .addSource(new SensorSource())
  .assignTimestampsAndWatermarks(
    WatermarkStrategy.&lt;SensorReading&gt;forMonotonousTimestamps()
      .withTimestampAssigner(new SerializableTimestampAssigner&lt;SensorReading&gt;() {
        @Override
        public long extractTimestamp(SensorReading element, long recordTimestamp) {
          return element.timestamp;
        }
    }))
    .keyBy(r -&gt; r.id)
    .window(TumblingEventTimeWindows.of(Time.seconds(5)))
    .aggregate(...)</programlisting>
</section>
<section xml:id="_自定义水位线的产生逻辑">
<title>自定义水位线的产生逻辑</title>
<simpara><emphasis role="strong">产生水位线的接口</emphasis></simpara>
<programlisting language="java" linenumbering="unnumbered">@Public
public interface WatermarkGenerator&lt;T&gt; {

    /**
     * 每来一个事件都会调用, 允许水位线产生器记忆和检查事件的时间戳。
     * 允许水位线产生器基于事件本身发射水位线。
     */
    void onEvent(T event, long eventTimestamp, WatermarkOutput output);

    /**
     * 周期性的调用（默认200ms调用一次）, 可能会产生新的水位线，也可能不会。
     *
     * 调用周期通过ExecutionConfig#getAutoWatermarkInterval()方法来配置。
     */
    void onPeriodicEmit(WatermarkOutput output);
}</programlisting>
<simpara><emphasis role="strong">周期性产生水位线</emphasis></simpara>
<programlisting language="java" linenumbering="unnumbered">public class BoundedOutOfOrdernessGenerator implements WatermarkGenerator&lt;MyEvent&gt; {

    private final long maxOutOfOrderness = 3500; // 最大延迟时间是3.5s

    private long currentMaxTimestamp;

    @Override
    public void onEvent(MyEvent event, long eventTimestamp, WatermarkOutput output) {
        currentMaxTimestamp = Math.max(currentMaxTimestamp, eventTimestamp);
    }

    @Override
    public void onPeriodicEmit(WatermarkOutput output) {
        // 产生水位线的公式：观察到的最大时间戳 - 最大延迟时间 - 1ms
        output.emitWatermark(new Watermark(currentMaxTimestamp - maxOutOfOrderness - 1));
    }

}</programlisting>
<simpara><emphasis role="strong">不规则水位线的产生</emphasis></simpara>
<programlisting language="java" linenumbering="unnumbered">public class PunctuatedAssigner implements WatermarkGenerator&lt;MyEvent&gt; {

    @Override
    public void onEvent(MyEvent event, long eventTimestamp, WatermarkOutput output) {
        if (event.hasWatermarkMarker()) {
            output.emitWatermark(new Watermark(event.getWatermarkTimestamp()));
        }
    }

    @Override
    public void onPeriodicEmit(WatermarkOutput output) {
        // 不需要做任何事情，因为我们在onEvent方法中发射了水位线
    }
}</programlisting>
</section>
<section xml:id="_水位线总结">
<title>水位线总结</title>
<blockquote>
<simpara>水位线是一种逻辑时钟，Flink认为时间戳小于等于水位线的事件都到了。</simpara>
</blockquote>
<blockquote>
<simpara>水位线 = 观察到的事件所包含的最大时间戳 - 最大延迟时间 - 1毫秒</simpara>
</blockquote>
<blockquote>
<simpara>在事件时间的世界里，时间就是水位线。</simpara>
</blockquote>
<simpara>现在我们要讨论一下水位线会对我们的程序产生什么样的影响。</simpara>
<simpara>水位线用来平衡延迟和计算结果的正确性。水位线告诉我们，在触发计算（例如关闭窗口并触发窗口计算）之前，我们需要等待事件多长时间。基于事件时间的操作符根据水位线来衡量系统的逻辑时间的进度。</simpara>
<simpara>完美的水位线永远不会错：时间戳小于水位线的事件不会再出现。在特殊情况下(例如非乱序事件流)，最近一次事件的时间戳就可能是完美的水位线。启发式水位线则相反，它只估计时间，因此有可能出错，即迟到的事件(其时间戳小于水位线标记时间)晚于水位线出现。针对启发式水位线，Flink提供了处理迟到元素的机制。</simpara>
<simpara>设定水位线通常需要用到领域知识。举例来说，如果知道事件的迟到时间不会超过5秒，就可以将水位线标记时间设为收到的最大时间戳减去5秒。另一种做法是，采用一个Flink作业监控事件流，学习事件的迟到规律，并以此构建水位线生成模型。</simpara>
<simpara>如果最大延迟时间设置的很大，计算出的结果会更精确，但收到计算结果的速度会很慢，同时系统会缓存大量的数据，并对系统造成比较大的压力。如果最大延迟时间设置的很小，那么收到计算结果的速度会很快，但可能收到错误的计算结果。不过Flink处理迟到数据的机制可以解决这个问题。上述问题看起来很复杂，但是恰恰符合现实世界的规律：大部分真实的事件流都是乱序的，并且通常无法了解它们的乱序程度(因为理论上不能预见未来)。水位线是唯一让我们直面乱序事件流并保证正确性的机制;
否则只能选择忽视事实，假装错误的结果是正确的。</simpara>
</section>
</section>
<section xml:id="_处理函数">
<title>处理函数</title>
<simpara>我们之前学习的转换算子是无法访问事件的时间戳信息和水位线信息的。而这在一些应用场景下，极为重要。例如MapFunction这样的map转换算子就无法访问时间戳或者当前事件的事件时间。</simpara>
<simpara>基于此，DataStream
API提供了一系列的Low-Level转换算子。可以访问时间戳、水位线以及注册定时事件。还可以输出特定的一些事件，例如超时事件等。Process
Function用来构建事件驱动的应用以及实现自定义的业务逻辑(使用之前的window函数和转换算子无法实现)。例如，Flink-SQL就是使用Process
Function实现的。</simpara>
<simpara>Flink提供了8个Process Function：</simpara>
<itemizedlist>
<listitem>
<simpara>ProcessFunction</simpara>
</listitem>
<listitem>
<simpara>KeyedProcessFunction</simpara>
</listitem>
<listitem>
<simpara>CoProcessFunction</simpara>
</listitem>
<listitem>
<simpara>ProcessJoinFunction</simpara>
</listitem>
<listitem>
<simpara>BroadcastProcessFunction</simpara>
</listitem>
<listitem>
<simpara>KeyedBroadcastProcessFunction</simpara>
</listitem>
<listitem>
<simpara>ProcessWindowFunction</simpara>
</listitem>
<listitem>
<simpara>ProcessAllWindowFunction</simpara>
</listitem>
</itemizedlist>
<section xml:id="_keyedprocessfunction的使用">
<title>KeyedProcessFunction的使用</title>
<simpara>我们这里详细介绍一下KeyedProcessFunction。</simpara>
<simpara>KeyedProcessFunction用来操作KeyedStream。KeyedProcessFunction会处理流的每一个元素，输出为0个、1个或者多个元素。所有的Process
Function都继承自RichFunction接口，所以都有open()、close()和getRuntimeContext()等方法。而<literal>KeyedProcessFunction&lt;KEY, IN, OUT&gt;</literal>还额外提供了两个方法:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>processElement(IN value, Context ctx, Collector&lt;O&gt; out)</literal>：流中的每一个元素都会调用这个方法，调用结果将会放在Collector数据类型中输出。Context可以访问元素的时间戳，元素的key，以及TimerService时间服务。Context还可以将结果输出到别的流(side outputs)。</simpara>
</listitem>
<listitem>
<simpara><literal>onTimer(long timestamp, OnTimerContext ctx, Collector&lt;O&gt; out)</literal>：本方法是一个回调函数。当之前注册的定时器触发时调用。参数timestamp为定时器所设定的触发的时间戳。Collector为输出结果的集合。OnTimerContext和processElement的Context参数一样，提供了上下文的一些信息，例如firing trigger的时间信息(事件时间或者处理时间)。</simpara>
</listitem>
</itemizedlist>
<simpara><emphasis role="strong">时间服务和定时器</emphasis></simpara>
<simpara>Context和OnTimerContext所持有的TimerService对象拥有以下方法:</simpara>
<itemizedlist>
<listitem>
<simpara>currentProcessingTime()返回当前处理时间</simpara>
</listitem>
<listitem>
<simpara>currentWatermark()返回当前水位线的时间戳</simpara>
</listitem>
<listitem>
<simpara>registerProcessingTimeTimer(long time)会注册当前key的processing
time的timer。当processing time到达定时时间时，触发timer。</simpara>
</listitem>
<listitem>
<simpara>registerEventTimeTimer(long time)会注册当前key的event time timer。当水位线大于等于定时器注册的时间时，触发定时器执行回调函数。</simpara>
</listitem>
<listitem>
<simpara>deleteProcessingTimeTimer(long time)删除之前注册处理时间定时器。如果没有这个时间戳的定时器，则不执行。</simpara>
</listitem>
<listitem>
<simpara>deleteEventTimeTimer(long time)删除之前注册的事件时间定时器，如果没有此时间戳的定时器，则不执行。</simpara>
</listitem>
</itemizedlist>
<simpara>当定时器timer触发时，执行回调函数onTimer()。processElement()方法和onTimer()方法是同步（不是异步）方法，这样可以避免并发访问和操作状态。</simpara>
<simpara>针对每一个key和timestamp，只能注册一个定期器。也就是说，每一个key可以注册多个定时器，但在每一个时间戳只能注册一个定时器。KeyedProcessFunction默认将所有定时器的时间戳放在一个优先队列中。在Flink做检查点操作时，定时器也会被保存到状态后端中。</simpara>
<simpara>举个例子说明KeyedProcessFunction如何操作KeyedStream。</simpara>
<simpara>下面的程序展示了如何监控温度传感器的温度值，如果温度值在一秒钟之内(processing
time)连续上升，报警。</simpara>
<simpara>主程序</simpara>
<programlisting language="java" linenumbering="unnumbered">DataStream&lt;String&gt; warings = readings
    .keyBy(r -&gt; r.id)
    .process(new TempIncreaseAlertFunction());</programlisting>
<simpara>处理函数的实现如下</simpara>
<programlisting language="java" linenumbering="unnumbered">public static class TempIncreaseAlertFunction extends KeyedProcessFunction&lt;String, SensorReading, String&gt; {

    private ValueState&lt;Double&gt; lastTemp;
    private ValueState&lt;Long&gt; currentTimer;

    @Override
    public void open(Configuration parameters) throws Exception {
        super.open(parameters);
        lastTemp = getRuntimeContext().getState(
                new ValueStateDescriptor&lt;&gt;("last-temp", Types.DOUBLE)
        );
        currentTimer = getRuntimeContext().getState(
                new ValueStateDescriptor&lt;&gt;("current-timer", Types.LONG)
        );
    }

    @Override
    public void processElement(SensorReading r, Context ctx, Collector&lt;String&gt; out) throws Exception {
        // 取出上一次的温度
        Double prevTemp = 0.0;
        if (lastTemp.value() != null) {
            prevTemp = lastTemp.value();
        }
        // 将当前温度更新到上一次的温度这个变量中
        lastTemp.update(r.temperature);

        Long curTimerTimestamp = 0L;
        if (currentTimer.value() != null) {
            curTimerTimestamp = currentTimer.value();
        }
        if (prevTemp == 0.0 || r.temperature &lt; prevTemp) {
            // 温度下降或者是第一个温度值，删除定时器
            ctx.timerService().deleteProcessingTimeTimer(curTimerTimestamp);
            // 清空状态变量
            currentTimer.clear();
        } else if (r.temperature &gt; prevTemp &amp;&amp; curTimerTimestamp == 0) {
            // 温度上升且我们并没有设置定时器
            long timerTs = ctx.timerService().currentProcessingTime() + 1000L;
            ctx.timerService().registerProcessingTimeTimer(timerTs);
            // 保存定时器时间戳
            currentTimer.update(timerTs);
        }
    }

    @Override
    public void onTimer(long timestamp, OnTimerContext ctx, Collector&lt;String&gt; out) throws Exception {
        super.onTimer(timestamp, ctx, out);
        out.collect("传感器id为: "
                + ctx.getCurrentKey()
                + "的传感器温度值已经连续1s上升了。");
        currentTimer.clear();
    }
}</programlisting>
<simpara><emphasis role="strong">将事件发送到旁路输出</emphasis></simpara>
<simpara>大部分的DataStream API的算子的输出是单一输出，也就是某种数据类型的流。除了split算子，可以将一条流分成多条流，这些流的数据类型也都相同。process
function的side outputs功能可以产生多条流，并且这些流的数据类型可以不一样。一个side output可以定义为`OutputTag&lt;X&gt;`对象，X是输出流的数据类型。process function可以通过Context对象发射一个事件到一个或者多个side outputs。</simpara>
<programlisting language="java" linenumbering="unnumbered">public class SideOutputExample {

    private static OutputTag&lt;String&gt; output = new OutputTag&lt;String&gt;("side-output"){};

    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        DataStream&lt;SensorReading&gt; stream = env.addSource(new SensorSource());

        SingleOutputStreamOperator&lt;SensorReading&gt; warnings = stream
                .process(new ProcessFunction&lt;SensorReading, SensorReading&gt;() {
                    @Override
                    public void processElement(SensorReading value, Context ctx, Collector&lt;SensorReading&gt; out) throws Exception {
                        if (value.temperature &lt; 32) {
                            ctx.output(output, "温度小于32度！");
                        }
                        out.collect(value);
                    }
                });

        warnings.print();
        warnings.getSideOutput(output).print();

        env.execute();
    }
}</programlisting>
</section>
<section xml:id="_coprocessfunction的使用">
<title>CoProcessFunction的使用</title>
<simpara>对于两条输入流，DataStream
API提供了CoProcessFunction这样的low-level操作。CoProcessFunction提供了操作每一个输入流的方法:
processElement1()和processElement2()。类似于ProcessFunction，这两种方法都通过Context对象来调用。这个Context对象可以访问事件数据，定时器时间戳，TimerService，以及side
outputs。CoProcessFunction也提供了onTimer()回调函数。下面的例子展示了如何使用CoProcessFunction来合并两条流。</simpara>
<programlisting language="java" linenumbering="unnumbered">public class SensorSwitch {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        KeyedStream&lt;SensorReading, String&gt; stream = env
                .addSource(new SensorSource())
                .keyBy(r -&gt; r.id);

        KeyedStream&lt;Tuple2&lt;String, Long&gt;, String&gt; switches = env
                .fromElements(Tuple2.of("sensor_2", 10 * 1000L))
                .keyBy(r -&gt; r.f0);

        stream
                .connect(switches)
                .process(new SwitchProcess())
                .print();

        env.execute();
    }

    public static class SwitchProcess extends CoProcessFunction&lt;SensorReading, Tuple2&lt;String, Long&gt;, SensorReading&gt; {

        private ValueState&lt;Boolean&gt; forwardingEnabled;

        @Override
        public void open(Configuration parameters) throws Exception {
            super.open(parameters);
            forwardingEnabled = getRuntimeContext().getState(
                    new ValueStateDescriptor&lt;&gt;("filterSwitch", Types.BOOLEAN)
            );
        }

        @Override
        public void processElement1(SensorReading value, Context ctx, Collector&lt;SensorReading&gt; out) throws Exception {
            if (forwardingEnabled.value() != null &amp;&amp; forwardingEnabled.value()) {
                out.collect(value);
            }
        }

        @Override
        public void processElement2(Tuple2&lt;String, Long&gt; value, Context ctx, Collector&lt;SensorReading&gt; out) throws Exception {
            forwardingEnabled.update(true);
            ctx.timerService().registerProcessingTimeTimer(ctx.timerService().currentProcessingTime() + value.f1);
        }

        @Override
        public void onTimer(long timestamp, OnTimerContext ctx, Collector&lt;SensorReading&gt; out) throws Exception {
            super.onTimer(timestamp, ctx, out);
            forwardingEnabled.clear();
        }
    }
}</programlisting>
</section>
<section xml:id="_窗口聚合函数的使用">
<title>窗口聚合函数的使用</title>
<simpara>窗口聚合函数定义了窗口中数据的计算逻辑。有两种计算逻辑：</simpara>
<itemizedlist>
<listitem>
<simpara>增量聚合函数：当一个事件被添加到窗口时，触发函数计算，并且更新window的状态(单个值)。最终聚合的结果将作为输出。ReduceFunction和AggregateFunction是增量聚合函数。</simpara>
</listitem>
<listitem>
<simpara>全窗口函数：这个函数将会收集窗口中所有的元素，可以做一些复杂计算。ProcessWindowFunction是全窗口聚合函数。</simpara>
</listitem>
</itemizedlist>
<simpara><emphasis role="strong">增量聚合函数</emphasis></simpara>
<itemizedlist>
<listitem>
<simpara>每条数据到来就进行计算，只保存一个简单的状态（累加器）</simpara>
</listitem>
<listitem>
<simpara>ReduceFunction, AggregateFunction</simpara>
</listitem>
<listitem>
<simpara>当窗口闭合的时候，增量聚合完成</simpara>
</listitem>
<listitem>
<simpara>处理时间：当机器时间超过窗口结束时间的时候，窗口闭合</simpara>
</listitem>
<listitem>
<simpara>事件时间：当水位线超过窗口结束时间的时候，窗口闭合</simpara>
</listitem>
<listitem>
<simpara>来一条数据计算一次</simpara>
</listitem>
</itemizedlist>
<simpara><emphasis>REDUCE</emphasis></simpara>
<programlisting language="java" linenumbering="unnumbered">DataStream&lt;Tuple2&lt;String, Double&gt;&gt; minTempPerwindow = sensorData
    .map(new MapFunction&lt;SensorReading, Tuple2&lt;String, Double&gt;&gt;() {
        @Override
        public Tuple2&lt;String, Double&gt; map(SensorReading value) throws Exception {
            return Tuple2.of(value.id, value.temperature);
        }
    })
    .keyBy(r -&gt; r.f0)
    .window(TumblingProcessingTimeWindows.of(Time.seconds(5)))
    .reduce(new ReduceFunction&lt;Tuple2&lt;String, Double&gt;&gt;() {
        @Override
        public Tuple2&lt;String, Double&gt; reduce(Tuple2&lt;String, Double&gt; value1, Tuple2&lt;String, Double&gt; value2) throws Exception {
            if (value1.f1 &lt; value2.f1) {
                return value1;
            } else {
                return value2;
            }
        }
    })</programlisting>
<simpara><emphasis>AggregateFunction</emphasis></simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0604.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>先来看接口定义</simpara>
<programlisting language="java" linenumbering="unnumbered">public interface AggregateFunction&lt;IN, ACC, OUT&gt; extends Function, Serializable {

  // 创建新累加器
  ACC createAccumulator();

  // 累加操作的逻辑：将每一条输入元素累加到累加器上，并返回累加器
  ACC add(IN value, ACC accumulator);

  // 返回累加结果
  OUT getResult(ACC accumulator);

  // 将两个累加器聚合，只在事件时间会话窗口才会用到
  ACC merge(ACC a, ACC b);
}</programlisting>
<simpara>IN是输入元素的类型，ACC是累加器的类型，OUT是输出元素的类型。</simpara>
<simpara>例子：计算窗口平均温度</simpara>
<programlisting language="java" linenumbering="unnumbered">stream
    .keyBy(r -&gt; r.id)
    .window(TumblingProcessingTimeWindows.of(Time.seconds(5)))
    .aggregate(new AvgTemp())
    .print();

public static class AvgTemp implements AggregateFunction&lt;SensorReading, Tuple3&lt;String, Double, Long&gt;, Tuple2&lt;String, Double&gt;&gt; {
    @Override
    public Tuple3&lt;String, Double, Long&gt; createAccumulator() {
        return Tuple3.of("", 0.0, 0L);
    }

    @Override
    public Tuple3&lt;String, Double, Long&gt; add(SensorReading value, Tuple3&lt;String, Double, Long&gt; accumulator) {
        return Tuple3.of(value.id, accumulator.f1 + value.temperature, accumulator.f2 + 1);
    }

    @Override
    public Tuple2&lt;String, Double&gt; getResult(Tuple3&lt;String, Double, Long&gt; accumulator) {
        return Tuple2.of(accumulator.f0, accumulator.f1 / accumulator.f2);
    }

    @Override
    public Tuple3&lt;String, Double, Long&gt; merge(Tuple3&lt;String, Double, Long&gt; a, Tuple3&lt;String, Double, Long&gt; b) {
        return null;
    }
}</programlisting>
<simpara><emphasis role="strong">全窗口聚合函数</emphasis></simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0605.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>一些业务场景，我们需要收集窗口内所有的数据进行计算，例如计算窗口数据的中位数，或者计算窗口数据中出现频率最高的值。这样的需求，使用ReduceFunction和AggregateFunction就无法实现了。这个时候就需要ProcessWindowFunction了。</simpara>
<simpara>先来看接口定义</simpara>
<programlisting language="java" linenumbering="unnumbered">public abstract class ProcessWindowFunction&lt;IN, OUT, KEY, W extends Window&gt; extends AbstractRichFunction {

  // 对窗口进行求值
  void process(KEY key, Context ctx, Iterable&lt;IN&gt; vals, Collector&lt;OUT&gt; out)
    throws Exception;

  // Deletes any custom per-window state when the window is purged
  public void clear(Context ctx) throws Exception {}

  // 持有窗口元数据的上下文
  public abstract class Context implements Serializable {
    // Returns the metadata of the window
    public abstract W window();

    // 返回当前处理时间
    public abstract long currentProcessingTime();

    // 返回当前水位线
    public abstract long currentWatermark();

    // State accessor for per-window state
    public abstract KeyedStateStore windowState();

    // State accessor for per-key global state
    public abstract KeyedStateStore globalState();

    // Emits a record to the side output identified by the OutputTag.
    public abstract &lt;X&gt; void output(OutputTag&lt;X&gt; outputTag, X value);
  }
}</programlisting>
<simpara>process()方法接受的参数为：window的key，Iterable迭代器包含窗口的所有元素，Collector用于输出结果流。Context参数和别的process方法一样。而ProcessWindowFunction的Context对象还可以访问window的元数据(窗口开始和结束时间)，当前处理时间和水位线，per-window
state和per-key global state，side outputs。</simpara>
<itemizedlist>
<listitem>
<simpara>per-window state: 用于保存一些信息，这些信息可以被process()访问，只要process所处理的元素属于这个窗口。</simpara>
</listitem>
<listitem>
<simpara>per-key global state: 同一个key，也就是在一条KeyedStream上，不同的window可以访问per-key global state保存的值。</simpara>
</listitem>
</itemizedlist>
<simpara>例子：计算5s滚动窗口中的最低和最高的温度。输出的元素包含了(流的Key, 最低温度, 最高温度, 窗口结束时间)。</simpara>
<programlisting language="java" linenumbering="unnumbered">stream
    .keyBy(r -&gt; r.id)
    .window(TumblingProcessingTimeWindows.of(Time.seconds(5)))
    .process(new ProcessWindowFunction&lt;SensorReading, MinMaxTemp, String, TimeWindow&gt;() {
        @Override
        public void process(String s, Context context, Iterable&lt;SensorReading&gt; elements, Collector&lt;MinMaxTemp&gt; out) throws Exception {
            double min = Double.MAX_VALUE;
            double max = Double.MIN_VALUE;
            for (SensorReading element : elements) {
                double temp = element.temperature;
                if (temp &gt;= max) {
                    max = temp;
                }
                if (temp &lt; min) {
                    min = temp;
                }
            }
            out.collect(new MinMaxTemp(s, min, max, ((Long)context.window().getEnd()).toString()));
        }
    })
    .print();</programlisting>
<simpara><emphasis role="strong">增量聚合和全窗口聚合结合使用</emphasis></simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0606.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>我们还可以将ReduceFunction/AggregateFunction和ProcessWindowFunction结合起来使用。ReduceFunction/AggregateFunction做增量聚合，ProcessWindowFunction提供更多的对数据流的访问权限。如果只使用ProcessWindowFunction(底层的实现为将事件都保存在ListState中)，将会非常占用空间。分配到某个窗口的元素将被提前聚合，而当窗口的trigger触发时，也就是窗口收集完数据关闭时，将会把聚合结果发送到ProcessWindowFunction中，这时Iterable参数将会只有一个值，就是前面聚合的值。</simpara>
<simpara>我们把前面的例子用这种方式重新写一下</simpara>
<programlisting language="java" linenumbering="unnumbered">stream
    .keyBy(r -&gt; r.id)
    .window(TumblingProcessingTimeWindows.of(Time.seconds(5)))
    .aggregate(new Agg(), new Win())
    .print();

public static class Win extends ProcessWindowFunction&lt;Tuple2&lt;Double, Double&gt;, MinMaxTemp, String, TimeWindow&gt; {
    @Override
    public void process(String s, Context context, Iterable&lt;Tuple2&lt;Double, Double&gt;&gt; elements, Collector&lt;MinMaxTemp&gt; out) throws Exception {
        Tuple2&lt;Double, Double&gt; minMax = elements.iterator().next();
        out.collect(new MinMaxTemp(s, minMax.f0, minMax.f1, ((Long)context.window().getEnd()).toString()));
    }
}

public static class Agg implements AggregateFunction&lt;SensorReading, Tuple2&lt;Double, Double&gt;, Tuple2&lt;Double, Double&gt;&gt; {
    @Override
    public Tuple2&lt;Double, Double&gt; createAccumulator() {
        return Tuple2.of(Double.MAX_VALUE, Double.MIN_VALUE);
    }

    @Override
    public Tuple2&lt;Double, Double&gt; add(SensorReading value, Tuple2&lt;Double, Double&gt; accumulator) {
        if (value.temperature &gt; accumulator.f1) {
            accumulator.f1 = value.temperature;
        }
        if (value.temperature &lt; accumulator.f0) {
            accumulator.f0  = value.temperature;
        }
        return accumulator;
    }

    @Override
    public Tuple2&lt;Double, Double&gt; getResult(Tuple2&lt;Double, Double&gt; accumulator) {
        return accumulator;
    }

    @Override
    public Tuple2&lt;Double, Double&gt; merge(Tuple2&lt;Double, Double&gt; a, Tuple2&lt;Double, Double&gt; b) {
        return null;
    }
}</programlisting>
</section>
<section xml:id="_触发器">
<title>触发器</title>
<simpara>触发器定义了window何时会被求值以及何时发送求值结果。触发器可以到了特定的时间触发也可以碰到特定的事件触发。例如：观察到事件数量符合一定条件或者观察到了特定的事件。</simpara>
<simpara>默认的触发器将会在两种情况下触发：</simpara>
<itemizedlist>
<listitem>
<simpara>处理时间：机器时间到达处理时间</simpara>
</listitem>
<listitem>
<simpara>事件时间：水位线超过了窗口的结束时间</simpara>
</listitem>
</itemizedlist>
<simpara>触发器可以访问流的时间属性以及定时器，还可以对state状态编程。所以触发器和process function一样强大。例如我们可以实现一个触发逻辑：当窗口接收到一定数量的元素时，触发器触发。再比如当窗口接收到一个特定元素时，触发器触发。还有就是当窗口接收到的元素里面包含特定模式(5秒钟内接收到了两个同样类型的事件)，触发器也可以触发。在一个事件时间的窗口中，一个自定义的触发器可以提前(在水位线没过窗口结束时间之前)计算和发射计算结果。这是一个常见的低延迟计算策略，尽管计算不完全，但不像默认的那样需要等待水位线没过窗口结束时间。</simpara>
<simpara>每次调用触发器都会产生一个TriggerResult来决定窗口接下来发生什么。TriggerResult可以取以下结果：</simpara>
<itemizedlist>
<listitem>
<simpara>CONTINUE：什么都不做</simpara>
</listitem>
<listitem>
<simpara>FIRE：如果window operator有ProcessWindowFunction这个参数，将会调用这个ProcessWindowFunction。如果窗口仅有增量聚合函数(ReduceFunction或者AggregateFunction)作为参数，那么当前的聚合结果将会被发送。窗口的state不变。</simpara>
</listitem>
<listitem>
<simpara>PURGE：窗口所有内容包括窗口的元数据都将被丢弃。</simpara>
</listitem>
<listitem>
<simpara>FIRE_AND_PURGE：先对窗口进行求值，再将窗口中的内容丢弃。</simpara>
</listitem>
</itemizedlist>
<simpara>TriggerResult可能的取值使得我们可以实现很复杂的窗口逻辑。一个自定义触发器可以触发多次，可以计算或者更新结果，可以在发送结果之前清空窗口。</simpara>
<simpara>接下来我们看一下Trigger API：</simpara>
<programlisting language="java" linenumbering="unnumbered">public abstract class Trigger&lt;T, W extends Window&gt;
    implements Serializable {

  TriggerResult onElement(
    long timestamp,
    W window,
    TriggerContext ctx);

  public abstract TriggerResult onProcessingTime(
    long timestamp,
    W window,
    TriggerContext ctx);

  public abstract TriggerResult onEventTime(
    long timestamp,
    W window,
    TriggerContext ctx);

  public boolean canMerge();

  public void onMerge(W window, OnMergeContext ctx);

  public abstract void clear(W window, TriggerContext ctx);
}

public interface TriggerContext {

  long getCurrentProcessingTime();

  long getCurrentWatermark();

  void registerProcessingTimeTimer(long time);

  void registerEventTimeTimer(long time);

  void deleteProcessingTimeTimer(long time);

  void deleteEventTimeTimer(long time);

  &lt;S extends State&gt; S getPartitionedState(
    StateDescriptor&lt;S, ?&gt; stateDescriptor);
}

public interface OnMergeContext extends TriggerContext {

  void mergePartitionedState(
    StateDescriptor&lt;S, ?&gt; stateDescriptor
  );
}</programlisting>
<simpara>这里要注意两个地方：清空state和merging合并触发器。</simpara>
<simpara>当在触发器中使用per-window state时，这里我们需要保证当窗口被删除时state也要被删除，否则随着时间的推移，window operator将会积累越来越多的数据，最终可能使应用崩溃。</simpara>
<simpara>当窗口被删除时，为了清空所有状态，触发器的clear()方法需要需要删掉所有的自定义per-window state，以及使用TriggerContext对象将处理时间和事件时间的定时器都删除。</simpara>
<simpara>下面的例子展示了一个触发器在窗口结束时间之前触发。当第一个事件被分配到窗口时，这个触发器注册了一个定时器，定时时间为水位线之前一秒钟。当定时事件执行，将会注册一个新的定时事件，这样，这个触发器每秒钟最多触发一次。</simpara>
<programlisting language="java" linenumbering="unnumbered">public class TriggerExample {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);
        env.setParallelism(1);

        env
                .socketTextStream("localhost", 9999)
                .map(new MapFunction&lt;String, Tuple2&lt;String, Long&gt;&gt;() {
                    @Override
                    public Tuple2&lt;String, Long&gt; map(String s) throws Exception {
                        String[] arr = s.split(" ");
                        return Tuple2.of(arr[0], Long.parseLong(arr[1]) * 1000L);
                    }
                })
                .assignTimestampsAndWatermarks(
                        WatermarkStrategy.&lt;Tuple2&lt;String, Long&gt;&gt;forMonotonousTimestamps()
                        .withTimestampAssigner(new SerializableTimestampAssigner&lt;Tuple2&lt;String, Long&gt;&gt;() {
                            @Override
                            public long extractTimestamp(Tuple2&lt;String, Long&gt; stringLongTuple2, long l) {
                                return stringLongTuple2.f1;
                            }
                        })
                )
                .keyBy(r -&gt; r.f0)
                .timeWindow(Time.seconds(5))
                .trigger(new OneSecondIntervalTrigger())
                .process(new ProcessWindowFunction&lt;Tuple2&lt;String, Long&gt;, String, String, TimeWindow&gt;() {
                    @Override
                    public void process(String s, Context context, Iterable&lt;Tuple2&lt;String, Long&gt;&gt; iterable, Collector&lt;String&gt; collector) throws Exception {
                        long count = 0L;
                        for (Tuple2&lt;String, Long&gt; i : iterable) count += 1;
                        collector.collect("窗口中有 " + count + " 条元素");
                    }
                })
                .print();

        env.execute();
    }

    public static class OneSecondIntervalTrigger extends Trigger&lt;Tuple2&lt;String, Long&gt;, TimeWindow&gt; {
        // 来一条调用一次
        @Override
        public TriggerResult onElement(Tuple2&lt;String, Long&gt; r, long l, TimeWindow window, TriggerContext ctx) throws Exception {
            ValueState&lt;Boolean&gt; firstSeen = ctx.getPartitionedState(
                    new ValueStateDescriptor&lt;Boolean&gt;("first-seen", Types.BOOLEAN)
            );

            if (firstSeen.value() == null) {
                // 4999 + (1000 - 4999 % 1000) = 5000
                System.out.println("第一条数据来的时候 ctx.getCurrentWatermark() 的值是 " + ctx.getCurrentWatermark());
                long t = ctx.getCurrentWatermark() + (1000L - ctx.getCurrentWatermark() % 1000L);
                ctx.registerEventTimeTimer(t);
                ctx.registerEventTimeTimer(window.getEnd());
                firstSeen.update(true);
            }
            return TriggerResult.CONTINUE;
        }

        // 定时器逻辑
        @Override
        public TriggerResult onEventTime(long ts, TimeWindow window, TriggerContext ctx) throws Exception {
            if (ts == window.getEnd()) {
                return TriggerResult.FIRE_AND_PURGE;
            } else {
                System.out.println("当前水位线是：" + ctx.getCurrentWatermark());
                long t = ctx.getCurrentWatermark() + (1000L - ctx.getCurrentWatermark() % 1000L);
                if (t &lt; window.getEnd()) {
                    ctx.registerEventTimeTimer(t);
                }
                return TriggerResult.FIRE;
            }
        }

        @Override
        public TriggerResult onProcessingTime(long l, TimeWindow timeWindow, TriggerContext triggerContext) throws Exception {
            return TriggerResult.CONTINUE;
        }

        @Override
        public void clear(TimeWindow timeWindow, TriggerContext ctx) throws Exception {
            ValueState&lt;Boolean&gt; firstSeen = ctx.getPartitionedState(
                    new ValueStateDescriptor&lt;Boolean&gt;("first-seen", Types.BOOLEAN)
            );
            firstSeen.clear();
        }
    }
}</programlisting>
</section>
</section>
<section xml:id="_基于时间的双流join">
<title>基于时间的双流Join</title>
<simpara>数据流操作的另一个常见需求是对两条数据流中的事件进行联结（connect）或Join。Flink
DataStream
API中内置有两个可以根据时间条件对数据流进行Join的算子：基于间隔的Join和基于窗口的Join。本节我们会对它们进行介绍。</simpara>
<simpara>如果Flink内置的Join算子无法表达所需的Join语义，那么你可以通过CoProcessFunction、BroadcastProcessFunction或KeyedBroadcastProcessFunction实现自定义的Join逻辑。</simpara>
<blockquote>
<simpara>注意，你要设计的Join算子需要具备高效的状态访问模式及有效的状态清理策略。</simpara>
</blockquote>
<section xml:id="_基于间隔的join">
<title>基于间隔的Join</title>
<simpara>基于间隔的Join会对两条流中拥有相同键值以及彼此之间时间戳不超过某一指定间隔的事件进行Join。</simpara>
<simpara>下图展示了两条流（A和B）上基于间隔的Join，如果B中事件的时间戳相较于A中事件的时间戳不早于1小时且不晚于15分钟，则会将两个事件Join起来。Join间隔具有对称性，因此上面的条件也可以表示为A中事件的时间戳相较B中事件的时间戳不早于15分钟且不晚于1小时。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0607.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>基于间隔的Join目前只支持事件时间以及INNER
JOIN语义（无法发出未匹配成功的事件）。下面的例子定义了一个基于间隔的Join。</simpara>
<programlisting language="java" linenumbering="unnumbered">input1
  .intervalJoin(input2)
  .between(&lt;lower-bound&gt;, &lt;upper-bound&gt;) // 相对于input1的上下界
  .process(ProcessJoinFunction) // 处理匹配的事件对</programlisting>
<simpara>Join成功的事件对会发送给ProcessJoinFunction。下界和上界分别由负时间间隔和正时间间隔来定义，例如between(Time.hour(-1),
Time.minute(15))。在满足下界值小于上界值的前提下，你可以任意对它们赋值。例如，允许出现B中事件的时间戳相较A中事件的时间戳早1～2小时这样的条件。</simpara>
<simpara>基于间隔的Join需要同时对双流的记录进行缓冲。对第一个输入而言，所有时间戳大于当前水位线减去间隔上界的数据都会被缓冲起来；对第二个输入而言，所有时间戳大于当前水位线加上间隔下界的数据都会被缓冲起来。注意，两侧边界值都有可能为负。上图中的Join需要存储数据流A中所有时间戳大于当前水位线减去15分钟的记录，以及数据流B中所有时间戳大于当前水位线减去1小时的记录。不难想象，如果两条流的事件时间不同步，那么Join所需的存储就会显著增加，因为水位线总是由<literal>较慢</literal>的那条流来决定。</simpara>
<simpara>例子：每个用户的点击Join这个用户最近10分钟内的浏览</simpara>
<programlisting language="java" linenumbering="unnumbered">public class IntervalJoinExample {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        KeyedStream&lt;Tuple3&lt;String, Long, String&gt;, String&gt; stream1 = env
            .fromElements(
                Tuple3.of("user_1", 10 * 60 * 1000L, "click")
            )
            .assignTimestampsAndWatermarks(
                WatermarkStrategy
                    .&lt;Tuple3&lt;String, Long, String&gt;&gt;forMonotonousTimestamps()
                    .withTimestampAssigner(new SerializableTimestampAssigner&lt;Tuple3&lt;String, Long, String&gt;&gt;() {
                        @Override
                        public long extractTimestamp(Tuple3&lt;String, Long, String&gt; stringLongStringTuple3, long l) {
                            return stringLongStringTuple3.f1;
                        }
                    })
            )
            .keyBy(r -&gt; r.f0);

        KeyedStream&lt;Tuple3&lt;String, Long, String&gt;, String&gt; stream2 = env
            .fromElements(
                Tuple3.of("user_1", 5 * 60 * 1000L, "browse"),
                Tuple3.of("user_1", 6 * 60 * 1000L, "browse")
            )
            .assignTimestampsAndWatermarks(
                WatermarkStrategy
                    .&lt;Tuple3&lt;String, Long, String&gt;&gt;forMonotonousTimestamps()
                    .withTimestampAssigner(new SerializableTimestampAssigner&lt;Tuple3&lt;String, Long, String&gt;&gt;() {
                        @Override
                        public long extractTimestamp(Tuple3&lt;String, Long, String&gt; stringLongStringTuple3, long l) {
                            return stringLongStringTuple3.f1;
                        }
                    })
            )
            .keyBy(r -&gt; r.f0);

        stream1
            .intervalJoin(stream2)
            .between(Time.minutes(-10), Time.minutes(0))
            .process(new ProcessJoinFunction&lt;Tuple3&lt;String, Long, String&gt;, Tuple3&lt;String, Long, String&gt;, String&gt;() {
                @Override
                public void processElement(Tuple3&lt;String, Long, String&gt; stringLongStringTuple3, Tuple3&lt;String, Long, String&gt; stringLongStringTuple32, Context context, Collector&lt;String&gt; collector) throws Exception {
                    collector.collect(stringLongStringTuple3 + " =&gt; " + stringLongStringTuple32);
                }
            })
            .print();

        env.execute();

    }
}</programlisting>
</section>
<section xml:id="_基于窗口的join">
<title>基于窗口的Join</title>
<simpara>顾名思义，基于窗口的Join需要用到Flink中的窗口机制。其原理是将两条输入流中的元素分配到公共窗口中并在窗口完成时进行Join（或Cogroup）。</simpara>
<simpara>下面的例子展示了如何定义基于窗口的Join。</simpara>
<programlisting language="java" linenumbering="unnumbered">input1.join(input2)
  .where(...)       // 为input1指定键值属性
  .equalTo(...)     // 为input2指定键值属性
  .window(...)      // 指定WindowAssigner
  [.trigger(...)]   // 选择性的指定Trigger
  [.evictor(...)]   // 选择性的指定Evictor
  .apply(...)       // 指定JoinFunction</programlisting>
<simpara>下图展示了DataStream API中基于窗口的Join是如何工作的。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0608.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>两条输入流都会根据各自的键值属性进行分区，公共窗口分配器会将二者的事件映射到公共窗口内（其中同时存储了两条流中的数据）。当窗口的计时器触发时，算子会遍历两个输入中元素的每个组合（叉乘积）去调用JoinFunction。同时你也可以自定义触发器或移除器。由于两条流中的事件会被映射到同一个窗口中，因此该过程中的触发器和移除器与常规窗口算子中的完全相同。</simpara>
<simpara>除了对窗口中的两条流进行Join，你还可以对它们进行Cogroup，只需将算子定义开始位置的join改为coGroup()即可。Join和Cogroup的总体逻辑相同，二者的唯一区别是：Join会为两侧输入中的每个事件对调用JoinFunction；而Cogroup中用到的CoGroupFunction会以两个输入的元素遍历器为参数，只在每个窗口中被调用一次。</simpara>
<blockquote>
<simpara>注意，对划分窗口后的数据流进行Join可能会产生意想不到的语义。例如，假设你为执行Join操作的算子配置了1小时的滚动窗口，那么一旦来自两个输入的元素没有被划分到同一窗口，它们就无法Join在一起，即使二者彼此仅相差1秒钟。</simpara>
</blockquote>
<programlisting language="java" linenumbering="unnumbered">public class TwoWindowJoinExample {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        DataStream&lt;Tuple2&lt;String, Long&gt;&gt; stream1 = env
            .fromElements(
                Tuple2.of("a", 1000L),
                Tuple2.of("b", 1000L),
                Tuple2.of("a", 2000L),
                Tuple2.of("b", 2000L)
            )
            .assignTimestampsAndWatermarks(
                WatermarkStrategy
                    .&lt;Tuple2&lt;String, Long&gt;&gt;forMonotonousTimestamps()
                    .withTimestampAssigner(
                        new SerializableTimestampAssigner&lt;Tuple2&lt;String, Long&gt;&gt;() {
                            @Override
                            public long extractTimestamp(Tuple2&lt;String, Long&gt; stringLongTuple2, long l) {
                                return stringLongTuple2.f1;
                            }
                        }
                    )
            );

        DataStream&lt;Tuple2&lt;String, Long&gt;&gt; stream2 = env
            .fromElements(
                Tuple2.of("a", 3000L),
                Tuple2.of("b", 3000L),
                Tuple2.of("a", 4000L),
                Tuple2.of("b", 4000L)
            )
            .assignTimestampsAndWatermarks(
                WatermarkStrategy
                    .&lt;Tuple2&lt;String, Long&gt;&gt;forMonotonousTimestamps()
                    .withTimestampAssigner(
                        new SerializableTimestampAssigner&lt;Tuple2&lt;String, Long&gt;&gt;() {
                            @Override
                            public long extractTimestamp(Tuple2&lt;String, Long&gt; stringLongTuple2, long l) {
                                return stringLongTuple2.f1;
                            }
                        }
                    )
            );

        stream1
            .join(stream2)
            .where(r -&gt; r.f0)
            .equalTo(r -&gt; r.f0)
            .window(TumblingEventTimeWindows.of(Time.seconds(5)))
            .apply(new JoinFunction&lt;Tuple2&lt;String, Long&gt;, Tuple2&lt;String, Long&gt;, String&gt;() {
                @Override
                public String join(Tuple2&lt;String, Long&gt; stringLongTuple2, Tuple2&lt;String, Long&gt; stringLongTuple22) throws Exception {
                    return stringLongTuple2 + " =&gt; " + stringLongTuple22;
                }
            })
            .print();

        env.execute();
    }
}</programlisting>
</section>
</section>
<section xml:id="_处理迟到的元素">
<title>处理迟到的元素</title>
<simpara>水位线可以用来平衡计算的完整性和延迟两方面。除非我们选择一种非常保守的水位线策略(最大延时设置的非常大，以至于包含了所有的元素，但结果是非常大的延迟)，否则我们总需要处理迟到的元素。</simpara>
<simpara>迟到的元素是指当这个元素来到时，这个元素所对应的窗口已经计算完毕了(也就是说水位线已经没过窗口结束时间了)。这说明迟到这个特性只针对事件时间。</simpara>
<simpara>DataStream API提供了三种策略来处理迟到元素</simpara>
<itemizedlist>
<listitem>
<simpara>直接抛弃迟到的元素</simpara>
</listitem>
<listitem>
<simpara>将迟到的元素发送到另一条流中去</simpara>
</listitem>
<listitem>
<simpara>可以更新窗口已经计算完的结果，并发出计算结果。</simpara>
</listitem>
</itemizedlist>
<section xml:id="_抛弃迟到元素">
<title>抛弃迟到元素</title>
<simpara>抛弃迟到的元素是事件时间窗口操作符的默认行为。也就是说一个迟到的元素不会创建一个新的窗口。</simpara>
<simpara>process function可以通过比较迟到元素的时间戳和当前水位线的大小来很轻易的过滤掉迟到元素。</simpara>
</section>
<section xml:id="_重定向迟到元素">
<title>重定向迟到元素</title>
<simpara>迟到的元素也可以使用旁路输出(side
output)特性被重定向到另外的一条流中去。迟到元素所组成的旁路输出流可以继续处理或者sink到持久化设施中去。</simpara>
<simpara>例子</simpara>
<programlisting language="java" linenumbering="unnumbered">public class RedirectLateEvent {

    private static OutputTag&lt;Tuple2&lt;String, Long&gt;&gt; output = new OutputTag&lt;Tuple2&lt;String, Long&gt;&gt;("late-readings"){};

    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        DataStream&lt;Tuple2&lt;String, Long&gt;&gt; stream = env
            .socketTextStream("localhost", 9999)
            .map(new MapFunction&lt;String, Tuple2&lt;String, Long&gt;&gt;() {
                @Override
                public Tuple2&lt;String, Long&gt; map(String s) throws Exception {
                    String[] arr = s.split(" ");
                    return Tuple2.of(arr[0], Long.parseLong(arr[1]) * 1000L);
                }
            })
            .assignTimestampsAndWatermarks(
                WatermarkStrategy.
                    // like scala: assignAscendingTimestamps(_._2)
                    &lt;Tuple2&lt;String, Long&gt;&gt;forMonotonousTimestamps()
                    .withTimestampAssigner(new SerializableTimestampAssigner&lt;Tuple2&lt;String, Long&gt;&gt;() {
                        @Override
                        public long extractTimestamp(Tuple2&lt;String, Long&gt; value, long l) {
                            return value.f1;
                        }
                    })
            );

        SingleOutputStreamOperator&lt;String&gt; lateReadings = stream
            .keyBy(r -&gt; r.f0)
            .window(TumblingEventTimeWindows.of(Time.seconds(5)))
            .sideOutputLateData(output) // use after keyBy and timeWindow
            .process(new ProcessWindowFunction&lt;Tuple2&lt;String, Long&gt;, String, String, TimeWindow&gt;() {
                @Override
                public void process(String s, Context context, Iterable&lt;Tuple2&lt;String, Long&gt;&gt; iterable, Collector&lt;String&gt; collector) throws Exception {
                    long exactSizeIfKnown = iterable.spliterator().getExactSizeIfKnown();
                    collector.collect(exactSizeIfKnown + " of elements");
                }
            });

        lateReadings.print();
        lateReadings.getSideOutput(output).print();

        env.execute();
    }
}</programlisting>
<simpara>下面这个例子展示了ProcessFunction如何过滤掉迟到的元素然后将迟到的元素发送到旁路输出流中去。</simpara>
<programlisting language="java" linenumbering="unnumbered">public class RedirectLateEvent {

    private static OutputTag&lt;String&gt; output = new OutputTag&lt;String&gt;("late-readings"){};

    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        SingleOutputStreamOperator&lt;Tuple2&lt;String, Long&gt;&gt; stream = env
            .socketTextStream("localhost", 9999)
            .map(new MapFunction&lt;String, Tuple2&lt;String, Long&gt;&gt;() {
                @Override
                public Tuple2&lt;String, Long&gt; map(String s) throws Exception {
                    String[] arr = s.split(" ");
                    return Tuple2.of(arr[0], Long.parseLong(arr[1]) * 1000L);
                }
            })
            .assignTimestampsAndWatermarks(
                WatermarkStrategy.
                    &lt;Tuple2&lt;String, Long&gt;&gt;forMonotonousTimestamps()
                    .withTimestampAssigner(new SerializableTimestampAssigner&lt;Tuple2&lt;String, Long&gt;&gt;() {
                        @Override
                        public long extractTimestamp(Tuple2&lt;String, Long&gt; value, long l) {
                            return value.f1;
                        }
                    })
            )
            .process(new ProcessFunction&lt;Tuple2&lt;String, Long&gt;, Tuple2&lt;String, Long&gt;&gt;() {
                @Override
                public void processElement(Tuple2&lt;String, Long&gt; stringLongTuple2, Context context, Collector&lt;Tuple2&lt;String, Long&gt;&gt; collector) throws Exception {
                    if (stringLongTuple2.f1 &lt; context.timerService().currentWatermark()) {
                        context.output(output, "late event is comming!");
                    } else {
                        collector.collect(stringLongTuple2);
                    }

                }
            });

        stream.print();
        stream.getSideOutput(output).print();

        env.execute();
    }
}</programlisting>
</section>
<section xml:id="_使用迟到元素更新窗口计算结果">
<title>使用迟到元素更新窗口计算结果</title>
<simpara>由于存在迟到的元素，所以已经计算出的窗口结果是不准确和不完全的。我们可以使用迟到元素更新已经计算完的窗口结果。</simpara>
<simpara>如果我们要求一个operator支持重新计算和更新已经发出的结果，就需要在第一次发出结果以后也要保存之前所有的状态。但显然我们不能一直保存所有的状态，肯定会在某一个时间点将状态清空，而一旦状态被清空，结果就再也不能重新计算或者更新了。而迟到的元素只能被抛弃或者发送到旁路输出流。</simpara>
<simpara>window operator
API提供了方法来明确声明我们要等待迟到元素。当使用event-time
window，我们可以指定一个时间段叫做allowed lateness。window
operator如果设置了allowed lateness，这个window
operator在水位线没过窗口结束时间时也将不会删除窗口和窗口中的状态。窗口会在一段时间内(allowed
lateness设置的)保留所有的元素。</simpara>
<simpara>当迟到元素在allowed
lateness时间内到达时，这个迟到元素会被实时处理并发送到触发器(trigger)。当水位线没过了窗口结束时间+allowed
lateness时间时，窗口会被删除，并且所有后来的迟到的元素都会被丢弃。</simpara>
<simpara>Allowed lateness可以使用allowedLateness()方法来指定，如下所示：</simpara>
<programlisting language="java" linenumbering="unnumbered">public class UpdateWindowResultWithLateEvent {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        DataStreamSource&lt;String&gt; stream = env.socketTextStream("localhost", 9999);

        stream
            .map(new MapFunction&lt;String, Tuple2&lt;String, Long&gt;&gt;() {
                @Override
                public Tuple2&lt;String, Long&gt; map(String s) throws Exception {
                    String[] arr = s.split(" ");
                    return Tuple2.of(arr[0], Long.parseLong(arr[1]) * 1000L);
                }
            })
            .assignTimestampsAndWatermarks(
                WatermarkStrategy.&lt;Tuple2&lt;String, Long&gt;&gt;forBoundedOutOfOrderness(Duration.ofSeconds(5))
                .withTimestampAssigner(new SerializableTimestampAssigner&lt;Tuple2&lt;String, Long&gt;&gt;() {
                    @Override
                    public long extractTimestamp(Tuple2&lt;String, Long&gt; stringLongTuple2, long l) {
                        return stringLongTuple2.f1;
                    }
                })
            )
            .keyBy(r -&gt; r.f0)
            .window(TumblingEventTimeWindows.of(Time.seconds(5)))
            .allowedLateness(Time.seconds(5))
            .process(new UpdateWindowResult())
            .print();

        env.execute();
    }

    public static class UpdateWindowResult extends ProcessWindowFunction&lt;Tuple2&lt;String, Long&gt;, String, String, TimeWindow&gt; {
        @Override
        public void process(String s, Context context, Iterable&lt;Tuple2&lt;String, Long&gt;&gt; iterable, Collector&lt;String&gt; collector) throws Exception {
            long count = 0L;
            for (Tuple2&lt;String, Long&gt; i : iterable) {
                count += 1;
            }

            // 可见范围比getRuntimeContext.getState更小，只对当前key、当前window可见
            // 基于窗口的状态变量，只能当前key和当前窗口访问
            ValueState&lt;Boolean&gt; isUpdate = context.windowState().getState(
                    new ValueStateDescriptor&lt;Boolean&gt;("isUpdate", Types.BOOLEAN)
            );

            // 当水位线超过窗口结束时间时，触发窗口的第一次计算！
            if (isUpdate.value() == null) {
                collector.collect("窗口第一次触发计算！一共有 " + count + " 条数据！");
                isUpdate.update(true);
            } else {
                collector.collect("窗口更新了！一共有 " + count + " 条数据！");
            }
        }
    }
}</programlisting>
</section>
</section>
</section>
<section xml:id="_有状态算子和应用">
<title>有状态算子和应用</title>
<simpara>状态操作符和用户自定义函数都是我们在写流处理程序时，常用的工具。事实上，大部分稍微复杂一点的逻辑都需要保存数据或者保存计算结果。很多Flink内置的操作符例如：source操作符，sink操作符等等都是有状态的，也就是说会缓存流数据或者计算结果。例如，窗口操作符将会为ProcessWindowFunction收集输入的数据，或者收集ReduceFunction计算的结果。而ProcessFunction也会保存定时器事件，一些sink方法为了做到exactly-once，会将事务保存下来。除了内置的操作符以及提供的source和sink操作符，Flink的DataStream
API还在UDF函数中暴露了可以注册、保存和访问状态的接口。</simpara>
<simpara>本章重点讨论有状态的用户自定义函数的实现，以及讨论有状态应用的性能和健壮性。特别的，我们将解释在用户自定义函数中，如何定义不同类型的状态，以及如何与状态进行交互。我们还讨论了性能方面的问题以及如何控制状态大小的问题。</simpara>
<section xml:id="_实现有状态的用户自定义函数">
<title>实现有状态的用户自定义函数</title>
<simpara>我们知道函数有两种状态，键控状态(keyed state)和操作符状态(operator
state)。我们重点来看一下键控状态。我们来看一下如何在RuntimeContext中定义键控状态。</simpara>
<simpara>用户自定义函数可以使用keyed
state来存储和访问key对应的状态。对于每一个key，Flink将会维护一个状态实例。一个操作符的状态实例将会被分发到操作符的所有并行任务中去。这表明函数的每一个并行任务只为所有key的某一部分key保存key对应的状态实例。所以keyed
state和分布式key-value map数据结构非常类似。</simpara>
<simpara>keyed state仅可用于KeyedStream。Flink支持以下数据类型的状态变量：</simpara>
<itemizedlist>
<listitem>
<simpara>`ValueState&lt;T&gt;`保存单个的值，值的类型为T。</simpara>
<itemizedlist>
<listitem>
<simpara>get操作: <literal>ValueState.value()</literal></simpara>
</listitem>
<listitem>
<simpara>set操作: <literal>ValueState.update(T value)</literal></simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>`ListState&lt;T&gt;`保存一个列表，列表里的元素的数据类型为T。基本操作如下：</simpara>
<itemizedlist>
<listitem>
<simpara><literal>ListState.add(T value)</literal></simpara>
</listitem>
<listitem>
<simpara><literal>ListState.addAll(List&lt;T&gt; values)</literal></simpara>
</listitem>
<listitem>
<simpara><literal>ListState.get()返回Iterable&lt;T&gt;</literal></simpara>
</listitem>
<listitem>
<simpara><literal>ListState.update(List&lt;T&gt; values)</literal></simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>`MapState&lt;K, V&gt;`保存Key-Value对。</simpara>
<itemizedlist>
<listitem>
<simpara><literal>MapState.get(K key)</literal></simpara>
</listitem>
<listitem>
<simpara><literal>MapState.put(K key, V value)</literal></simpara>
</listitem>
<listitem>
<simpara><literal>MapState.contains(K key)</literal></simpara>
</listitem>
<listitem>
<simpara><literal>MapState.remove(K key)</literal></simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><literal>ReducingState&lt;T&gt;</literal></simpara>
</listitem>
<listitem>
<simpara><literal>AggregatingState&lt;I, O&gt;</literal></simpara>
</listitem>
</itemizedlist>
<simpara>State.clear()是清空操作。</simpara>
<programlisting language="java" linenumbering="unnumbered">public class ListStateExample {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        env
            .addSource(new SensorSource())
            .filter(r -&gt; r.id.equals("sensor_1"))
            .keyBy(r -&gt; r.id)
            .process(new KeyedProcessFunction&lt;String, SensorReading, String&gt;() {

                private ListState&lt;SensorReading&gt; readings;
                private ValueState&lt;Long&gt; timerTs;

                @Override
                public void open(Configuration parameters) throws Exception {
                    super.open(parameters);
                    readings = getRuntimeContext().getListState(
                            new ListStateDescriptor&lt;SensorReading&gt;("readings", SensorReading.class)
                    );
                    timerTs = getRuntimeContext().getState(
                            new ValueStateDescriptor&lt;Long&gt;("ts", Types.LONG)
                    );
                }

                @Override
                public void processElement(SensorReading sensorReading, Context context, Collector&lt;String&gt; collector) throws Exception {
                    readings.add(sensorReading);
                    if (timerTs.value() == null) {
                        context.timerService().registerProcessingTimeTimer(context.timerService().currentProcessingTime() + 10 * 1000L);
                        timerTs.update(context.timerService().currentProcessingTime() + 10 * 1000L);
                    }
                }

                @Override
                public void onTimer(long timestamp, OnTimerContext ctx, Collector&lt;String&gt; out) throws Exception {
                    super.onTimer(timestamp, ctx, out);
                    long count = 0L;
                    for(SensorReading r : readings.get()) {
                        count++;
                    }
                    out.collect("there are " + count + " readings");
                    timerTs.clear();
                }
            })
            .print();

        env.execute();
    }
}</programlisting>
<simpara>上面例子中的KeyedProcessFunction只能访问当前处理的元素所包含的key所对应的状态变量。</simpara>
<blockquote>
<simpara>不同key对应的keyed state是相互隔离的。</simpara>
</blockquote>
<itemizedlist>
<listitem>
<simpara>通过RuntimeContext注册StateDescriptor。StateDescriptor以状态state的名字和存储的数据类型为参数。数据类型必须指定，因为Flink需要选择合适的序列化器。</simpara>
</listitem>
<listitem>
<simpara>在open()方法中创建state变量。注意复习之前的RichFunction相关知识。</simpara>
</listitem>
</itemizedlist>
<simpara>当一个函数注册了StateDescriptor描述符，Flink会检查状态后端是否已经存在这个状态。这种情况通常出现在应用挂掉要从检查点或者保存点恢复的时候。在这两种情况下，Flink会将注册的状态连接到已经存在的状态。如果不存在状态，则初始化一个空的状态。</simpara>
</section>
<section xml:id="_状态后端_2">
<title>状态后端</title>
<section xml:id="_选择一个状态后端">
<title>选择一个状态后端</title>
<itemizedlist>
<listitem>
<simpara>MemoryStateBackend将状态当作Java的对象(没有序列化操作)存储在TaskManager
JVM进程的堆上。</simpara>
</listitem>
<listitem>
<simpara>FsStateBackend将状态存储在本地的文件系统或者远程的文件系统如HDFS。</simpara>
</listitem>
<listitem>
<simpara>RocksDBStateBackend将状态存储在RocksDB中。</simpara>
</listitem>
</itemizedlist>
<programlisting language="java" linenumbering="unnumbered">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
env.setStateBackend(new FsStateBackend("file:///tmp/checkpoints", false))</programlisting>
</section>
<section xml:id="_防止状态泄露">
<title>防止状态泄露</title>
<simpara>流应用通常需要运行几个月或者几年。如果state数据不断增长的话，会爆炸。所以控制state数据的大小十分重要。而Flink并不会清理state和gc。所以所有的stateful
operator都需要控制他们各自的状态数据大小，保证不爆炸。</simpara>
<simpara>例如我们之前讲过增量聚合函数ReduceFunction/AggregateFunction，就可以提前聚合而不给state太多压力。</simpara>
<simpara>我们来看一个例子，我们实现了一个KeyedProcessFunction，用来计算连续两次的温度的差值，如果差值超过阈值，报警。</simpara>
<simpara>我们之前实现过这个需求，但没有清理掉状态数据。比如一小时内不再产生温度数据的传感器对应的状态数据就可以清理掉了。</simpara>
</section>
<section xml:id="_配置检查点">
<title>配置检查点</title>
<simpara>10秒钟保存一次检查点。</simpara>
<programlisting language="java" linenumbering="unnumbered">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
env.enableCheckpointing(10000L);</programlisting>
</section>
</section>
</section>
<section xml:id="_读写外部系统">
<title>读写外部系统</title>
<section xml:id="_自定义数据源函数">
<title>自定义数据源函数</title>
<section xml:id="_sourcefunction">
<title>SourceFunction</title>
<programlisting language="java" linenumbering="unnumbered">import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.functions.source.SourceFunction;

import java.util.Random;

public class RandomLongSource {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        env.addSource(new CustomSource()).print();

        env.execute();
    }

    public static class CustomSource implements SourceFunction&lt;Long&gt; {
        private boolean running = true;
        @Override
        public void run(SourceContext&lt;Long&gt; sourceContext) throws Exception {
            Random random = new Random();

            while (running) {
                sourceContext.collect(random.nextLong());
                Thread.sleep(1000L);
            }
        }

        @Override
        public void cancel() {
            running = false;
        }
    }
}</programlisting>
<itemizedlist>
<listitem>
<simpara>RichSourceFunction: 富函数版本</simpara>
</listitem>
<listitem>
<simpara>ParellelSourceFunction: 并行版本，可以设置多个并行度</simpara>
</listitem>
<listitem>
<simpara>RichParellelSourceFunction: 并行数据源函数的富函数版本</simpara>
</listitem>
</itemizedlist>
<simpara>我们可以使用SourceFunction来发送水位线。</simpara>
<note>
<simpara>使用SourceFunction设置水位线以后，后面不能再使用assignTimestampsAndWatermarks设置水位线，二选一。</simpara>
</note>
<simpara>数据可以存储在不同的系统中，例如：文件系统，对象存储系统（OSS），关系型数据库，Key-Value存储，搜索引擎索引，日志系统，消息队列，等等。每一种系统都是给特定的应用场景设计的，在某一个特定的目标上超越了其他系统。今天的数据架构，往往包含着很多不同的存储系统。在将一个组件加入到我们的系统中时，我们需要问一个问题：<literal>这个组件和架构中的其他组件能多好的一起工作？</literal></simpara>
<simpara>添加一个像Flink这样的数据处理系统，需要仔细的考虑。因为Flink没有自己的存储层，而是读取数据和持久化数据都需要依赖外部存储。所以，对于Flink，针对外部系统提供良好的读取和写入的连接器就很重要了。尽管如此，仅仅能够读写外部系统对于Flink这样想要提供任务故障情况下一致性保证的流处理器来讲，是不够的。</simpara>
<simpara>在本章中，我们将会讨论source和sink的连接器。这些连接器影响了Flink的一致性保证，也提供了对于最流行的一些外部系统的读写的连接器。我们还将学习如何实现自定义source和sink连接器，以及如何实现可以向外部系统发送异步读写请求的函数。</simpara>
</section>
</section>
<section xml:id="_应用的一致性保证">
<title>应用的一致性保证</title>
<simpara>Flink的检查点和恢复机制定期的会保存应用程序状态的一致性检查点。在故障的情况下，应用程序的状态将会从最近一次完成的检查点恢复，并继续处理。尽管如此，可以使用检查点来重置应用程序的状态无法完全达到令人满意的一致性保证。相反，source和sink的连接器需要和Flink的检查点和恢复机制进行集成才能提供有意义的一致性保证。</simpara>
<simpara>为了给应用程序提供恰好处理一次语义的状态一致性保证，应用程序的source连接器需要能够将source的读位置重置到之前保存的检查点位置。当处理一次检查点时，source操作符将会把source的读位置持久化，并在恢复的时候从这些读位置开始重新读取。支持读位置的检查点的source连接器一般来说是基于文件的存储系统，如：文件流或者Kafka
source（检查点会持久化某个正在消费的topic的读偏移量）。如果一个应用程序从一个无法存储和重置读位置的source连接器摄入数据，那么当任务出现故障的时候，数据就会丢失。也就是说我们只能提供at-most-once）的一致性保证。</simpara>
<simpara>Fink的检查点和恢复机制和可以重置读位置的source连接器结合使用，可以保证应用程序不会丢失任何数据。尽管如此，应用程序可能会发出两次计算结果，因为从上一次检查点恢复的应用程序所计算的结果将会被重新发送一次（一些结果已经发送出去了，这时任务故障，然后从上一次检查点恢复，这些结果将被重新计算一次然后发送出去）。所以，可重置读位置的source和Flink的恢复机制不足以提供端到端的恰好处理一次语义，即使应用程序的状态是恰好处理一次一致性级别。</simpara>
<simpara>一个志在提供端到端恰好处理一次语义一致性的应用程序需要特殊的sink连接器。sink连接器可以在不同的情况下使用两种技术来达到恰好处理一次一致性语义：幂等性写入和事务性写入。</simpara>
<section xml:id="_幂等性写入">
<title>幂等性写入</title>
<simpara>一个幂等操作无论执行多少次都会返回同样的结果。例如，重复的向hashmap中插入同样的key-value对就是幂等操作，因为头一次插入操作之后所有的插入操作都不会改变这个hashmap，因为hashmap已经包含这个key-value对了。另一方面，append操作就不是幂等操作了，因为多次append同一个元素将会导致列表每次都会添加一个元素。在流处理程序中，幂等写入操作是很有意思的，因为幂等写入操作可以执行多次但不改变结果。所以它们可以在某种程度上缓和Flink检查点机制带来的重播计算结果的效应。</simpara>
<simpara>需要注意的是，依赖于幂等性sink来达到exactly-once语义的应用程序，必须保证在从检查点恢复以后，它将会覆盖之前已经写入的结果。例如，一个包含有sink操作的应用在sink到一个key-value存储时必须保证它能够确定的计算出将要更新的key值。同时，从Flink程序sink到的key-value存储中读取数据的应用，在Flink从检查点恢复的过程中，可能会看到不想看到的结果。当重播开始时，之前已经发出的计算结果可能会被更早的结果所覆盖（因为在恢复过程中）。所以，一个消费Flink程序输出数据的应用，可能会观察到时间回退，例如读到了比之前小的计数。也就是说，当流处理程序处于恢复过程中时，流处理程序的结果将处于不稳定的状态，因为一些结果被覆盖掉，而另一些结果还没有被覆盖。一旦重播完成，也就是说应用程序已经通过了之前出故障的点，结果将会继续保持一致性。</simpara>
</section>
<section xml:id="_事务性写入">
<title>事务性写入</title>
<simpara>第二种实现端到端的恰好处理一次一致性语义的方法基于事务性写入。其思想是只将最近一次成功保存的检查点之前的计算结果写入到外部系统中去。这样就保证了在任务故障的情况下，端到端恰好处理一次语义。应用将被重置到最近一次的检查点，而在这个检查点之后并没有向外部系统发出任何计算结果。通过只有当检查点保存完成以后再写入数据这种方法，事务性的方法将不会遭受幂等性写入所遭受的重播不一致的问题。尽管如此，事务性写入却带来了延迟，因为只有在检查点完成以后，我们才能看到计算结果。</simpara>
<simpara>Flink提供了两种构建模块来实现事务性sink连接器：write-ahead-log（WAL，预写式日志）sink和两阶段提交sink。WAL式sink将会把所有计算结果写入到应用程序的状态中，等接到检查点完成的通知，才会将计算结果发送到sink系统。因为sink操作会把数据都缓存在状态后段，所以WAL可以使用在任何外部sink系统上。尽管如此，WAL还是无法提供刀枪不入的恰好处理一次语义的保证，再加上由于要缓存数据带来的状态后段的状态大小的问题，WAL模型并不十分完美。</simpara>
<simpara>与之形成对比的，2PC
sink需要sink系统提供事务的支持或者可以模拟出事务特性的模块。对于每一个检查点，sink开始一个事务，然后将所有的接收到的数据都添加到事务中，并将这些数据写入到sink系统，但并没有提交（commit）它们。当事务接收到检查点完成的通知时，事务将被commit，数据将被真正的写入sink系统。这项机制主要依赖于一次sink可以在检查点完成之前开始事务，并在应用程序从一次故障中恢复以后再commit的能力。</simpara>
<simpara>2PC协议依赖于Flink的检查点机制。检查点屏障是开始一个新的事务的通知，所有操作符自己的检查点成功的通知是它们可以commit的投票，而作业管理器通知一个检查点成功的消息是commit事务的指令。于WAL
sink形成对比的是，2PC
sinks依赖于sink系统和sink本身的实现可以实现恰好处理一次语义。更多的，2PC
sink不断的将数据写入到sink系统中，而WAL写模型就会有之前所述的问题。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/source-sink.svg"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
</section>
<section xml:id="_flink提供的连接器">
<title>Flink提供的连接器</title>
<simpara>Flink提供了读写很多存储系统的连接器。消息队列，日志系统，例如Apache
Kafka, Kinesis,
RabbitMQ等等这些是常用的数据源。在批处理环境中，数据流很可能是监听一个文件系统，而当新的数据落盘的时候，读取这些新数据。</simpara>
<simpara>在sink一端，数据流经常写入到消息队列中，以供接下来的流处理程序消费。数据流也可能写入到文件系统中做持久化，或者交给批处理程序来进行分析。数据流还可能被写入到key-value存储或者关系型数据库中，例如Cassandra，ElasticSearch或者MySQL中，这样数据可供查询，还可以在仪表盘中显示出来。</simpara>
<simpara>不幸的是，对于大多数存储系统并没有标准接口，除了针对DBMS的JDBC。相反，每一个存储系统都需要有自己的特定的连接器。所以，Flink需要维护针对不同存储系统（消息队列，日志系统，文件系统，k-v数据库，关系型数据库等等）的连接器实现。</simpara>
<simpara>Flink提供了针对Apache Kafka, Kinesis, RabbitMQ, Apache Nifi,
各种文件系统，Cassandra, Elasticsearch,
还有JDBC的连接器。除此之外，Apache
Bahir项目还提供了额外的针对例如ActiveMQ, Akka, Flume, Netty,
和Redis等的连接器。</simpara>
</section>
<section xml:id="_apache_kafka_source连接器">
<title>Apache Kafka Source连接器</title>
<simpara>Apache
Kafka是一个分布式流式平台。它的核心是一个分布式的发布订阅消息系统。</simpara>
<simpara>Kafka将事件流组织为所谓的topics。一个主题就是一个事件日志系统，Kafka可以保证主题中的数据在被读取时和这些数据在被写入时相同的顺序。为了扩大读写的规模，主题可以分裂为多个分区，这些分区分布在一个集群上面。这时，读写顺序的保证就限制到了分区这个粒度，
Kafka并没有提供从不同分区读取数据时的顺序保证。Kafka分区的读位置称为偏移量（offset）。</simpara>
<simpara>Kafka的依赖引入如下：</simpara>
<programlisting language="xml" linenumbering="unnumbered">&lt;dependency&gt;
   &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;
   &lt;artifactId&gt;flink-connector-kafka_${scala.binary.version}&lt;/artifactId&gt;
   &lt;version&gt;${flink.version}&lt;/version&gt;
&lt;/dependency&gt;</programlisting>
<simpara>Flink
Kafka连接器并行的摄入事件流。每一个并行source任务可以从一个或者多个分区中读取数据。任务将会跟踪每一个分区当前的读偏移量，然后将读偏移量写入到检查点数据中。当从任务故障恢复时，读偏移量将被恢复，而source任务将从检查点保存的读偏移量开始重新读取数据。Flink
Kafka连接器并不依赖Kafka自己的offset-tracking机制（基于消费者组实现）。下图展示了分区如何分配给source实例。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0801.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>Kafka source连接器使用如下代码创建</simpara>
<programlisting language="java" linenumbering="unnumbered">Properties properties = new Properties();
properties.setProperty("bootstrap.servers", "localhost:9092");
properties.setProperty("group.id", "consumer-group");
properties.setProperty("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
properties.setProperty("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
properties.setProperty("auto.offset.reset", "latest");

env
    .addSource(new FlinkKafkaConsumer&lt;String&gt;(
        "atguigu",
        new SimpleStringSchema(),
        properties
    ))
    .print();</programlisting>
<simpara>构造器接受三个参数。第一个参数定义了从哪些topic中读取数据，可以是一个topic，也可以是topic列表，还可以是匹配所有想要读取的topic的正则表达式。当从多个topic中读取数据时，Kafka连接器将会处理所有topic的分区，将这些分区的数据放到一条流中去。</simpara>
<simpara>第二个参数是一个DeserializationSchema或者KeyedDeserializationSchema。Kafka消息被存储为原始的字节数据，所以需要反序列化成Java对象。上例中使用的SimpleStringSchema，是一个内置的DeserializationSchema，它仅仅是简单的将字节数组反序列化成字符串。DeserializationSchema和KeyedDeserializationSchema是公共的接口，所以我们可以自定义反序列化逻辑。</simpara>
<simpara>第三个参数是一个Properties对象，设置了用来读写的Kafka客户端的一些属性。</simpara>
</section>
<section xml:id="_apache_kafka_sink连接器">
<title>Apache Kafka Sink连接器</title>
<simpara>下面的例子展示了如何创建一个Kafka sink</simpara>
<programlisting language="java" linenumbering="unnumbered">Properties properties = new Properties();
properties.put("bootstrap.servers", "localhost:9092");
stream
    .addSink(
        new FlinkKafkaProducer&lt;String&gt;(
            "test",
            new SimpleStringSchema(),
            properties
    ));</programlisting>
</section>
<section xml:id="_kakfa_sink的at_least_once保证">
<title>Kakfa Sink的at-least-once保证</title>
<simpara>Flink的Kafka sink提供了基于配置的一致性保证。Kafka
sink使用下面的条件提供了至少处理一次保证：</simpara>
<itemizedlist>
<listitem>
<simpara>Flink检查点机制开启，所有的数据源都是可重置的。</simpara>
</listitem>
<listitem>
<simpara>当写入失败时，sink连接器将会抛出异常，使得应用程序挂掉然后重启。这是默认行为。应用程序内部的Kafka客户端还可以配置为重试写入，只要提前声明当写入失败时，重试几次这样的属性（retries
property）。</simpara>
</listitem>
<listitem>
<simpara>sink连接器在完成它的检查点之前会等待Kafka发送已经将数据写入的通知。</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="_kafka_sink的恰好处理一次语义保证">
<title>Kafka Sink的恰好处理一次语义保证</title>
<simpara>Kafka 0.11版本引入了事务写特性。由于这个新特性，Flink Kafka
sink可以为输出结果提供恰好处理一次语义的一致性保证，只要经过合适的配置就行。Flink程序必须开启检查点机制，并从可重置的数据源进行消费。FlinkKafkaProducer还提供了包含Semantic参数的构造器来控制sink提供的一致性保证。可能的取值如下：</simpara>
<itemizedlist>
<listitem>
<simpara>Semantic.NONE，不提供任何一致性保证。数据可能丢失或者被重写多次。</simpara>
</listitem>
<listitem>
<simpara>Semantic.AT_LEAST_ONCE，保证无数据丢失，但可能被处理多次。这个是默认设置。</simpara>
</listitem>
<listitem>
<simpara>Semantic.EXACTLY_ONCE，基于Kafka的事务性写入特性实现，保证每条数据恰好处理一次。</simpara>
</listitem>
</itemizedlist>
<simpara>例子</simpara>
<programlisting language="java" linenumbering="unnumbered">DataStreamSource&lt;String&gt; text = env.socketTextStream("localhost", 9999, "\n");

String brokerList = "localhost:9092";
String topic = "topic";

Properties prop = new Properties();
prop.setProperty("bootstrap.servers", brokerList);

//第一种解决方案，设置FlinkKafkaProducer里面的事务超时时间
//设置事务超时时间
//prop.setProperty("transaction.timeout.ms",60000*15+"");

//第二种解决方案，设置kafka的最大事务超时时间

//FlinkKafkaProducer&lt;String&gt; myProducer = new FlinkKafkaProducer&lt;&gt;(brokerList, topic, new SimpleStringSchema());

//使用仅一次语义的kafkaProducer
FlinkKafkaProducer&lt;String&gt; myProducer = new FlinkKafkaProducer&lt;&gt;(
    topic,
    new KeyedSerializationSchemaWrapper&lt;String&gt;(new SimpleStringSchema()),
    prop,
    FlinkKafkaProducer.Semantic.EXACTLY_ONCE
);
text.addSink(myProducer);</programlisting>
</section>
<section xml:id="_redis_sink连接器">
<title>Redis Sink连接器</title>
<simpara>导入依赖</simpara>
<programlisting language="xml" linenumbering="unnumbered">&lt;dependency&gt;
  &lt;groupId&gt;org.apache.bahir&lt;/groupId&gt;
  &lt;artifactId&gt;flink-connector-redis_${scala.binary.version}&lt;/artifactId&gt;
  &lt;version&gt;1.0&lt;/version&gt;
&lt;/dependency&gt;</programlisting>
<simpara>例子</simpara>
<programlisting language="java" linenumbering="unnumbered">public class WriteToRedisExample {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        DataStream&lt;SensorReading&gt; stream = env.addSource(new SensorSource());

        FlinkJedisPoolConfig conf = new FlinkJedisPoolConfig.Builder().setHost("localhost").build();

        stream.addSink(new RedisSink&lt;Event&gt;(conf, new MyRedisSink()));

        env.execute();
    }

    public static class MyRedisSink implements RedisMapper&lt;SensorReading&gt; {
        @Override
        public String getKeyFromData(SensorReading r) {
            return r.id;
        }

        @Override
        public String getValueFromData(SensorReading r) {
            return r.temperature + "";
        }

        @Override
        public RedisCommandDescription getCommandDescription() {
            return new RedisCommandDescription(RedisCommand.HSET, "sensor");
        }
    }
}</programlisting>
</section>
<section xml:id="_jdbc_sink连接器">
<title>JDBC sink连接器</title>
<simpara>导入依赖</simpara>
<programlisting language="xml" linenumbering="unnumbered">&lt;dependency&gt;
    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;
    &lt;artifactId&gt;flink-connector-jdbc_${scala.binary.version}&lt;/artifactId&gt;
    &lt;version&gt;${flink.version}&lt;/version&gt;
&lt;/dependency&gt;</programlisting>
<simpara>例子</simpara>
<programlisting language="java" linenumbering="unnumbered">public class SinkToMySQL {

    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        env.addSource(new SensorSource()
        ).addSink(
                JdbcSink.sink(
                        "INSERT INTO temps (id, temp) VALUES (?, ?)",
                        (statement, r) -&gt; {
                            statement.setString(1, r.id);
                            statement.setDouble(2, r.temperature);
                        },
                        JdbcExecutionOptions.builder()
                                .withBatchSize(1000)
                                .withBatchIntervalMs(200)
                                .withMaxRetries(5)
                                .build(),
                        new JdbcConnectionOptions.JdbcConnectionOptionsBuilder()
                                .withUrl("jdbc:mysql://localhost:3306/sensor")
                                .withDriverName("com.mysql.cj.jdbc.Driver")
                                .withUsername("zuoyuan")
                                .withPassword("zuoyuan")
                                .build()
                ));

        env.execute();
    }
}</programlisting>
</section>
<section xml:id="_elasticsearch_sink连接器">
<title>ElasticSearch Sink连接器</title>
<simpara>导入依赖</simpara>
<programlisting language="xml" linenumbering="unnumbered">&lt;dependency&gt;
  &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;
  &lt;artifactId&gt;flink-connector-elasticsearch7_${scala.binary.version}&lt;/artifactId&gt;
  &lt;version&gt;${flink.version}&lt;/version&gt;
&lt;/dependency&gt;</programlisting>
<simpara>例子</simpara>
<programlisting language="java" linenumbering="unnumbered">public class SinkToES {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        ArrayList&lt;HttpHost&gt; httpHosts = new ArrayList&lt;&gt;();
        httpHosts.add(new HttpHost("127.0.0.1", 9200, "http"));

        ElasticsearchSink.Builder&lt;SensorReading&gt; esBuilder = new ElasticsearchSink.Builder&lt;&gt;(
            httpHosts,
            new ElasticsearchSinkFunction&lt;SensorReading&gt;() {
                @Override
                public void process(SensorReading r, RuntimeContext runtimeContext, RequestIndexer requestIndexer) {
                    HashMap&lt;String, String&gt; data = new HashMap&lt;&gt;();
                    data.put(r.id, r.temperature + "");

                    IndexRequest indexRequest = Requests
                            .indexRequest()
                            .index("sensor-reading")
                            .source(data);

                    requestIndexer.add(indexRequest);
                }
            }
        );

        esBuilder.setBulkFlushMaxActions(1);

        DataStream&lt;Event&gt; stream = env.addSource(new SensorSource());

        stream.addSink(esBuilder.build());

        env.execute();
    }
}</programlisting>
</section>
<section xml:id="_写入hbase自定义数据源">
<title>写入HBase（自定义数据源）</title>
<simpara>导入依赖</simpara>
<programlisting language="xml" linenumbering="unnumbered">&lt;dependency&gt;
    &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;
    &lt;artifactId&gt;hbase-client&lt;/artifactId&gt;
    &lt;version&gt;${hbase.version}&lt;/version&gt;
&lt;/dependency&gt;</programlisting>
<simpara>例子如下</simpara>
<programlisting language="java" linenumbering="unnumbered">public class WriteToHBase {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        env
                .fromElements("hello", "world")
                .addSink(
                        new RichSinkFunction&lt;String&gt;() {
                            public org.apache.hadoop.conf.Configuration configuration; // 管理Hbase的配置信息
                            public Connection connection; // 管理Hbase连接
                            @Override
                            public void open(Configuration parameters) throws Exception {
                                super.open(parameters);
                                configuration = HBaseConfiguration.create();
                                configuration.set("hbase.zookeeper.quorum", "hadoop105:2181");
                                connection = ConnectionFactory.createConnection(configuration);
                            }

                            @Override
                            public void invoke(String value, Context context) throws Exception {
                                Table table = connection.getTable(TableName.valueOf("test"));
                                Put put = new Put("rowkey".getBytes(StandardCharsets.UTF_8));
                                put.addColumn("info".getBytes(StandardCharsets.UTF_8)
                                        , value.getBytes(StandardCharsets.UTF_8)
                                        , "1".getBytes(StandardCharsets.UTF_8));
                                table.put(put);
                                table.close();
                            }

                            @Override
                            public void close() throws Exception {
                                super.close();
                                connection.close(); // 关闭连接
                            }
                        }
                );

        env.execute();
    }
}</programlisting>
</section>
</section>
<section xml:id="_实现自定义源函数">
<title>实现自定义源函数</title>
<simpara>DataStream API提供了两个接口来实现source连接器：</simpara>
<itemizedlist>
<listitem>
<simpara>SourceFunction和RichSourceFunction可以用来定义非并行的source连接器，source跑在单任务上。</simpara>
</listitem>
<listitem>
<simpara>ParallelSourceFunction和RichParallelSourceFunction可以用来定义跑在并行实例上的source连接器。</simpara>
</listitem>
</itemizedlist>
<simpara>除了并行和非并行的区别，这两种接口完全一样。就像process
function的rich版本一样，RichSourceFunction和RichParallelSourceFunction的子类可以override
open()和close()方法，也可以访问RuntimeContext，RuntimeContext提供了并行任务实例的数量，当前任务实例的索引，以及一些其他信息。</simpara>
<simpara>SourceFunction和ParallelSourceFunction定义了两种方法：</simpara>
<itemizedlist>
<listitem>
<simpara>void run(SourceContext ctx)</simpara>
</listitem>
<listitem>
<simpara>cancel()</simpara>
</listitem>
</itemizedlist>
<simpara>run()方法用来读取或者接收数据然后将数据摄入到Flink应用中。根据接收数据的系统，数据可能是推送的也可能是拉取的。Flink仅仅在特定的线程调用run()方法一次，通常情况下会是一个无限循环来读取或者接收数据并发送数据。任务可以在某个时间点被显式的取消，或者由于流是有限流，当数据被消费完毕时，任务也会停止。</simpara>
<simpara>当应用被取消或者关闭时，cancel()方法会被Flink调用。为了优雅的关闭Flink应用，run()方法需要在cancel()被调用以后，立即终止执行。下面的例子显示了一个简单的源函数的例子：从0数到Long.MAX_VALUE。</simpara>
<programlisting language="java" linenumbering="unnumbered">public static class CountSource implements SourceFunction&lt;Long&gt; {
    private Boolean isRunning = true;

    @Override
    public void run(SourceContext&lt;Long&gt; ctx) throws Exception {
        long cnt = -1;
        while (isRunning &amp;&amp; cnt &lt; Long.MAX_VALUE) {
            cnt += 1;
            ctx.collect(cnt);
        }
    }

    @Override
    public void cancel() {
        isRunning = false;
    }
}</programlisting>
<section xml:id="_可重置的源函数">
<title>可重置的源函数</title>
<simpara>之前我们讲过，应用程序只有使用可以重播输出数据的数据源时，才能提供令人满意的一致性保证。如果外部系统暴露了获取和重置读偏移量的API，那么source函数就可以重播源数据。这样的例子包括一些能够提供文件流的偏移量的文件系统，或者提供seek方法用来移动到文件的特定位置的文件系统。或者Apache
Kafka这种可以为每一个主题的分区提供偏移量并且可以设置分区的读位置的系统。一个反例就是source连接器连接的是socket，socket将会立即丢弃已经发送过的数据。</simpara>
<simpara>支持重播输出的源函数需要和Flink的检查点机制集成起来，还需要在检查点被处理时，持久化当前所有的读取位置。当应用从一个保存点（savepoint）恢复或者从故障恢复时，Flink会从最近一次的检查点或者保存点中获取读偏移量。如果程序开始时并不存在状态，那么读偏移量将会被设置到一个默认值。一个可重置的源函数需要实现CheckpointedFunction接口，还需要能够存储读偏移量和相关的元数据，例如文件的路径，分区的ID。这些数据将被保存在list
state或者union list state中。</simpara>
<simpara>下面的例子将CountSource重写为可重置的数据源。</simpara>
<programlisting language="java" linenumbering="unnumbered">public static class ResettableCountSource
        implements SourceFunction&lt;Long&gt;, CheckpointedFunction {

    private Boolean isRunning = true;
    private Long cnt;
    private ListState&lt;Long&gt; offsetState;

    @Override
    public void run(SourceContext&lt;Long&gt; ctx) throws Exception {
        while (isRunning &amp;&amp; cnt &lt; Long.MAX_VALUE) {
            // synchronize data emission and checkpoints
            synchronized (ctx.getCheckpointLock()) {
                cnt += 1;
                ctx.collect(cnt);
            }
        }
    }

    @Override
    public void cancel() {
        isRunning = false;
    }

    @Override
    public void snapshotState(FunctionSnapshotContext context) throws Exception {
        // remove previous cnt
        offsetState.clear();
        // add current cnt
        offsetState.add(cnt);
    }

    @Override
    public void initializeState(FunctionInitializationContext context) throws Exception {
        ListStateDescriptor&lt;Long&gt; desc = new ListStateDescriptor&lt;&gt;(
                "offset", Types.LONG);
        offsetState = context
                .getOperatorStateStore()
                .getListState(desc);
        // initialize cnt variable
        Iterable&lt;Long&gt; it = offsetState.get();
        if (null == it || !it.iterator().hasNext()) {
            cnt = -1L;
        } else {
            cnt = it.iterator().next();
        }
    }
}</programlisting>
</section>
</section>
<section xml:id="_实现自定义sink函数">
<title>实现自定义sink函数</title>
<simpara>DataStream
API中，任何运算符或者函数都可以向外部系统发送数据。DataStream不需要最终流向sink运算符。例如，我们可能实现了一个FlatMapFunction，这个函数将每一个接收到的数据通过HTTP
POST请求发送出去，而不使用Collector发送到下一个运算符。DataStream
API也提供了SinkFunction接口以及对应的rich版本RichSinkFunction抽象类。SinkFunction接口提供了一个方法：</simpara>
<programlisting language="java" linenumbering="unnumbered">void invode(IN value, Context ctx)</programlisting>
<simpara>SinkFunction的Context可以访问当前处理时间，当前水位线，以及数据的时间戳。</simpara>
<simpara>下面的例子展示了一个简单的SinkFunction，可以将传感器读数写入到socket中去。需要注意的是，我们需要在启动Flink程序前启动一个监听相关端口的进程。否则将会抛出ConnectException异常。可以运行nc
-l localhost 9191命令。</simpara>
<programlisting language="java" linenumbering="unnumbered">DataStream&lt;SensorReading&gt; readings = env.addSource(new SensorSource());

// write the sensor readings to a socket
readings
    .addSink(new SimpleSocketSink("localhost", 9191))
    // set parallelism to 1 because only one thread can write to a socket
    .setParallelism(1);

public static class SimpleSocketSink extends RichSinkFunction&lt;SensorReading&gt; {
    private String host;
    private Integer port;

    private Socket socket;
    private PrintStream writer;

    public SimpleSocketSink(String host, Integer port) {
        this.host = host;
        this.port = port;
    }

    @Override
    public void open(Configuration parameters) throws Exception {
        super.open(parameters);
        socket = new Socket(InetAddress.getByName(host), port);
        writer = new PrintStream(socket.getOutputStream());
    }

    @Override
    public void invoke(SensorReading value, Context context) throws Exception {
        writer.println(value.toString());
        writer.flush();
    }

    @Override
    public void close() throws Exception {
        super.close();
        writer.close();
        socket.close();
    }
}</programlisting>
<simpara>之前我们讨论过，端到端的一致性保证建立在sink连接器的属性上面。为了达到端到端的恰好处理一次语义的目的，应用程序需要幂等性的sink连接器或者事务性的sink连接器。上面例子中的SinkFunction既不是幂等写入也不是事务性的写入。由于socket具有只能添加（append-only）这样的属性，所以不可能实现幂等性的写入。又因为socket不具备内置的事务支持，所以事务性写入就只能使用Flink的WAL
sink特性来实现了。接下来我们将学习如何实现幂等sink连接器和事务sink连接器。</simpara>
<section xml:id="_幂等sink连接器">
<title>幂等sink连接器</title>
<simpara>对于大多数应用，SinkFunction接口足以实现一个幂等性写入的sink连接器了。需要以下两个条件：</simpara>
<itemizedlist>
<listitem>
<simpara>结果数据必须具有确定性的key，在这个key上面幂等性更新才能实现。例如一个计算每分钟每个传感器的平均温度值的程序，确定性的key值可以是传感器的ID和每分钟的时间戳。确定性的key值，对于在故障恢复的场景下，能够正确的覆盖结果非常的重要。</simpara>
</listitem>
<listitem>
<simpara>外部系统支持针对每个key的更新，例如关系型数据库或者key-value存储。</simpara>
</listitem>
</itemizedlist>
<simpara>下面的例子展示了如何实现一个针对JDBC数据库的幂等写入sink连接器，这里使用的是MySQL数据库。</simpara>
<simpara>建表语句</simpara>
<programlisting language="sql" linenumbering="unnumbered">create database sensor;
create table temps(id varchar(20), temp float);</programlisting>
<simpara>导入依赖</simpara>
<programlisting language="xml" linenumbering="unnumbered">&lt;dependency&gt;
  &lt;groupId&gt;mysql&lt;/groupId&gt;
  &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;
  &lt;version&gt;8.0.21&lt;/version&gt;
&lt;/dependency&gt;</programlisting>
<simpara>代码</simpara>
<programlisting language="java" linenumbering="unnumbered">public class WriteToMySQLExample {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        DataStream&lt;SensorReading&gt; stream = env.addSource(new SensorSource());

        stream.addSink(new MyJDBCSink());

        env.execute();
    }

    public static class MyJDBCSink extends RichSinkFunction&lt;SensorReading&gt; {
        private Connection conn;
        private PreparedStatement insertStmt;
        private PreparedStatement updateStmt;

        @Override
        public void open(Configuration parameters) throws Exception {
            super.open(parameters);
            conn = DriverManager.getConnection(
                    "jdbc:mysql://localhost:3306/sensor",
                    "zuoyuan",
                    "zuoyuan"
            );
            insertStmt = conn.prepareStatement("INSERT INTO temps (id, temp) VALUES (?, ?)");
            updateStmt = conn.prepareStatement("UPDATE temps SET temp = ? WHERE id = ?");
        }

        @Override
        public void invoke(SensorReading value, Context context) throws Exception {
            updateStmt.setDouble(1, value.temperature);
            updateStmt.setString(2, value.id);
            updateStmt.execute();

            if (updateStmt.getUpdateCount() == 0) {
                insertStmt.setString(1, value.id);
                insertStmt.setDouble(2, value.temperature);
                insertStmt.execute();
            }
        }

        @Override
        public void close() throws Exception {
            super.close();
            insertStmt.close();
            updateStmt.close();
            conn.close();
        }
    }
}</programlisting>
</section>
<section xml:id="_事务性sink连接器">
<title>事务性sink连接器</title>
<simpara>事务写入sink连接器需要和Flink的检查点机制集成，因为只有在检查点成功完成以后，事务写入sink连接器才会向外部系统commit数据。</simpara>
<simpara>为了简化事务性sink的实现，Flink提供了两个模版用来实现自定义sink运算符。这两个模版都实现了CheckpointListener接口。CheckpointListener接口将会从作业管理器接收到检查点完成的通知。</simpara>
<itemizedlist>
<listitem>
<simpara>GenericWriteAheadSink模版会收集检查点之前的所有的数据，并将数据存储到sink任务的运算符状态中。状态保存到了检查点中，并在任务故障的情况下恢复。当任务接收到检查点完成的通知时，任务会将所有的数据写入到外部系统中。</simpara>
</listitem>
<listitem>
<simpara>TwoPhaseCommitSinkFunction模版利用了外部系统的事务特性。对于每一个检查点，任务首先开始一个新的事务，并将接下来所有的数据都写到外部系统的当前事务上下文中去。当任务接收到检查点完成的通知时，sink连接器将会commit这个事务。</simpara>
</listitem>
</itemizedlist>
<simpara><emphasis>GENERICWRITEAHEADSINK</emphasis></simpara>
<simpara>GenericWriteAheadSink使得sink运算符可以很方便的实现。这个运算符和Flink的检查点机制集成使用，目标是将每一条数据恰好一次写入到外部系统中去。需要注意的是，在发生故障的情况下，write-ahead
log
sink可能会不止一次的发送相同的数据。所以GenericWriteAheadSink无法提供完美无缺的恰好处理一次语义的一致性保证，而是仅能提供at-least-once这样的保证。我们接下来详细的讨论这些场景。</simpara>
<simpara>GenericWriteAheadSink的原理是将接收到的所有数据都追加到有检查点分割好的预写式日志中去。每当sink运算符碰到检查点屏障，运算符将会开辟一个新的section，并将接下来的所有数据都追加到新的section中去。WAL（预写式日志）将会保存到运算符状态中。由于log能被恢复，所有不会有数据丢失。</simpara>
<simpara>当GenericWriteAheadSink接收到检查点完成的通知时，将会发送对应检查点的WAL中存储的所有数据。当所有数据发送成功，对应的检查点必须在内部提交。</simpara>
<simpara>检查点的提交分两步。第一步，sink持久化检查点被提交的信息。第二步，删除WAL中所有的数据。我们不能将commit信息保存在Flink应用程序状态中，因为状态不是持久化的，会在故障恢复时重置状态。相反，GenericWriteAheadSink依赖于可插拔的组件在一个外部持久化存储中存储和查找提交信息。这个组件就是CheckpointCommitter。</simpara>
<simpara>继承GenericWriteAheadSink的运算符需要提供三个构造器函数。</simpara>
<itemizedlist>
<listitem>
<simpara>CheckpointCommitter</simpara>
</listitem>
<listitem>
<simpara>TypeSerializer，用来序列化输入数据。</simpara>
</listitem>
<listitem>
<simpara>一个job ID，传给CheckpointCommitter，当应用重启时可以识别commit信息。</simpara>
</listitem>
</itemizedlist>
<simpara>还有，write-ahead运算符需要实现一个单独的方法：</simpara>
<programlisting language="java" linenumbering="unnumbered">boolean sendValues(Iterable&lt;IN&gt; values, long chkpntId, long timestamp)</programlisting>
<simpara>当检查点完成时，GenericWriteAheadSink调用sendValues()方法来将数据写入到外部存储系统中。这个方法接收一个检查点对应的所有数据的迭代器，检查点的ID，检查点被处理时的时间戳。当数据写入成功时，方法必须返回true，写入失败返回false。</simpara>
<simpara>之前我们讲过，GenericWriteAheadSink无法提供完美的exactly-once保证。有两个故障状况会导致数据可能被发送不止一次。</simpara>
<itemizedlist>
<listitem>
<simpara>当任务执行sendValues()方法时，程序挂掉了。如果外部系统无法原子性的写入所有数据（要么都写入要么都不写），一些数据可能会写入，而另一些数据并没有被写入。由于checkpoint还没有commit，所以在任务恢复的过程中一些数据可能会被再次写入。</simpara>
</listitem>
<listitem>
<simpara>所有数据都写入成功了，sendValues()方法也返回true了；但在CheckpointCommitter方法被调用之前程序挂了，或者CheckpointCommitter在commit检查点时失败了。那么在恢复的过程中，所有未被提交的检查点将会被重新写入。</simpara>
</listitem>
</itemizedlist>
<simpara><emphasis>TWOPHASECOMMITSINKFUNCTION</emphasis></simpara>
<simpara>Flink提供了TwoPhaseCommitSinkFunction接口来简化sink函数的实现。这个接口保证了端到端的exactly-once语义。2PC
sink函数是否提供这样的一致性保证取决于我们的实现细节。我们需要讨论一个问题：<literal>2PC协议是否开销太大？</literal></simpara>
<simpara>通常来讲，为了保证分布式系统的一致性，2PC是一个非常昂贵的方法。尽管如此，在Flink的语境下，2PC协议针对每一个检查点只运行一次。TwoPhaseCommitSinkFunction和WAL
sink很相似，不同点在于前者不会将数据收集到state中，而是会写入到外部系统事务的上下文中。</simpara>
<simpara>TwoPhaseCommitSinkFunction实现了以下协议。在sink任务发送出第一条数据之前，任务将在外部系统中开始一个事务，所有接下来的数据将被写入这个事务的上下文中。当作业管理器初始化检查点并将检查点屏障插入到流中的时候，2PC协议的投票阶段开始。当运算符接收到检查点屏障，运算符将保存它的状态，当保存完成时，运算符将发送一个acknowledgement信息给作业管理器。当sink任务接收到检查点屏障时，运算符将会持久化它的状态，并准备提交当前的事务，以及acknowledge
JobManager中的检查点。发送给作业管理器的acknowledgement信息类似于2PC协议中的commit投票。sink任务还不能提交事务，因为它还没有保证所有的任务都已经完成了它们的检查点操作。sink任务也会为下一个检查点屏障之前的所有数据开始一个新的事务。</simpara>
<simpara>当作业管理器成功接收到所有任务实例发出的检查点操作成功的通知时，作业管理器将会把检查点完成的通知发送给所有感兴趣的任务。这里的通知对应于2PC协议的提交命令。当sink任务接收到通知时，它将commit所有处于开启状态的事务。一旦sink任务acknowledge了检查点操作，它必须能够commit对应的事务，即使任务发生故障。如果commit失败，数据将会丢失。</simpara>
<simpara>让我们总结一下外部系统需要满足什么样的要求：</simpara>
<itemizedlist>
<listitem>
<simpara>外部系统必须提供事务支持，或者sink的实现能在外部系统上模拟事务功能。</simpara>
</listitem>
<listitem>
<simpara>在检查点操作期间，事务必须处于open状态，并接收这段时间数据的持续写入。</simpara>
</listitem>
<listitem>
<simpara>事务必须等到检查点操作完成的通知到来才可以提交。在恢复周期中，可能需要一段时间等待。如果sink系统关闭了事务（例如超时了），那么未被commit的数据将会丢失。</simpara>
</listitem>
<listitem>
<simpara>sink必须在进程挂掉后能够恢复事务。一些sink系统会提供事务ID，用来commit或者abort一个开始的事务。</simpara>
</listitem>
<listitem>
<simpara>commit一个事务必须是一个幂等性操作。sink系统或者外部系统能够观察到事务已经被提交，或者重复提交并没有副作用。</simpara>
</listitem>
</itemizedlist>
<simpara>下面的例子可能会让上面的一些概念好理解一些。</simpara>
<programlisting language="java" linenumbering="unnumbered">public static class TransactionalFileSink extends TwoPhaseCommitSinkFunction&lt;Long, String, Void&gt; {

    private BufferedWriter transactionWriter;

    public TransactionalFileSink() {
        super(StringSerializer.INSTANCE, VoidSerializer.INSTANCE);
    }

    @Override
    protected String beginTransaction() throws Exception {
        long timeNow = System.currentTimeMillis();
        int taskIdx = this.getRuntimeContext().getIndexOfThisSubtask();
        String transactionFile = timeNow + "-" + taskIdx;
        Path tFilePath = Paths.get("/home/zuoyuan/filetemp/" + transactionFile);
        Files.createFile(tFilePath);
        this.transactionWriter = Files.newBufferedWriter(tFilePath);
        System.out.println("create tx");
        return transactionFile;
    }

    @Override
    protected void invoke(String transaction, Long value, Context context) throws Exception {
        transactionWriter.write(value.toString());
        transactionWriter.write('\n');
    }

    @Override
    protected void preCommit(String transaction) throws Exception {
        transactionWriter.flush();
        transactionWriter.close();
    }

    @Override
    protected void commit(String transaction) {
        Path tFilePath = Paths.get("/home/zuoyuan/filetemp/" + transaction);
        if (Files.exists(tFilePath)) {
            try {
                Path cFilePath = Paths.get("/home/zuoyuan/filetarget/" + transaction);
                Files.move(tFilePath, cFilePath);
                System.out.println("commit complete");
            } catch (IOException e) {
                e.printStackTrace();
            }
        }
    }

    @Override
    protected void abort(String transaction) {
        Path tFilePath = Paths.get("/home/zuoyuan/filetemp/" + transaction);
        if (Files.exists(tFilePath)) {
            try {
                Files.delete(tFilePath);
            } catch (IOException e) {
                e.printStackTrace();
            }
        }
    }
}</programlisting>
<simpara>`TwoPhaseCommitSinkFunction&lt;IN, TXN, CONTEXT&gt;`包含如下三个范型参数：</simpara>
<itemizedlist>
<listitem>
<simpara>IN表示输入数据的类型。</simpara>
</listitem>
<listitem>
<simpara>TXN定义了一个事务的标识符，可以用来识别和恢复事务。</simpara>
</listitem>
<listitem>
<simpara>CONTEXT定义了自定义的上下文。</simpara>
</listitem>
</itemizedlist>
<simpara>TwoPhaseCommitSinkFunction的构造器需要两个TypeSerializer。一个是TXN的类型，另一个是CONTEXT的类型。</simpara>
<simpara>最后，TwoPhaseCommitSinkFunction定义了五个需要实现的方法：</simpara>
<itemizedlist>
<listitem>
<simpara>beginTransaction()开始一个事务，并返回事务的标识符。</simpara>
</listitem>
<listitem>
<simpara>invoke(TXN txn, IN value, Context context)将值写入到当前事务中。</simpara>
</listitem>
<listitem>
<simpara>preCommit(TXN txn)预提交一个事务。一个预提交的事务不会接收新的写入。</simpara>
</listitem>
<listitem>
<simpara>commit(TXN txn)提交一个事务。这个操作必须是幂等的。</simpara>
</listitem>
<listitem>
<simpara>abort(TXN txn)终止一个事务。</simpara>
</listitem>
</itemizedlist>
</section>
</section>
</section>
<section xml:id="_搭建flink运行流式应用">
<title>搭建Flink运行流式应用</title>
<section xml:id="_独立集群部署">
<title>独立集群部署</title>
<simpara>独立集群包含至少一个master进程，以及至少一个TaskManager进程，TaskManager进程运行在一台或者多台机器上。所有的进程都是JVM进程。下图展示了独立集群的部署。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0901.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>master进程在不同的线程中运行了一个Dispatcher和一个ResourceManager。一旦它们开始运行，所有TaskManager都将在Resourcemanager中进行注册。下图展示了一个任务如何提交到一个独立集群中去。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0902.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>客户端向Dispatcher提交了一个任务，Dispatcher将会启动一个作业管理器线程，并提供执行所需的JobGraph。作业管理器向ResourceManager请求必要的task
slots。一旦请求的slots分配好，作业管理器就会部署job。</simpara>
<simpara>在standalone这种部署方式中，master和worker进程在失败以后，并不会自动重启。如果有足够的slots可供使用，job是可以从一次worker失败中恢复的。只要我们运行多个worker就好了。但如果job想从master失败中恢复的话，则需要进行高可用(HA)的配置了。</simpara>
</section>
<section xml:id="_yarn部署">
<title>YARN部署</title>
<simpara>YARN是Apache
Hadoop的资源管理组件。用来计算集群环境所需要的CPU和内存资源，然后提供给应用程序请求的资源。</simpara>
<simpara>Flink在YARN上运行，有两种模式：job模式和session模式。在job模式中，Flink集群用来运行一个单独的job。一旦job结束，Flink集群停止，并释放所有资源。下图展示了Flink的job如何提交到YARN集群。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0903.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>当客户端提交任务时，客户端将建立和YARN
ResourceManager的连接，然后启动一个新的YARN应用的master进程，进程中包含一个作业管理器线程和一个ResourceManager。作业管理器向ResourceManager请求所需要的slots，用来运行Flink的job。接下来，Flink的ResourceManager将向Yarn的ResourceManager请求容器，然后启动TaskManager进程。一旦启动，TaskManager会将slots注册在Flink的ResourceManager中，Flink的ResourceManager将把slots提供给作业管理器。最终，作业管理器把job的任务提交给TaskManager执行。</simpara>
<simpara>sesison模式将启动一个长期运行的Flink集群，这个集群可以运行多个job，需要手动停止集群。如果以session模式启动，Flink将会连接到YARN的ResourceManager，然后启动一个master进程，包括一个Dispatcher线程和一个Flink的ResourceManager的线程。下图展示了一个Flink
YARN session的启动。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0904.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>当一个作业被提交运行，分发器将启动一个作业管理器线程，这个线程将向Flink的资源管理器请求所需要的slots。如果没有足够的slots，Flink的资源管理器将向YARN的资源管理器请求额外的容器，来启动TaskManager进程，并在Flink的资源管理器中注册。一旦所需slots可用，Flink的资源管理器将把slots分配给作业管理器，然后开始执行job。下图展示了job如何在session模式下执行。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/spaf_0905.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>无论是作业模式还是会话模式，Flink的ResourceManager都会自动对故障的TaskManager进行重启。你可以通过./conf/flink-conf.yaml配置文件来控制Flink在YARN上的故障恢复行为。例如，可以配置有多少容器发生故障后终止应用。</simpara>
<simpara>无论使用job模式还是sesison模式，都需要能够访问Hadoop。</simpara>
</section>
<section xml:id="_高可用部署">
<title>高可用部署</title>
<simpara>Flink的高可用配置需要Apache
ZooKeeper组件，以及一个分布式文件系统，例如HDFS等等。作业管理器将会把相关信息都存储在文件系统中，并将指向文件系统中相关信息的指针保存在ZooKeeper中。一旦失败，一个新的作业管理器将从ZooKeeper中指向相关信息的指针所指向的文件系统中读取元数据，并恢复运行。</simpara>
</section>
<section xml:id="_保存点操作">
<title>保存点操作</title>
<programlisting language="bash" linenumbering="unnumbered">$ ./bin/flink savepoint &lt;jobId&gt; [savepointPath]</programlisting>
<simpara>例如</simpara>
<programlisting language="bash" linenumbering="unnumbered">$ ./bin/flink savepoint bc0b2ad61ecd4a615d92ce25390f61ad \
hdfs:///xxx:50070/savepoints
Triggering savepoint for job bc0b2ad61ecd4a615d92ce25390f61ad.
Waiting for response...
Savepoint completed.
Path: hdfs:///xxx:50070/savepoints/savepoint-bc0b2a-63cf5d5ccef8
You can resume your program from this savepoint with the run command.</programlisting>
<simpara>删除保存点文件</simpara>
<programlisting language="bash" linenumbering="unnumbered">$ ./bin/flink savepoint -d &lt;savepointPath&gt;</programlisting>
<simpara>例子</simpara>
<programlisting language="bash" linenumbering="unnumbered">$ ./bin/flink savepoint -d \
hdfs:///xxx:50070/savepoints/savepoint-bc0b2a-63cf5d5ccef8
Disposing savepoint 'hdfs:///xxx:50070/savepoints/savepoint-bc0b2a-63cf5d5ccef8'.
Waiting for response...
​Savepoint 'hdfs:///xxx:50070/savepoints/savepoint-bc0b2a-63cf5d5ccef8' disposed.</programlisting>
<simpara>从保存点启动应用程序</simpara>
<programlisting language="bash" linenumbering="unnumbered">$ ./bin/flink run -s &lt;savepointPath&gt; [options] &lt;jobJar&gt; [arguments]</programlisting>
</section>
<section xml:id="_取消一个应用">
<title>取消一个应用</title>
<programlisting language="bash" linenumbering="unnumbered">$ ./bin/flink cancel &lt;jobId&gt;</programlisting>
<simpara>取消的同时做保存点操作</simpara>
<programlisting language="bash" linenumbering="unnumbered">$ ./bin/flink cancel -s [savepointPath] &lt;jobId&gt;</programlisting>
<simpara>例如</simpara>
<programlisting language="bash" linenumbering="unnumbered">$ ./bin/flink cancel -s \
hdfs:///xxx:50070/savepoints d5fdaff43022954f5f02fcd8f25ef855
Cancelling job bc0b2ad61ecd4a615d92ce25390f61ad
with savepoint to hdfs:///xxx:50070/savepoints.
Cancelled job bc0b2ad61ecd4a615d92ce25390f61ad.
Savepoint stored in hdfs:///xxx:50070/savepoints/savepoint-bc0b2a-d08de07fbb10.</programlisting>
</section>
<section xml:id="_扩容改变并行度操作">
<title>扩容，改变并行度操作</title>
<programlisting language="bash" linenumbering="unnumbered">$ ./bin/flink modify &lt;jobId&gt; -p &lt;newParallelism&gt;</programlisting>
<simpara>例子</simpara>
<programlisting language="bash" linenumbering="unnumbered">$ ./bin/flink modify bc0b2ad61ecd4a615d92ce25390f61ad -p 16
Modify job bc0b2ad61ecd4a615d92ce25390f61ad.
​Rescaled job bc0b2ad61ecd4a615d92ce25390f61ad. Its new parallelism is 16.</programlisting>
</section>
</section>
<section xml:id="_flink_table_api_sql">
<title>Flink Table API &amp; SQL</title>
<section xml:id="_流式概念">
<title>流式概念</title>
<simpara>Flink 的 Table API 和 SQL 是流批统一的 API。 这意味着 Table API &amp; SQL 在无论有限的批式输入还是无限的流式输入下，都具有相同的语义。 因为传统的关系代数以及 SQL 最开始都是为了批式处理而设计的， 关系型查询在流式场景下不如在批式场景下容易懂。</simpara>
</section>
<section xml:id="_需要引入的依赖">
<title>需要引入的依赖</title>
<simpara>以下是我们需要引入的依赖。</simpara>
<programlisting language="xml" linenumbering="unnumbered">&lt;dependency&gt;
    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;
    &lt;artifactId&gt;flink-table-api-java-bridge_${scala.binary.version}&lt;/artifactId&gt;
    &lt;version&gt;${flink.version}&lt;/version&gt;
&lt;/dependency&gt;

&lt;dependency&gt;
    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;
    &lt;artifactId&gt;flink-table-planner-blink_${scala.binary.version}&lt;/artifactId&gt;
    &lt;version&gt;${flink.version}&lt;/version&gt;
&lt;/dependency&gt;</programlisting>
<simpara>内部实现上，部分 Table API 相关的代码是用 Scala 实现的。所以，下面的依赖也需要添加：</simpara>
<programlisting language="xml" linenumbering="unnumbered">&lt;dependency&gt;
    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;
    &lt;artifactId&gt;flink-streaming-scala_${scala.binary.version}&lt;/artifactId&gt;
    &lt;version&gt;${flink.version}&lt;/version&gt;
&lt;/dependency&gt;</programlisting>
<simpara><emphasis role="strong">扩展依赖</emphasis></simpara>
<simpara>如果你想实现自定义格式来解析 Kafka 数据，或者自定义函数，下面的依赖就足够了，编译出来的 JAR 包可以直接给 SQL Client 使用：</simpara>
<programlisting language="xml" linenumbering="unnumbered">&lt;dependency&gt;
    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;
    &lt;artifactId&gt;flink-table-common&lt;/artifactId&gt;
    &lt;version&gt;${flink.version}&lt;/version&gt;
&lt;/dependency&gt;</programlisting>
<simpara>当前，本模块包含以下可以扩展的接口：</simpara>
<itemizedlist>
<listitem>
<simpara>SerializationSchemaFactory</simpara>
</listitem>
<listitem>
<simpara>DeserializationSchemaFactory</simpara>
</listitem>
<listitem>
<simpara>ScalarFunction</simpara>
</listitem>
<listitem>
<simpara>TableFunction</simpara>
</listitem>
<listitem>
<simpara>AggregateFunction</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="_动态表dynamic_table">
<title>动态表(Dynamic Table)</title>
<simpara>SQL 和关系代数在设计时并未考虑流数据。因此，在关系代数(和 SQL)之间几乎没有概念上的差异。</simpara>
<simpara>本章会讨论这种差异，并介绍 Flink 如何在无界数据集上实现与数据库引擎在有界数据上的处理具有相同的语义。</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>DataStream 上的关系查询</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">关系代数 / SQL</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara><emphasis role="strong">流处理</emphasis></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>关系(或表)是有界(多)元组集合。</simpara></entry>
<entry align="left" valign="top"><simpara>流是一个无限元组序列。</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>对批数据(例如关系数据库中的表)执行的查询可以访问完整的输入数据。</simpara></entry>
<entry align="left" valign="top"><simpara>流式查询在启动时不能访问所有数据，必须“等待”数据流入。</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>批处理查询在产生固定大小的结果后终止。</simpara></entry>
<entry align="left" valign="top"><simpara>流查询不断地根据接收到的记录更新其结果，并且始终不会结束。</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara>尽管存在这些差异，但是使用关系查询和 SQL 处理流并不是不可能的。高级关系数据库系统提供了一个称为 <emphasis>物化视图(Materialized Views)</emphasis> 的特性。物化视图被定义为一条 SQL 查询，就像常规的虚拟视图一样。与虚拟视图相反，物化视图缓存查询的结果，因此在访问视图时不需要对查询进行计算。缓存的一个常见难题是防止缓存为过期的结果提供服务。当其定义查询的基表被修改时，物化视图将过期。 <emphasis>即时视图维护(Eager View Maintenance)</emphasis> 是一种一旦更新了物化视图的基表就立即更新视图的技术。</simpara>
<simpara>如果我们考虑以下问题，那么即时视图维护和流上的SQL查询之间的联系就会变得显而易见:</simpara>
<itemizedlist>
<listitem>
<simpara>数据库表是 INSERT、UPDATE 和 DELETE DML 语句的 stream 的结果，通常称为 changelog stream 。</simpara>
</listitem>
<listitem>
<simpara>物化视图被定义为一条 SQL 查询。为了更新视图，查询不断地处理视图的基本关系的changelog 流。</simpara>
</listitem>
<listitem>
<simpara>物化视图是流式 SQL 查询的结果。</simpara>
</listitem>
</itemizedlist>
<simpara>了解了这些要点之后，我们将在下一节中介绍 <emphasis>动态表(Dynamic tables)</emphasis> 的概念。</simpara>
<section xml:id="_动态表_连续查询continuous_query">
<title>动态表 &amp; 连续查询(Continuous Query)</title>
<simpara><emphasis>动态表</emphasis> 是 Flink 的支持流数据的 Table API 和 SQL 的核心概念。与表示批处理数据的静态表不同，动态表是随时间变化的。可以像查询静态批处理表一样查询它们。查询动态表将生成一个 连续查询 。一个连续查询永远不会终止，结果会生成一个动态表。查询不断更新其(动态)结果表，以反映其(动态)输入表上的更改。本质上，动态表上的连续查询非常类似于定义物化视图的查询。</simpara>
<simpara>需要注意的是，连续查询的结果在语义上总是等价于以批处理模式在输入表快照上执行的相同查询的结果。</simpara>
<simpara>下图显示了流、动态表和连续查询之间的关系：</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/stream-query-stream.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>将流转换为动态表。</simpara>
</listitem>
<listitem>
<simpara>在动态表上计算一个连续查询，生成一个新的动态表。</simpara>
</listitem>
<listitem>
<simpara>生成的动态表被转换回流。</simpara>
</listitem>
</orderedlist>
<note>
<simpara>注意： 动态表首先是一个逻辑概念。在查询执行期间不一定(完全)物化动态表。</simpara>
</note>
<simpara>在下面，我们将解释动态表和连续查询的概念，并使用具有以下模式的单击事件流：</simpara>
<programlisting language="sql" linenumbering="unnumbered">CREATE TABLE clicks (
  user  VARCHAR,     -- 用户名
  url   VARCHAR,     -- 用户访问的url
  cTime TIMESTAMP(3) -- 访问url的时间
) WITH (...);</programlisting>
</section>
<section xml:id="_在流上定义表">
<title>在流上定义表</title>
<simpara>为了使用关系查询处理流，必须将其转换成 Table。从概念上讲，流的每条记录都被解释为对结果表的 INSERT 操作。本质上我们正在从一个 INSERT-only 的 changelog 流构建表。</simpara>
<simpara>下图显示了单击事件流(左侧)如何转换为表(右侧)。当插入更多的单击流记录时，结果表将不断增长。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/append-mode.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<note>
<simpara>注意： 在流上定义的表在内部没有物化。</simpara>
</note>
<simpara>我们来写一下上图所对应的代码。</simpara>
<programlisting language="java" linenumbering="unnumbered">import org.apache.flink.api.common.eventtime.SerializableTimestampAssigner;
import org.apache.flink.api.common.eventtime.WatermarkStrategy;
import org.apache.flink.api.java.tuple.Tuple3;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.EnvironmentSettings;
import org.apache.flink.table.api.Table;
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;
import org.apache.flink.types.Row;

import static org.apache.flink.table.api.Expressions.$;

public class DataStreamToDynamicTable {
    public static void main(String[] args) throws Exception {
        // 获取流环境
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        // 数据源，且设置水位线
        SingleOutputStreamOperator&lt;Tuple3&lt;String, String, Long&gt;&gt; stream = env
                .fromElements(
                        Tuple3.of("Mary", "./home", 12 * 60 * 60 * 1000L),
                        Tuple3.of("Bob", "./cart", 12 * 60 * 60 * 1000L),
                        Tuple3.of("Mary", "./prod?id=1", 12 * 60 * 60 * 1000L + 5 * 1000L),
                        Tuple3.of("Liz", "./home", 12 * 60 * 60 * 1000L + 60 * 1000L),
                        Tuple3.of("Bob", "./prod?id=3", 12 * 60 * 60 * 1000L + 90 * 1000L),
                        Tuple3.of("Mary", "./prod?id=7", 12 * 60 * 60 * 1000L + 105 * 1000L)
                )
                .assignTimestampsAndWatermarks(
                        WatermarkStrategy.&lt;Tuple3&lt;String, String, Long&gt;&gt;forMonotonousTimestamps()
                                .withTimestampAssigner(new SerializableTimestampAssigner&lt;Tuple3&lt;String, String, Long&gt;&gt;() {
                                    @Override
                                    public long extractTimestamp(Tuple3&lt;String, String, Long&gt; element, long recordTimestamp) {
                                        return element.f2;
                                    }
                                })
                );

        // 表环境配置
        EnvironmentSettings settings = EnvironmentSettings.newInstance().inStreamingMode().build();

        // 获取表环境
        StreamTableEnvironment tableEnvironment = StreamTableEnvironment.create(env, settings);

        // 将数据源转换成动态表
        Table table = tableEnvironment
                .fromDataStream(
                        stream,
                        $("f0").as("user"),
                        $("f1").as("url"),
                        $("f2").rowtime().as("cTime")
                );

        // 将动态表转换成数据流
        tableEnvironment
                .toAppendStream(
                        table,
                        Row.class // 动态表中每一行的泛型
                )
                .print();

        // 执行程序
        env.execute();
    }
}</programlisting>
<simpara>输出结果是：</simpara>
<programlisting language="text" linenumbering="unnumbered">Mary,./home,1970-01-01T12:00
Bob,./cart,1970-01-01T12:00
Mary,./prod?id=1,1970-01-01T12:00:05
Liz,./home,1970-01-01T12:01
Bob,./prod?id=3,1970-01-01T12:01:30
Mary,./prod?id=7,1970-01-01T12:01:45</programlisting>
</section>
<section xml:id="_连续查询">
<title>连续查询</title>
<simpara>在动态表上计算一个连续查询，并生成一个新的动态表。与批处理查询不同，连续查询从不终止，并根据其输入表上的更新更新其结果表。在任何时候，连续查询的结果在语义上与以批处理模式在输入表快照上执行的相同查询的结果相同。</simpara>
<simpara>在接下来的代码中，我们将展示 clicks 表上的两个示例查询，这个表是在点击事件流上定义的。</simpara>
<simpara>第一个查询是一个简单的 GROUP-BY COUNT 聚合查询。它基于 user 字段对 clicks 表进行分组，并统计访问的 URL 的数量。下面的图显示了当 clicks 表被附加的行更新时，查询是如何被求值的。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/query-groupBy-cnt.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>当查询开始，clicks 表(左侧)是空的。当第一行数据被插入到 clicks 表时，查询开始计算结果表。第一行数据 [Mary,./home] 插入后，结果表(右侧，上部)由一行 [Mary, 1] 组成。当第二行 [Bob, ./cart] 插入到 clicks 表时，查询会更新结果表并插入了一行新数据 [Bob, 1]。第三行 [Mary, ./prod?id=1] 将产生已计算的结果行的更新，[Mary, 1] 更新成 [Mary, 2]。最后，当第四行数据加入 clicks 表时，查询将第三行 [Liz, 1] 插入到结果表中。</simpara>
<simpara>我们来写一下第一个查询的代码：</simpara>
<programlisting language="java" linenumbering="unnumbered">import org.apache.flink.api.java.tuple.Tuple2;
import org.apache.flink.streaming.api.datastream.DataStreamSource;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.EnvironmentSettings;
import org.apache.flink.table.api.Table;
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;
import org.apache.flink.types.Row;

import static org.apache.flink.table.api.Expressions.$;

public class QueryGroupByCnt {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        DataStreamSource&lt;Tuple2&lt;String, String&gt;&gt; stream = env
                .fromElements(
                        Tuple2.of("Mary", "./home"),
                        Tuple2.of("Bob", "./cart"),
                        Tuple2.of("Mary", "./prod?id=1"),
                        Tuple2.of("Liz", "./home")
                );

        EnvironmentSettings settings = EnvironmentSettings.newInstance().inStreamingMode().build();

        StreamTableEnvironment tableEnvironment = StreamTableEnvironment.create(env, settings);

        Table table = tableEnvironment
                .fromDataStream(
                        stream,
                        $("f0").as("user"), // 将f0字段命名为user
                        $("f1").as("url")   // 将f1字段命名为url
                );

        // 将动态表注册为临时视图
        tableEnvironment.createTemporaryView("clicks", table);

        // 在临时视图上进行查询
        Table result = tableEnvironment
                .sqlQuery("SELECT user, COUNT(url) FROM clicks GROUP BY user");

        // 将查询结果转换成数据流，由于查询存在聚合操作，所以使用toRetractStream方法
        tableEnvironment.toRetractStream(result, Row.class).print();

        env.execute();
    }
}</programlisting>
<simpara>输出结果是：</simpara>
<programlisting language="text" linenumbering="unnumbered">(true,Mary,1)
(true,Bob,1)
(false,Mary,1)
(true,Mary,2)
(true,Liz,1)</programlisting>
<simpara>第二条查询与第一条类似，但是除了用户属性之外，还将 clicks 分组至每小时滚动窗口中，然后计算 url 数量(基于时间的计算，例如基于特定时间属性的窗口，后面会讨论)。同样，该图显示了不同时间点的输入和输出，以可视化动态表的变化特性。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/query-groupBy-window-cnt.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>与前面一样，左边显示了输入表 clicks。查询每小时持续计算结果并更新结果表。clicks表包含四行带有时间戳(cTime)的数据，时间戳在 12:00:00 和 12:59:59 之间。查询从这个输入计算出两个结果行(每个 user 一个)，并将它们附加到结果表中。对于 13:00:00 和 13:59:59 之间的下一个窗口，clicks 表包含三行，这将导致另外两行被追加到结果表。随着时间的推移，更多的行被添加到 click 中，结果表将被更新。</simpara>
<simpara>我们来写一下第二个查询的代码：</simpara>
<programlisting language="java" linenumbering="unnumbered">import org.apache.flink.api.common.eventtime.SerializableTimestampAssigner;
import org.apache.flink.api.common.eventtime.WatermarkStrategy;
import org.apache.flink.api.java.tuple.Tuple3;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.EnvironmentSettings;
import org.apache.flink.table.api.Table;
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;
import org.apache.flink.types.Row;

import static org.apache.flink.table.api.Expressions.$;

public class Example {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        SingleOutputStreamOperator&lt;Tuple3&lt;String, String, Long&gt;&gt; stream = env
                .fromElements(
                        Tuple3.of("Mary", "./home", 12 * 60 * 60 * 1000L),
                        Tuple3.of("Bob", "./cart", 12 * 60 * 60 * 1000L),
                        Tuple3.of("Mary", "./prod?id=1", 12 * 60 * 60 * 1000L + 2 * 60 * 1000L),
                        Tuple3.of("Mary", "./prod?id=4", 12 * 60 * 60 * 1000L + 55 * 60 * 1000L),
                        Tuple3.of("Bob", "./prod?id=5", 13 * 60 * 60 * 1000L + 60 * 1000L),
                        Tuple3.of("Liz", "./home", 13 * 60 * 60 * 1000L + 30 * 60 * 1000L),
                        Tuple3.of("Liz", "./prod?id=7", 13 * 60 * 60 * 1000L + 59 * 60 * 1000L),
                        Tuple3.of("Mary", "./cart", 14 * 60 * 60 * 1000L),
                        Tuple3.of("Liz", "./home", 14 * 60 * 60 * 1000L + 2 * 60 * 1000L),
                        Tuple3.of("Bob", "./prod?id=3", 14 * 60 * 60 * 1000L + 30 * 60 * 1000L),
                        Tuple3.of("Bob", "./home", 14 * 60 * 60 * 1000L + 40 * 60 * 1000L)
                )
                .assignTimestampsAndWatermarks(
                        WatermarkStrategy.&lt;Tuple3&lt;String, String, Long&gt;&gt;forMonotonousTimestamps()
                                .withTimestampAssigner(new SerializableTimestampAssigner&lt;Tuple3&lt;String, String, Long&gt;&gt;() {
                                    @Override
                                    public long extractTimestamp(Tuple3&lt;String, String, Long&gt; element, long recordTimestamp) {
                                        return element.f2;
                                    }
                                })
                );

        EnvironmentSettings settings = EnvironmentSettings.newInstance().inStreamingMode().build();

        StreamTableEnvironment tableEnvironment = StreamTableEnvironment.create(env, settings);

        Table table = tableEnvironment
                .fromDataStream(
                        stream,
                        $("f0").as("user"),
                        $("f1").as("url"),
                        $("f2").rowtime().as("cTime")   // 将f2指定为事件时间，并命名为cTime
                );

        tableEnvironment.createTemporaryView("clicks", table);

        Table result = tableEnvironment
                .sqlQuery(
                        "SELECT " +
                          "user, " +
                          "TUMBLE_END(" +                 // 窗口结束时间
                            "cTime," +
                            "INTERVAL '1' HOUR)" +
                          "AS endT," +
                          "COUNT(url) AS cnt " +
                        "FROM clicks " +
                        "GROUP BY " +                     // 使用窗口和用户名进行分组
                          "user, " +
                          "TUMBLE(" +                     // 1小时滚动窗口
                            "cTime, " +
                            "INTERVAL '1' HOUR)"
                );

        tableEnvironment
                .toRetractStream(
                        result,
                        Row.class
                )
                .print();

        env.execute();
    }
}</programlisting>
<simpara>输出结果：</simpara>
<programlisting language="text" linenumbering="unnumbered">(true,Mary,1970-01-01T13:00,3)
(true,Bob,1970-01-01T13:00,1)
(true,Liz,1970-01-01T14:00,2)
(true,Bob,1970-01-01T14:00,1)
(true,Bob,1970-01-01T15:00,2)
(true,Mary,1970-01-01T15:00,1)
(true,Liz,1970-01-01T15:00,1)</programlisting>
</section>
<section xml:id="_更新和追加查询">
<title>更新和追加查询</title>
<simpara>虽然这两个示例查询看起来非常相似(都计算分组计数聚合)，但它们在一个重要方面不同：</simpara>
<itemizedlist>
<listitem>
<simpara>第一个查询更新先前输出的结果，即定义结果表的 changelog 流包含 INSERT 和 UPDATE 操作。</simpara>
</listitem>
<listitem>
<simpara>第二个查询只附加到结果表，即结果表的 changelog 流只包含 INSERT 操作。</simpara>
</listitem>
</itemizedlist>
<simpara>一个查询是产生一个只追加的表还是一个更新的表有一些含义：</simpara>
<itemizedlist>
<listitem>
<simpara>产生更新更改的查询通常必须维护更多的状态。</simpara>
</listitem>
<listitem>
<simpara>将 append-only 的表转换为流与将已更新的表转换为流是不同的。</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="_查询限制">
<title>查询限制</title>
<simpara>许多(但不是全部)语义上有效的查询可以作为流上的连续查询进行评估。有些查询代价太高而无法计算，这可能是由于它们需要维护的状态大小，也可能是由于计算更新代价太高。</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">状态大小</emphasis> ： 连续查询在无界流上计算，通常应该运行数周或数月。因此，连续查询处理的数据总量可能非常大。必须更新先前输出的结果的查询需要维护所有输出的行，以便能够更新它们。例如，第一个查询示例需要存储每个用户的 URL 计数，以便能够增加该计数并在输入表接收新行时发送新结果。如果只跟踪注册用户，则要维护的计数数量可能不会太高。但是，如果未注册的用户分配了一个惟一的用户名，那么要维护的计数数量将随着时间增长，并可能最终导致查询失败。</simpara>
</listitem>
</itemizedlist>
<programlisting language="sql" linenumbering="unnumbered">SELECT user, COUNT(url)
FROM clicks
GROUP BY user;</programlisting>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">计算更新</emphasis> ： 有些查询需要重新计算和更新大量已输出的结果行，即使只添加或更新一条输入记录。显然，这样的查询不适合作为连续查询执行。下面的查询就是一个例子，它根据最后一次单击的时间为每个用户计算一个 RANK。一旦 click 表接收到一个新行，用户的 lastAction 就会更新，并必须计算一个新的排名。然而，由于两行不能具有相同的排名，所以所有较低排名的行也需要更新。</simpara>
</listitem>
</itemizedlist>
<programlisting language="sql" linenumbering="unnumbered">SELECT user, RANK() OVER (ORDER BY lastAction)
FROM (
  SELECT user, MAX(cTime) AS lastAction FROM clicks GROUP BY user
);</programlisting>
</section>
<section xml:id="_表到流的转换">
<title>表到流的转换</title>
<simpara>动态表可以像普通数据库表一样通过 INSERT、UPDATE 和 DELETE 来不断修改。它可能是一个只有一行、不断更新的表，也可能是一个 insert-only 的表，没有 UPDATE 和 DELETE 修改，或者介于两者之间的其他表。</simpara>
<simpara>在将动态表转换为流或将其写入外部系统时，需要对这些更改进行编码。Flink的 Table API 和 SQL 支持三种方式来编码一个动态表的变化：</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Append-only 流</emphasis> ： 仅通过 INSERT 操作修改的动态表可以通过输出插入的行转换为流。</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Retract 流</emphasis> ： retract 流包含两种类型的 message： add messages 和 retract messages 。通过将INSERT 操作编码为 add message、将 DELETE 操作编码为 retract message、将 UPDATE 操作编码为更新(先前)行的 retract message 和更新(新)行的 add message，将动态表转换为 retract 流。下图显示了将动态表转换为 retract 流的过程。</simpara>
</listitem>
</itemizedlist>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/undo-redo-mode.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Upsert 流</emphasis> : upsert 流包含两种类型的 message： upsert messages 和delete messages。转换为 upsert 流的动态表需要(可能是组合的)唯一键。通过将 INSERT 和 UPDATE 操作编码为 upsert message，将 DELETE 操作编码为 delete message ，将具有唯一键的动态表转换为流。消费流的算子需要知道唯一键的属性，以便正确地应用 message。与 retract 流的主要区别在于 UPDATE 操作是用单个 message 编码的，因此效率更高。下图显示了将动态表转换为 upsert 流的过程。</simpara>
</listitem>
</itemizedlist>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/redo-mode.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<note>
<simpara>请注意，在将动态表转换为 DataStream 时，只支持 append 流和 retract 流。</simpara>
</note>
<simpara>通过我们以上写的几个程序，可以看到，编写Flink SQL的标准流程如下：</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>获取流环境</simpara>
</listitem>
<listitem>
<simpara>将流读入成DataStream数据结构</simpara>
</listitem>
<listitem>
<simpara>获取表环境</simpara>
</listitem>
<listitem>
<simpara>将DataStream转换成动态表，为了进行SQL查询，再将动态表注册为临时视图</simpara>
</listitem>
<listitem>
<simpara>在临时视图上使用SQL进行查询，查询结果还是动态表，Table数据结构</simpara>
</listitem>
<listitem>
<simpara>将查询结果动态表转换回DataStream数据结构</simpara>
</listitem>
<listitem>
<simpara>执行流程序</simpara>
</listitem>
</orderedlist>
<simpara>以上编写的Flink SQL的程序具有通用性，将数据流转换成动态表或者临时视图进行查询，然后再转换回数据流，尽管看起来有些繁琐，但编写起来很灵活，可以按照自己的想法对数据进行各种ETL操作，读取数据源和写入到第三方设备，也都可以去自定义。我们后面还会介绍一下SQL DDL的写法，编写程序会更加简洁。</simpara>
</section>
</section>
<section xml:id="_flink_sql的使用方法">
<title>Flink SQL的使用方法</title>
<section xml:id="_将数据源转换成动态表">
<title>将数据源转换成动态表</title>
<simpara><emphasis role="strong">原子类型</emphasis></simpara>
<simpara>Flink 将基础数据类型（Integer、Double、String）或者通用数据类型（不可再拆分的数据类型）视为原子类型。原子类型的 DataStream 或者 DataSet 会被转换成只有一条属性的 Table。属性的数据类型可以由原子类型推断出，还可以重新命名属性。</simpara>
<programlisting language="java" linenumbering="unnumbered">StreamTableEnvironment tableEnv = ...;

DataStream&lt;Long&gt; stream = ...;

// 将数据流转换成动态表，动态表只有一个字段myLong
Table table = tableEnv.fromDataStream(stream, $("myLong"));</programlisting>
<simpara><emphasis role="strong">Tuple类型</emphasis></simpara>
<simpara>基于名称的映射适用于任何数据类型包括 POJO 类型。这是定义 table schema 映射最灵活的方式。映射中的所有字段均按名称引用，并且可以通过 as 重命名。字段可以被重新排序和映射。</simpara>
<simpara>若果没有指定任何字段名称，则使用默认的字段名称和复合数据类型的字段顺序，或者使用 f0 表示原子类型。</simpara>
<programlisting language="java" linenumbering="unnumbered">StreamTableEnvironment tableEnv = ...;

DataStream&lt;Tuple2&lt;Long, Integer&gt;&gt; stream = ...;

// 将数据流转换成只包含f1字段的动态表
Table table = tableEnv.fromDataStream(stream, $("f1"));

// 将数据流转换成包含f0和f1字段的动态表，在动态表中f0和f1位置交换
Table table = tableEnv.fromDataStream(stream, $("f1"), $("f0"));

// 命名别名
Table table = tableEnv.fromDataStream(stream, $("f1").as("myInt"), $("f0").as("myLong"));</programlisting>
<simpara><emphasis role="strong">POJO 类型</emphasis></simpara>
<simpara>Flink 支持 POJO 类型作为复合类型。</simpara>
<simpara>在不指定字段名称的情况下将 POJO 类型的 DataStream 转换成 Table 时，将使用原始 POJO 类型字段的名称。名称映射需要原始名称，并且不能按位置进行。字段可以使用别名（带有 as 关键字）来重命名，重新排序和投影。</simpara>
<programlisting language="java" linenumbering="unnumbered">StreamTableEnvironment tableEnv = ...;

DataStream&lt;UserBehavior&gt; stream = ...;

Table table = tableEnv.fromDataStream(stream, $("userId").as("myUserId"), $("itemId").as("myItemId"));

Table table = tableEnv.fromDataStream(stream, $("userId"));

Table table = tableEnv.fromDataStream(stream, $("name").as("myName"));</programlisting>
</section>
<section xml:id="_时间属性和窗口">
<title>时间属性和窗口</title>
<simpara>我们在之前的章节中学习了Flink中的窗口概念，分为滚动窗口、滑动窗口和会话窗口。而在Flink Table API &amp; SQL中其实有两种类型的窗口存在。</simpara>
<itemizedlist>
<listitem>
<simpara>Flink中窗口到Table中的映射</simpara>
</listitem>
<listitem>
<simpara>Over窗口</simpara>
</listitem>
</itemizedlist>
<simpara>Over窗口的概念来自于标准SQL语法，也就是Over子句，由于本书侧重于Flink的快速上手，所以Over窗口不做重点讲解，读者朋友可以自行查看。</simpara>
<simpara>我们这里主要讲解的是Flink中的窗口概念和时间属性在Table API &amp; SQL中如何来定义和使用。Flink中最常用的时间属性有处理时间和事件时间，我们来看一下如何定义：</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>处理时间</simpara>
</listitem>
</orderedlist>
<simpara>处理时间属性可以在 schema 定义的时候用 .proctime 后缀来定义。由于在数据中并没有处理时间这个字段，所以处理时间属性一定不能定义在一个已有字段上，所以它只能定义在 schema 定义的最后。</simpara>
<simpara>处理时间的定义方法：</simpara>
<programlisting language="java" linenumbering="unnumbered">DataStream&lt;Tuple2&lt;String, String&gt;&gt; stream = ...;

// 声明一个额外的字段作为时间属性字段
Table table = tEnv.fromDataStream(stream, $("user"), $("url"), $("user_action_time").proctime());

// 注册临时视图
tEnv.createTemporaryView("clicks", table);

// 定义窗口
// 一小时的滚动窗口，时间戳使用处理时间user_action_time
tEnv.sqlQuery("SELECT user, COUNT(url), TUMBLE_START(user_action_time, INTERVAL '1' HOURS), TUMBLE_END(user_action_time, INTERVAL '1' HOURS) FROM clicks GROUP BY user, TUMBLE(user_action_time, INTERVAL '1' HOURS)");

// 滑动窗口长度1小时，滑动距离30分钟
tEnv.sqlQuery("SELECT user, COUNT(url), HOP_START(user_action_time, INTERVAL '30' MINUTES, INTERVAL '1' HOURS), HOP_END(user_action_time, INTERVAL '30' MINUTES, INTERVAL '1' HOURS) FROM clicks GROUP BY user, HOP(user_action_time, INTERVAL '30' MINUTES, INTERVAL '1' HOURS)");

// 会话窗口，超时时间是1小时
tEnv.sqlQuery("SELECT user, COUNT(url), SESSION_START(user_action_time, INTERVAL '1' HOURS), SESSION_END(user_action_time, INTERVAL '1' HOURS) FROM clicks GROUP BY user, SESSION(user_action_time, INTERVAL '1' HOURS)");</programlisting>
<orderedlist numeration="arabic">
<listitem>
<simpara>事件时间</simpara>
</listitem>
</orderedlist>
<simpara>事件时间属性可以用 .rowtime 后缀在定义 DataStream schema 的时候来定义。时间戳和水位线在这之前一定是在 DataStream 上已经定义好了。 在从 DataStream 转换到 Table 时，由于 DataStream 没有时区概念，因此 Flink 总是将 rowtime 属性解析成 TIMESTAMP WITHOUT TIME ZONE 类型，并且将所有事件时间的值都视为 UTC 时区的值。</simpara>
<simpara>在从 DataStream 到 Table 转换时定义事件时间属性有两种方式。取决于用 .rowtime 后缀修饰的字段名字是否是已有字段，事件时间字段可以是：</simpara>
<itemizedlist>
<listitem>
<simpara>在 schema 的结尾追加一个新的字段</simpara>
</listitem>
<listitem>
<simpara>替换一个已经存在的字段。</simpara>
</listitem>
</itemizedlist>
<simpara>不管在哪种情况下，事件时间字段都表示 DataStream 中定义的事件的时间戳。事件时间的定义方法：</simpara>
<programlisting language="java" linenumbering="unnumbered">// 方法一:

// 基于 stream 中的事件产生时间戳和 watermark
DataStream&lt;Tuple2&lt;String, String&gt;&gt; stream = inputStream.assignTimestampsAndWatermarks(...);

// 声明一个额外的逻辑字段作为事件时间属性
Table table = tEnv.fromDataStream(stream, $("user_name"), $("data"), $("user_action_time").rowtime());

// 方法二:

// 从第一个字段获取事件时间，并且产生 watermark
DataStream&lt;Tuple3&lt;String, String, Long&gt;&gt; stream = inputStream.assignTimestampsAndWatermarks(...);

// 第一个字段已经用作事件时间抽取了，不用再用一个新字段来表示事件时间了
Table table = tEnv.fromDataStream(stream, $("user_name"), $("data"), $("user_action_time").rowtime());

// 注册临时视图
tEnv.createTemporaryView("clicks", table);

// 定义窗口
// 一小时的滚动窗口，时间戳使用处理时间user_action_time
tEnv.sqlQuery("SELECT user, COUNT(url), TUMBLE_START(user_action_time, INTERVAL '1' HOURS), TUMBLE_END(user_action_time, INTERVAL '1' HOURS) FROM clicks GROUP BY user, TUMBLE(user_action_time, INTERVAL '1' HOURS)");

// 滑动窗口长度1小时，滑动距离30分钟
tEnv.sqlQuery("SELECT user, COUNT(url), HOP_START(user_action_time, INTERVAL '30' MINUTES, INTERVAL '1' HOURS), HOP_END(user_action_time, INTERVAL '30' MINUTES, INTERVAL '1' HOURS) FROM clicks GROUP BY user, HOP(user_action_time, INTERVAL '30' MINUTES, INTERVAL '1' HOURS)");

// 会话窗口，超时时间是1小时
tEnv.sqlQuery("SELECT user, COUNT(url), SESSION_START(user_action_time, INTERVAL '1' HOURS), SESSION_END(user_action_time, INTERVAL '1' HOURS) FROM clicks GROUP BY user, SESSION(user_action_time, INTERVAL '1' HOURS)");</programlisting>
</section>
<section xml:id="_over聚合">
<title>OVER聚合</title>
<simpara>OVER聚合为一系列有序行中的每个输入行计算一个聚合值。与GROUP BY聚合相反，OVER聚合不会将每个组的分组数据聚合为一行。OVER聚合会为每个输入行生成一个聚合值。</simpara>
<simpara>我们现在来举一个例子，计算一下1小时滚动窗口中访问量最大的用户。我们这里的思路是：</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>计算每个用户在每个一小时滚动窗口中的访问次数</simpara>
</listitem>
<listitem>
<simpara>按照窗口结束时间分区，然后按照浏览量降序排列，并取出访问量最大的用户，也就是排序第一的数据。</simpara>
</listitem>
</orderedlist>
<note>
<simpara>所以这里我们其实是使用Over聚合来实现了一个TopN的需求。</simpara>
</note>
<programlisting language="java" linenumbering="unnumbered">import org.apache.flink.api.common.eventtime.SerializableTimestampAssigner;
import org.apache.flink.api.common.eventtime.WatermarkStrategy;
import org.apache.flink.api.java.tuple.Tuple3;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.EnvironmentSettings;
import org.apache.flink.table.api.Table;
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;
import org.apache.flink.types.Row;

import static org.apache.flink.table.api.Expressions.$;

public class TopPvUserPerWindow {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        SingleOutputStreamOperator&lt;Tuple3&lt;String, String, Long&gt;&gt; stream = env
                .fromElements(
                        Tuple3.of("Mary", "./home", 12 * 60 * 60 * 1000L),
                        Tuple3.of("Bob", "./cart", 12 * 60 * 60 * 1000L),
                        Tuple3.of("Mary", "./prod?id=1", 12 * 60 * 60 * 1000L + 2 * 60 * 1000L),
                        Tuple3.of("Mary", "./prod?id=4", 12 * 60 * 60 * 1000L + 55 * 60 * 1000L),
                        Tuple3.of("Bob", "./prod?id=5", 13 * 60 * 60 * 1000L + 60 * 1000L),
                        Tuple3.of("Liz", "./home", 13 * 60 * 60 * 1000L + 30 * 60 * 1000L),
                        Tuple3.of("Liz", "./prod?id=7", 13 * 60 * 60 * 1000L + 59 * 60 * 1000L),
                        Tuple3.of("Mary", "./cart", 14 * 60 * 60 * 1000L),
                        Tuple3.of("Liz", "./home", 14 * 60 * 60 * 1000L + 2 * 60 * 1000L),
                        Tuple3.of("Bob", "./prod?id=3", 14 * 60 * 60 * 1000L + 30 * 60 * 1000L),
                        Tuple3.of("Bob", "./home", 14 * 60 * 60 * 1000L + 40 * 60 * 1000L)
                )
                .assignTimestampsAndWatermarks(
                        WatermarkStrategy.&lt;Tuple3&lt;String, String, Long&gt;&gt;forMonotonousTimestamps()
                                .withTimestampAssigner(new SerializableTimestampAssigner&lt;Tuple3&lt;String, String, Long&gt;&gt;() {
                                    @Override
                                    public long extractTimestamp(Tuple3&lt;String, String, Long&gt; element, long recordTimestamp) {
                                        return element.f2;
                                    }
                                })
                );

        EnvironmentSettings settings = EnvironmentSettings.newInstance().inStreamingMode().build();

        StreamTableEnvironment tableEnvironment = StreamTableEnvironment.create(env, settings);

        Table table = tableEnvironment
                .fromDataStream(
                        stream,
                        $("f0").as("user"),
                        $("f1").as("url"),
                        $("f2").rowtime().as("cTime")
                );

        tableEnvironment.createTemporaryView("clicks", table);

        // 每个用户在每个一小时滚动窗口中的访问次数
        String innerSQL = "SELECT " +
                            "user, " +
                            "TUMBLE_END(" +
                              "cTime," +
                              "INTERVAL '1' HOUR)" +
                            "AS endT," +
                            "COUNT(url) AS cnt " +
                          "FROM clicks " +
                            "GROUP BY " +
                              "user, " +
                              "TUMBLE(" +
                                "cTime, " +
                                "INTERVAL '1' HOUR)";

        // 按照窗口结束时间分区，然后按照浏览量降序排列
        String midSQL = "SELECT *, ROW_NUMBER() OVER (PARTITION BY endT ORDER BY cnt DESC) as row_num" +
                " FROM (" + innerSQL + ")";

        // 取出第一名
        String outerSQL = "SELECT * FROM (" + midSQL + ") WHERE row_num = 1";

        Table result = tableEnvironment.sqlQuery(outerSQL);

        tableEnvironment
                .toRetractStream(
                        result,
                        Row.class
                )
                .print();

        env.execute();
    }
}</programlisting>
<simpara>OVER窗口是在有序的行序列上定义的。由于表没有固有的顺序，因此ORDER BY子句是强制书写的。</simpara>
<simpara>以上的程序涉及到了TopN的求值，也就是计算每个窗口中访问量最大的用户。在计算TopN的SQL中，涉及到了这样几个参数：</simpara>
<itemizedlist>
<listitem>
<simpara>ROW_NUMBER()：根据分区中各行的顺序，为每一行分配一个唯一的顺序号（从1开始）。目前，我们仅支持ROW_NUMBER的使用。</simpara>
</listitem>
<listitem>
<simpara>WHERE row_num &#8656; N：Flink要求row_num &#8656; N子句才能将该查询识别为Top-N查询。N代表将保留N个最小或最大记录。</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="_联结查询">
<title>联结查询</title>
<simpara>Flink SQL支持对动态表进行复杂而灵活的联接操作。有几种不同类型的联接可解决可能需要的各种语义查询。</simpara>
<simpara>默认情况下，未优化连接顺序。表按照在FROM子句中指定的顺序进行连接。通过首先列出更新频率最低的表，最后列出更新频率最高的表，可以调整联接查询的性能。确保以不产生交叉联接（笛卡尔乘积）的顺序指定表，该顺序不受支持，并且会导致查询失败。</simpara>
<section xml:id="_常规联结查询">
<title>常规联结查询</title>
<simpara>常规联接是最通用的联接类型，在该联接中，任何新记录或对联接的任一侧所做的更改都是可见的，并且会影响整个联接结果。例如，如果左侧有新记录，则它将与右侧的所有以前和将来的记录合并在一起。</simpara>
<programlisting language="sql" linenumbering="unnumbered">SELECT * FROM Orders
INNER JOIN Product
ON Orders.productId = Product.id</programlisting>
<simpara>对于流查询，常规联接的语法是最灵活的，并且允许任何类型的更新（插入，更新，删除）输入表。但是，此操作具有重要的操作含义：它要求将连接输入的两端始终保持在Flink状态。因此，根据所有输入表和中间联接结果的不同输入行的数量，用于计算查询结果的所需状态可能会无限增长。我们可以为查询配置提供适当的状态生存时间（TTL），以防止状态大小过大。请注意，这可能会影响查询结果的正确性。</simpara>
<simpara>对于流式查询，根据聚合的类型和不同的分组键的数量，计算查询结果所需的状态可能会无限增长。请提供具有有效保留间隔的查询配置，以防止出现过多的状态。</simpara>
<simpara><emphasis role="strong">等值内联结查询</emphasis></simpara>
<simpara>返回受联接条件限制的简单笛卡尔积。当前，仅支持等值联接，即具有至少一个具有相等谓词的联合条件的联接。</simpara>
<programlisting language="sql" linenumbering="unnumbered">SELECT *
FROM Orders
INNER JOIN Product
ON Orders.product_id = Product.id</programlisting>
<simpara><emphasis role="strong">等值外联结查询</emphasis></simpara>
<simpara>返回合格笛卡尔乘积中的所有行（即，通过其联接条件的所有组合行），以及外部表中联接条件与另一表的任何行都不匹配的每行的一个副本。 Flink支持LEFT，RIGHT和FULL外部联接。当前，仅支持等值联结查询，即，具有至少一个具有相等谓词的联合条件的联接。</simpara>
<programlisting language="sql" linenumbering="unnumbered">SELECT *
FROM Orders
LEFT JOIN Product
ON Orders.product_id = Product.id

SELECT *
FROM Orders
RIGHT JOIN Product
ON Orders.product_id = Product.id

SELECT *
FROM Orders
FULL OUTER JOIN Product
ON Orders.product_id = Product.id</programlisting>
</section>
<section xml:id="_基于间隔的联结查询">
<title>基于间隔的联结查询</title>
<simpara>我们在之前的章节中已经学习过intervalJoin方法的使用，我们来看一下在SQL中如何编写基于间隔的联结查询。</simpara>
<simpara>举个例子，如果订单在收到订单后四个小时内发货，则此查询会将所有订单与其相应的发货合并在一起。</simpara>
<programlisting language="sql" linenumbering="unnumbered">SELECT *
FROM Orders o, Shipments s
WHERE o.id = s.order_id
AND o.order_time BETWEEN s.ship_time - INTERVAL '4' HOUR AND s.ship_time</programlisting>
<simpara>以下谓词是有效间隔加入条件的示例：</simpara>
<itemizedlist>
<listitem>
<simpara>ltime = rtime</simpara>
</listitem>
<listitem>
<simpara>ltime &gt;= rtime AND ltime &lt; rtime + INTERVAL '10' MINUTE</simpara>
</listitem>
<listitem>
<simpara>ltime BETWEEN rtime - INTERVAL '10' SECOND AND rtime + INTERVAL '5' SECOND</simpara>
</listitem>
</itemizedlist>
<simpara>对于流查询，与常规联接相比，间隔联接仅支持具有时间属性的仅追加表。由于时间属性是准单调增加的，因此Flink可以从其状态中删除旧值，而不会影响结果的正确性。</simpara>
</section>
</section>
</section>
<section xml:id="_用户自定义函数">
<title>用户自定义函数</title>
<simpara>Flink Table API &amp; SQL 为用户提供了很多内置的系统函数，例如大小写转换、比较大小等有用的函数，由于这些函数在MySQL、Hive等SQL写法中也很常见，我们这里就不详细讲解了，具体用法可以参阅官方文档。我们这里要重点讲解的是如何编写用户自定义函数。</simpara>
<simpara>当前 Flink 有如下几种用户自定义函数：</simpara>
<itemizedlist>
<listitem>
<simpara>标量函数： 将标量值转换成一个新标量值；</simpara>
</listitem>
<listitem>
<simpara>表值函数： 将标量值转换成新的行数据；</simpara>
</listitem>
<listitem>
<simpara>聚合函数： 将多行数据里的标量值转换成一个新标量值；</simpara>
</listitem>
<listitem>
<simpara>表值聚合函数： 将多行数据里的标量值转换成新的行数据；</simpara>
</listitem>
<listitem>
<simpara>异步表值函数： 是异步查询外部数据系统的特殊函数。</simpara>
</listitem>
</itemizedlist>
<section xml:id="_标量函数">
<title>标量函数</title>
<simpara>自定义标量函数可以把 0 到多个标量值映射成 1 个标量值，任何数据类型都可作为求值方法的参数和返回值类型。</simpara>
<simpara>想要实现自定义标量函数，你需要扩展 org.apache.flink.table.functions 里面的 ScalarFunction 并且实现一个或者多个求值方法。标量函数的行为取决于你写的求值方法。求值方法必须是 public 的，而且名字必须是 eval。</simpara>
<simpara>下面的例子展示了如何实现一个求哈希值的函数并在查询里调用它，这里要注意的是，在eval的参数中，我们对输入参数的类型做了标注，使用了DataTypeHint方法。</simpara>
<programlisting language="java" linenumbering="unnumbered">public static class HashFunction extends ScalarFunction {
  // 接受任意类型输入，返回 INT 型输出
  public int eval(@DataTypeHint(inputGroup = InputGroup.ANY) Object o) {
    return o.hashCode();
  }
}

// 注册函数
env.createTemporarySystemFunction("HashFunction", HashFunction.class);

// 在 SQL 里调用注册好的函数
env.sqlQuery("SELECT HashFunction(myField) FROM MyTable");</programlisting>
</section>
<section xml:id="_表值函数">
<title>表值函数</title>
<simpara>跟自定义标量函数一样，自定义表值函数的输入参数也可以是 0 到多个标量。但是跟标量函数只能返回一个值不同的是，它可以返回任意多行。返回的每一行可以包含 1 到多列，如果输出行只包含 1 列，会省略结构化信息并生成标量值，这个标量值在运行阶段会隐式地包装进行里。</simpara>
<simpara>要定义一个表值函数，你需要扩展 org.apache.flink.table.functions 下的 TableFunction，可以通过实现多个名为 eval 的方法对求值方法进行重载。像其他函数一样，输入和输出类型也可以通过反射自动提取出来。表值函数返回的表的类型取决于 TableFunction 类的泛型参数 T，不同于标量函数，表值函数的求值方法本身不包含返回类型，而是通过 collect(T) 方法来发送要输出的行。</simpara>
<simpara>在 SQL 里面用 JOIN 或者 以 ON TRUE 为条件的 LEFT JOIN 来配合 LATERAL TABLE(&lt;TableFunction&gt;) 的使用。</simpara>
<simpara>下面的例子展示了如何实现一个分隔函数并在查询里调用它：</simpara>
<programlisting language="java" linenumbering="unnumbered">// 注意这里的类型标注，输出是Row类型，Row中包含两个字段：word和length。
@FunctionHint(output = @DataTypeHint("ROW&lt;word STRING, length INT&gt;"))
public static class SplitFunction extends TableFunction&lt;Row&gt; {

  public void eval(String str) {
    for (String s : str.split(" ")) {
      // 使用collect(...)方法向下游发送一行数据
      collect(Row.of(s, s.length()));
    }
  }
}

// 注册函数
env.createTemporarySystemFunction("SplitFunction", SplitFunction.class);

// 在 SQL 里调用注册好的函数
env.sqlQuery(
  "SELECT myField, word, length " +
  "FROM MyTable, LATERAL TABLE(SplitFunction(myField))");
env.sqlQuery(
  "SELECT myField, word, length " +
  "FROM MyTable " +
  "LEFT JOIN LATERAL TABLE(SplitFunction(myField)) ON TRUE");

// 在 SQL 里重命名函数字段
env.sqlQuery(
  "SELECT myField, newWord, newLength " +
  "FROM MyTable " +
  "LEFT JOIN LATERAL TABLE(SplitFunction(myField)) AS T(newWord, newLength) ON TRUE");</programlisting>
</section>
<section xml:id="_聚合函数">
<title>聚合函数</title>
<simpara>自定义聚合函数（UDAGG）是把一个表（一行或者多行，每行可以有一列或者多列）聚合成一个标量值。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/udagg-mechanism.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>上面的图片展示了一个聚合的例子。假设你有一个关于饮料的表。表里面有三个字段，分别是 id、name、price，表里有 5 行数据。假设你需要找到所有饮料里最贵的饮料的价格，即执行一个 max() 聚合。你需要遍历所有 5 行数据，而结果就只有一个数值。</simpara>
<simpara>自定义聚合函数是通过扩展 AggregateFunction 来实现的。AggregateFunction 的工作过程如下。首先，它需要一个 accumulator，它是一个数据结构，存储了聚合的中间结果。通过调用 AggregateFunction 的 createAccumulator() 方法创建一个空的 accumulator。接下来，对于每一行数据，会调用 accumulate() 方法来更新 accumulator。当所有的数据都处理完了之后，通过调用 getValue 方法来计算和返回最终的结果。</simpara>
<simpara>下面几个方法是每个 AggregateFunction 必须要实现的：</simpara>
<itemizedlist>
<listitem>
<simpara>createAccumulator()</simpara>
</listitem>
<listitem>
<simpara>accumulate()</simpara>
</listitem>
<listitem>
<simpara>getValue()</simpara>
</listitem>
</itemizedlist>
<simpara>Flink 的类型推导在遇到复杂类型的时候可能会推导出错误的结果，比如那些非基本类型和普通的 POJO 类型的复杂类型。所以跟 ScalarFunction 和 TableFunction 一样，AggregateFunction 也提供了 AggregateFunction#getResultType() 和 AggregateFunction#getAccumulatorType() 来分别指定返回值类型和 accumulator 的类型，两个函数的返回值类型也都是 TypeInformation。</simpara>
<simpara>除了上面的方法，还有几个方法可以选择实现。这些方法有些可以让查询更加高效，而有些是在某些特定场景下必须要实现的。例如，如果聚合函数用在会话窗口（当两个会话窗口合并的时候需要 merge 他们的 accumulator）的话，merge() 方法就是必须要实现的。</simpara>
<simpara>AggregateFunction 的以下方法在某些场景下是必须实现的：</simpara>
<itemizedlist>
<listitem>
<simpara>retract() 在 bounded OVER 窗口中是必须实现的。</simpara>
</listitem>
<listitem>
<simpara>merge() 在许多批式聚合和会话以及滚动窗口聚合中是必须实现的。除此之外，这个方法对于优化也很多帮助。例如，两阶段聚合优化就需要所有的 AggregateFunction 都实现 merge 方法。</simpara>
</listitem>
<listitem>
<simpara>resetAccumulator() 在许多批式聚合中是必须实现的。</simpara>
</listitem>
</itemizedlist>
<simpara>AggregateFunction 的所有方法都必须是 public 的，不能是 static 的，而且名字必须跟上面写的一样。createAccumulator、getValue、getResultType 以及 getAccumulatorType 这几个函数是在抽象类 AggregateFunction 中定义的，而其他函数都是约定的方法。如果要定义一个聚合函数，你需要扩展 org.apache.flink.table.functions.AggregateFunction，并且实现一个（或者多个）accumulate 方法。accumulate 方法可以重载，每个方法的参数类型不同，并且支持变长参数。</simpara>
<simpara>下面的例子展示了如何：</simpara>
<itemizedlist>
<listitem>
<simpara>定义一个聚合函数来计算某一列的加权平均，</simpara>
</listitem>
<listitem>
<simpara>在 TableEnvironment 中注册函数，</simpara>
</listitem>
<listitem>
<simpara>在查询中使用函数。</simpara>
</listitem>
</itemizedlist>
<simpara>为了计算加权平均值，accumulator 需要存储加权总和以及数据的条数。在我们的例子里，我们定义了一个类 WeightedAvgAccum 来作为 accumulator。Flink 的 checkpoint 机制会自动保存 accumulator，在失败时进行恢复，以此来保证精确一次的语义。</simpara>
<simpara>我们的 WeightedAvg（聚合函数）的 accumulate 方法有三个输入参数。第一个是 WeightedAvgAccum accumulator，另外两个是用户自定义的输入：输入的值 ivalue 和 输入的权重 iweight。尽管 retract()、merge()、resetAccumulator() 这几个方法在大多数聚合类型中都不是必须实现的，我们也在样例中提供了他们的实现。请注意我们在 Scala 样例中也是用的是 Java 的基础类型，并且定义了 getResultType() 和 getAccumulatorType()，因为 Flink 的类型推导对于 Scala 的类型推导做的不是很好。</simpara>
<programlisting language="java" linenumbering="unnumbered">/**
 * 累加器的定义
 */
public static class WeightedAvgAccum {
    public long sum = 0;
    public int count = 0;
}

/**
 * 计算加权平均的用户自定义函数
 */
public static class WeightedAvg extends AggregateFunction&lt;Long, WeightedAvgAccum&gt; {

    @Override
    public WeightedAvgAccum createAccumulator() {
        return new WeightedAvgAccum();
    }

    @Override
    public Long getValue(WeightedAvgAccum acc) {
        if (acc.count == 0) {
            return null;
        } else {
            return acc.sum / acc.count;
        }
    }

    public void accumulate(WeightedAvgAccum acc, long iValue, int iWeight) {
        acc.sum += iValue * iWeight;
        acc.count += iWeight;
    }

    public void retract(WeightedAvgAccum acc, long iValue, int iWeight) {
        acc.sum -= iValue * iWeight;
        acc.count -= iWeight;
    }

    public void merge(WeightedAvgAccum acc, Iterable&lt;WeightedAvgAccum&gt; it) {
        Iterator&lt;WeightedAvgAccum&gt; iter = it.iterator();
        while (iter.hasNext()) {
            WeightedAvgAccum a = iter.next();
            acc.count += a.count;
            acc.sum += a.sum;
        }
    }

    public void resetAccumulator(WeightedAvgAccum acc) {
        acc.count = 0;
        acc.sum = 0L;
    }
}

// 注册函数
StreamTableEnvironment tEnv = ...
tEnv.registerFunction("wAvg", new WeightedAvg());

// 使用函数
tEnv.sqlQuery("SELECT user, wAvg(points, level) AS avgPoints FROM userScores GROUP BY user");</programlisting>
</section>
<section xml:id="_表值聚合函数">
<title>表值聚合函数</title>
<simpara>自定义表值聚合函数（UDTAGG）可以把一个表（一行或者多行，每行有一列或者多列）聚合成另一张表，结果中可以有多行多列。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/udtagg-mechanism.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>上图展示了一个表值聚合函数的例子。假设你有一个饮料的表，这个表有 3 列，分别是 id、name 和 price，一共有 5 行。假设你需要找到价格最高的两个饮料，类似于 top2() 表值聚合函数。你需要遍历所有 5 行数据，结果是有 2 行数据的一个表。</simpara>
<simpara>用户自定义表值聚合函数是通过扩展 TableAggregateFunction 类来实现的。一个 TableAggregateFunction 的工作过程如下。首先，它需要一个 accumulator，这个 accumulator 负责存储聚合的中间结果。 通过调用 TableAggregateFunction 的 createAccumulator 方法来构造一个空的 accumulator。接下来，对于每一行数据，会调用 accumulate 方法来更新 accumulator。当所有数据都处理完之后，调用 emitValue 方法来计算和返回最终的结果。</simpara>
<simpara>下面几个 TableAggregateFunction 的方法是必须要实现的：</simpara>
<itemizedlist>
<listitem>
<simpara>createAccumulator()</simpara>
</listitem>
<listitem>
<simpara>accumulate()</simpara>
</listitem>
</itemizedlist>
<simpara>Flink 的类型推导在遇到复杂类型的时候可能会推导出错误的结果，比如那些非基本类型和普通的 POJO 类型的复杂类型。所以类似于 ScalarFunction 和 TableFunction，TableAggregateFunction 也提供了 TableAggregateFunction#getResultType() 和 TableAggregateFunction#getAccumulatorType() 方法来指定返回值类型和 accumulator 的类型，这两个方法都需要返回 TypeInformation。</simpara>
<simpara>除了上面的方法，还有几个其他的方法可以选择性的实现。有些方法可以让查询更加高效，而有些方法对于某些特定场景是必须要实现的。比如，在会话窗口（当两个会话窗口合并时会合并两个 accumulator）中使用聚合函数时，必须要实现merge() 方法。</simpara>
<simpara>下面几个 TableAggregateFunction 的方法在某些特定场景下是必须要实现的：</simpara>
<itemizedlist>
<listitem>
<simpara>retract() 在 bounded OVER 窗口中的聚合函数必须要实现。</simpara>
</listitem>
<listitem>
<simpara>merge() 在许多批式聚合和以及流式会话和滑动窗口聚合中是必须要实现的。</simpara>
</listitem>
<listitem>
<simpara>resetAccumulator() 在许多批式聚合中是必须要实现的。</simpara>
</listitem>
<listitem>
<simpara>emitValue() 在批式聚合以及窗口聚合中是必须要实现的。</simpara>
</listitem>
</itemizedlist>
<simpara>下面的 TableAggregateFunction 的方法可以提升流式任务的效率：</simpara>
<itemizedlist>
<listitem>
<simpara>emitUpdateWithRetract() 在 retract 模式下，该方法负责发送被更新的值。</simpara>
</listitem>
</itemizedlist>
<simpara>emitValue 方法会发送所有 accumulator 给出的结果。拿 TopN 来说，emitValue 每次都会发送所有的最大的 n 个值。这在流式任务中可能会有一些性能问题。为了提升性能，用户可以实现 emitUpdateWithRetract 方法。这个方法在 retract 模式下会增量的输出结果，比如有数据更新了，我们必须要撤回老的数据，然后再发送新的数据。如果定义了 emitUpdateWithRetract 方法，那它会优先于 emitValue 方法被使用，因为一般认为 emitUpdateWithRetract 会更加高效，因为它的输出是增量的。</simpara>
<simpara>TableAggregateFunction 的所有方法都必须是 public 的、非 static 的，而且名字必须跟上面提到的一样。createAccumulator、getResultType 和 getAccumulatorType 这三个方法是在抽象父类 TableAggregateFunction 中定义的，而其他的方法都是约定的方法。要实现一个表值聚合函数，你必须扩展 org.apache.flink.table.functions.TableAggregateFunction，并且实现一个（或者多个）accumulate 方法。accumulate 方法可以有多个重载的方法，也可以支持变长参数。</simpara>
<simpara>下面的例子展示了如何</simpara>
<itemizedlist>
<listitem>
<simpara>定义一个 TableAggregateFunction 来计算给定列的最大的 2 个值，</simpara>
</listitem>
<listitem>
<simpara>在 TableEnvironment 中注册函数，</simpara>
</listitem>
<listitem>
<simpara>在 Table API 查询中使用函数（当前只在 Table API 中支持 TableAggregateFunction）。</simpara>
</listitem>
<listitem>
<simpara>为了计算最大的 2 个值，accumulator 需要保存当前看到的最大的 2 个值。在我们的例子中，我们定义了类 Top2Accum 来作为 accumulator。Flink 的 checkpoint 机制会自动保存 accumulator，并且在失败时进行恢复，来保证精确一次的语义。</simpara>
</listitem>
</itemizedlist>
<simpara>我们的 Top2 表值聚合函数（TableAggregateFunction）的 accumulate() 方法有两个输入，第一个是 Top2Accum accumulator，另一个是用户定义的输入：输入的值 v。尽管 merge() 方法在大多数聚合类型中不是必须的，我们也在样例中提供了它的实现。请注意，我们在 Scala 样例中也使用的是 Java 的基础类型，并且定义了 getResultType() 和 getAccumulatorType() 方法，因为 Flink 的类型推导对于 Scala 的类型推导支持的不是很好。</simpara>
<programlisting language="java" linenumbering="unnumbered">/**
 * 累加器的定义，用来保存第一名和第二名的数据
 */
public class Top2Accum {
    public Integer first;
    public Integer second;
}

/**
 * 用户自定义函数
 */
public static class Top2 extends TableAggregateFunction&lt;Tuple2&lt;Integer, Integer&gt;, Top2Accum&gt; {

    @Override
    public Top2Accum createAccumulator() {
        Top2Accum acc = new Top2Accum();
        acc.first = Integer.MIN_VALUE;
        acc.second = Integer.MIN_VALUE;
        return acc;
    }


    public void accumulate(Top2Accum acc, Integer v) {
        if (v &gt; acc.first) {
            acc.second = acc.first;
            acc.first = v;
        } else if (v &gt; acc.second) {
            acc.second = v;
        }
    }

    public void merge(Top2Accum acc, java.lang.Iterable&lt;Top2Accum&gt; iterable) {
        for (Top2Accum otherAcc : iterable) {
            accumulate(acc, otherAcc.first);
            accumulate(acc, otherAcc.second);
        }
    }

    public void emitValue(Top2Accum acc, Collector&lt;Tuple2&lt;Integer, Integer&gt;&gt; out) {
        // emit the value and rank
        if (acc.first != Integer.MIN_VALUE) {
            out.collect(Tuple2.of(acc.first, 1));
        }
        if (acc.second != Integer.MIN_VALUE) {
            out.collect(Tuple2.of(acc.second, 2));
        }
    }
}

// 注册函数
StreamTableEnvironment tEnv = ...
tEnv.registerFunction("top2", new Top2());

// 初始化表
Table tab = ...;

// 使用函数
tab.groupBy("key")
    .flatAggregate("top2(a) as (v, rank)")
    .select("key, v, rank");</programlisting>
<simpara>下面的例子展示了如何使用 emitUpdateWithRetract 方法来只发送更新的数据。为了只发送更新的结果，accumulator 保存了上一次的最大的2个值，也保存了当前最大的2个值。注意：如果 TopN 中的 n 非常大，这种既保存上次的结果，也保存当前的结果的方式不太高效。一种解决这种问题的方式是把输入数据直接存储到 accumulator 中，然后在调用 emitUpdateWithRetract 方法时再进行计算。</simpara>
<programlisting language="java" linenumbering="unnumbered">/**
 * Accumulator for Top2.
 */
public class Top2Accum {
    public Integer first;
    public Integer second;
    public Integer oldFirst;
    public Integer oldSecond;
}

/**
 * The top2 user-defined table aggregate function.
 */
public static class Top2 extends TableAggregateFunction&lt;Tuple2&lt;Integer, Integer&gt;, Top2Accum&gt; {

    @Override
    public Top2Accum createAccumulator() {
        Top2Accum acc = new Top2Accum();
        acc.first = Integer.MIN_VALUE;
        acc.second = Integer.MIN_VALUE;
        acc.oldFirst = Integer.MIN_VALUE;
        acc.oldSecond = Integer.MIN_VALUE;
        return acc;
    }

    public void accumulate(Top2Accum acc, Integer v) {
        if (v &gt; acc.first) {
            acc.second = acc.first;
            acc.first = v;
        } else if (v &gt; acc.second) {
            acc.second = v;
        }
    }

    public void emitUpdateWithRetract(Top2Accum acc, RetractableCollector&lt;Tuple2&lt;Integer, Integer&gt;&gt; out) {
        if (!acc.first.equals(acc.oldFirst)) {
            // if there is an update, retract old value then emit new value.
            if (acc.oldFirst != Integer.MIN_VALUE) {
                out.retract(Tuple2.of(acc.oldFirst, 1));
            }
            out.collect(Tuple2.of(acc.first, 1));
            acc.oldFirst = acc.first;
        }

        if (!acc.second.equals(acc.oldSecond)) {
            // if there is an update, retract old value then emit new value.
            if (acc.oldSecond != Integer.MIN_VALUE) {
                out.retract(Tuple2.of(acc.oldSecond, 2));
            }
            out.collect(Tuple2.of(acc.second, 2));
            acc.oldSecond = acc.second;
        }
    }
}

// 注册函数
StreamTableEnvironment tEnv = ...
tEnv.registerFunction("top2", new Top2());

// 初始化表
Table tab = ...;

// 使用函数
tab.groupBy("key")
    .flatAggregate("top2(a) as (v, rank)")
    .select("key, v, rank");</programlisting>
</section>
</section>
<section xml:id="_使用ddl的方式来编写flink_sql">
<title>使用DDL的方式来编写Flink SQL</title>
<section xml:id="_快速上手">
<title>快速上手</title>
<simpara>我们之前写的Flink SQL代码，都遵循了将数据流转换成动态表，在表上进行查询，然后再将查询所得的结果动态表转换成数据流这样的编程流程，这种写法虽然很灵活，但写起来确实感觉不太简洁，实际上我们还可以使用DDL的方式来编写Flink SQL程序，也就是尽量将所有操作都SQL化。我们先来看一个例子，看一下整体流程。</simpara>
<simpara>我们先提供一个 <literal>file.csv</literal> 文件，内容如下</simpara>
<programlisting language="csv" linenumbering="unnumbered">Mary,./home
Bob,./cart
Mary,./prod?id=1
Liz,./home</programlisting>
<simpara>我们在这个文件上进行查询。代码如下：</simpara>
<programlisting language="java" linenumbering="unnumbered">public class DDLExample {
    public static void main(String[] args) throws Exception {
        // 以下三行，获取流环境
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        EnvironmentSettings settings = EnvironmentSettings.newInstance().inStreamingMode().build();
        StreamTableEnvironment tableEnvironment = StreamTableEnvironment.create(env, settings);

        // 定义输入表，WITH定义了连接器，连接到文件file.csv
        tableEnvironment
                .executeSql("CREATE TABLE clicks (`user` STRING, `url` STRING) " +
                        "WITH (" +
                        "'connector' = 'filesystem'," +
                        "'path' = 'file.csv'," +
                        "'format' = 'csv')");

        // 定义输出表，连接到标准输出
        tableEnvironment
                .executeSql("CREATE TABLE ResultTable (`user` STRING, `cnt` BIGINT) " +
                        "WITH ('connector' = 'print')");

        // 在输出表上进行查询，查询结果写入输出表
        tableEnvironment
                .executeSql("INSERT INTO ResultTable SELECT user, COUNT(url) as cnt FROM clicks GROUP BY user");
    }
}</programlisting>
<simpara>首先我们创建了两张表，一张输入表，一张输出表，我们在创建表时，需要指定连接器，来明确数据从哪里来以及到哪里去。然后将查询结果插入到结果表中去。</simpara>
<simpara>输出结果是：</simpara>
<programlisting language="text" linenumbering="unnumbered">3&gt; +I[Mary, 1]
3&gt; -U[Mary, 1]
3&gt; +U[Mary, 2]
5&gt; +I[Liz, 1]
5&gt; +I[Bob, 1]</programlisting>
</section>
<section xml:id="_时间属性">
<title>时间属性</title>
<simpara>我们先来看一下处理时间在DDL中如何设置</simpara>
<programlisting language="sql" linenumbering="unnumbered">CREATE TABLE user_actions (
  user_name STRING,
  data STRING,
  user_action_time AS PROCTIME() -- 声明一个额外的字段作为处理时间
) WITH (
  ...
);

SELECT TUMBLE_START(user_action_time, INTERVAL '10' MINUTE), COUNT(DISTINCT user_name)
FROM user_actions
GROUP BY TUMBLE(user_action_time, INTERVAL '10' MINUTE);</programlisting>
<simpara>事件时间的设置和我们上一节在SQL中设置窗口和指定时间戳的写法是一样的。这里主要看一下在SQL中如何设置事件时间以及水位线，因为我们之前设置水位线和事件时间是通过核心API的方式来设置的。</simpara>
<programlisting language="java" linenumbering="unnumbered">import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.EnvironmentSettings;
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;

public class SetWatermark {

    public static void main(String[] args) throws Exception {
        // set up execution environment
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        EnvironmentSettings settings = EnvironmentSettings.newInstance().inStreamingMode().build();
        StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, settings);

        String ddl =
                "CREATE TABLE clicks (\n"
                        + "  `user` STRING,\n"
                        + "  url STRING,\n"
                        + "  cTime TIMESTAMP(3),\n"
                        + "  WATERMARK FOR cTime AS cTime - INTERVAL '3' SECOND\n" // 最大延迟时间设置为3秒
                        + ") WITH (\n"
                        + "  'connector.type' = 'filesystem',\n"
                        + "  'connector.path' = '"
                        + "file.csv"
                        + "',\n"
                        + "  'format.type' = 'csv'\n"
                        + ")";
        tEnv.executeSql(ddl);

        tEnv.executeSql("CREATE TABLE ResultTable (" +
                "`user` STRING, " +
                "endT TIMESTAMP(3), " +
                "cnt BIGINT) WITH (" +
                "'connector' = 'print')");

        // 每个用户在每个一小时滚动窗口中的访问次数
        String query = "SELECT " +
                "user, " +
                "TUMBLE_END(" +
                "cTime," +
                "INTERVAL '1' HOUR)" +
                "AS endT," +
                "COUNT(url) AS cnt " +
                "FROM clicks " +
                "GROUP BY " +
                "user, " +
                "TUMBLE(" +
                "cTime, " +
                "INTERVAL '1' HOUR)";

        tEnv.executeSql("INSERT INTO ResultTable " + query);
    }
}</programlisting>
<simpara>输出结果是：</simpara>
<programlisting language="text" linenumbering="unnumbered">3&gt; +I[Mary, 1970-01-01T13:00, 3]
5&gt; +I[Bob, 1970-01-01T13:00, 1]
3&gt; +I[Mary, 1970-01-01T15:00, 1]
5&gt; +I[Liz, 1970-01-01T14:00, 2]
5&gt; +I[Bob, 1970-01-01T14:00, 1]
5&gt; +I[Liz, 1970-01-01T15:00, 1]
5&gt; +I[Bob, 1970-01-01T15:00, 2]</programlisting>
</section>
<section xml:id="_flink_sql客户端的使用">
<title>Flink SQL客户端的使用</title>
<simpara>Flink为我们提供了SQL客户端来方便我们做一些测试。</simpara>
<simpara>首先启动本地集群</simpara>
<programlisting language="bash" linenumbering="unnumbered">./bin/start-cluster.sh</programlisting>
<simpara>然后启动flink sql客户端</simpara>
<programlisting language="bash" linenumbering="unnumbered">./bin/sql-client.sh embedded</programlisting>
<simpara>然后就可以愉快的编写Flink SQL语句了。我们把之前举的例子来写一下：</simpara>
<programlisting language="text" linenumbering="unnumbered">Flink SQL&gt; CREATE TABLE clicks (
&gt;   `user` STRING,
&gt;   url STRING
&gt; ) WITH (
&gt;   'connector' = 'filesystem',
&gt;   'path'      = 'file.csv',
&gt;   'format'    = 'csv'
&gt; );

Flink SQL&gt; CREATE TABLE ResultTable (
&gt;   `user` STRING,
&gt;   cnt BIGINT
&gt; ) WITH (
&gt;   'connector' = 'print'
&gt; );

Flink SQL&gt; INSERT INTO ResultTable SELECT user, COUNT(url) as cnt FROM clicks GROUP BY user;</programlisting>
</section>
<section xml:id="_连接器">
<title>连接器</title>
<simpara>我们使用连接器来读取数据源的数据，以及将查询结果写入到第三方设备中。我们来看几个比较常用的设备的连接器如何设置</simpara>
<simpara><emphasis role="strong">Kafka</emphasis></simpara>
<programlisting language="sql" linenumbering="unnumbered">CREATE TABLE KafkaTable (
  `user` STRING,
  `url` STRING,
  `ts` TIMESTAMP(3) METADATA FROM 'timestamp'
) WITH (
  'connector' = 'kafka',
  'topic' = 'clicks',
  'properties.bootstrap.servers' = 'localhost:9092',
  'properties.group.id' = 'testGroup',
  'scan.startup.mode' = 'earliest-offset',
  'format' = 'csv'
)</programlisting>
<simpara><emphasis role="strong">Upsert Kafka</emphasis></simpara>
<simpara>Upsert Kafka 连接器支持以 upsert 方式从 Kafka topic 中读取数据并将数据写入 Kafka topic。</simpara>
<simpara>作为 source，upsert-kafka 连接器生产 changelog 流，其中每条数据记录代表一个更新或删除事件。更准确地说，数据记录中的 value 被解释为同一 key 的最后一个 value 的 UPDATE，如果有这个 key（如果不存在相应的 key，则该更新被视为 INSERT）。用表来类比，changelog 流中的数据记录被解释为 UPSERT，也称为 INSERT/UPDATE，因为任何具有相同 key 的现有行都被覆盖。另外，value 为空的消息将会被视作为 DELETE 消息。</simpara>
<simpara>作为 sink，upsert-kafka 连接器可以消费 changelog 流。它会将 INSERT/UPDATE_AFTER 数据作为正常的 Kafka 消息写入，并将 DELETE 数据以 value 为空的 Kafka 消息写入（表示对应 key 的消息被删除）。Flink 将根据主键列的值对数据进行分区，从而保证主键上的消息有序，因此同一主键上的更新/删除消息将落在同一分区中。</simpara>
<simpara>下面的示例展示了如何创建和使用 Upsert Kafka 表：</simpara>
<programlisting language="sql" linenumbering="unnumbered">CREATE TABLE pageviews_per_region (
  user_region STRING,
  pv BIGINT,
  uv BIGINT,
  PRIMARY KEY (user_region) NOT ENFORCED
) WITH (
  'connector' = 'upsert-kafka',
  'topic' = 'pageviews_per_region',
  'properties.bootstrap.servers' = '...',
  'key.format' = 'avro',
  'value.format' = 'avro'
);

CREATE TABLE pageviews (
  user_id BIGINT,
  page_id BIGINT,
  viewtime TIMESTAMP,
  user_region STRING,
  WATERMARK FOR viewtime AS viewtime - INTERVAL '2' SECOND
) WITH (
  'connector' = 'kafka',
  'topic' = 'pageviews',
  'properties.bootstrap.servers' = '...',
  'format' = 'json'
);

-- 计算 pv、uv 并插入到 upsert-kafka sink
INSERT INTO pageviews_per_region
SELECT
  user_region,
  COUNT(*),
  COUNT(DISTINCT user_id)
FROM pageviews
GROUP BY user_region;</programlisting>
<simpara><emphasis role="strong">文件系统</emphasis></simpara>
<programlisting language="sql" linenumbering="unnumbered">CREATE TABLE MyUserTable (
  column_name1 INT,
  column_name2 STRING,
  ...
  part_name1 INT,
  part_name2 STRING
) PARTITIONED BY (part_name1, part_name2) WITH (
  'connector' = 'filesystem',           -- 连接器类型
  'path' = 'file:///path/to/whatever',  -- 文件路径
  'format' = '...'                      -- 文件格式
)</programlisting>
<simpara><emphasis role="strong">打印到控制台</emphasis></simpara>
<programlisting language="sql" linenumbering="unnumbered">CREATE TABLE print_table (
  f0 INT,
  f1 INT,
  f2 STRING,
  f3 DOUBLE
) WITH (
  'connector' = 'print'
);</programlisting>
<simpara><emphasis role="strong">JDBC</emphasis></simpara>
<programlisting language="sql" linenumbering="unnumbered">-- 在 Flink SQL 中注册一张 MySQL 表 'users'
CREATE TABLE MyUserTable (
  id BIGINT,
  name STRING,
  age INT,
  status BOOLEAN,
  PRIMARY KEY (id) NOT ENFORCED
) WITH (
   'connector' = 'jdbc',
   'url' = 'jdbc:mysql://localhost:3306/mydatabase',
   'table-name' = 'users'
);

-- 从另一张表 "T" 将数据写入到 JDBC 表中
INSERT INTO MyUserTable
SELECT id, name, age, status FROM T;

-- 查看 JDBC 表中的数据
SELECT id, name, age, status FROM MyUserTable;</programlisting>
<simpara><emphasis role="strong">ElasticSearch</emphasis></simpara>
<programlisting language="sql" linenumbering="unnumbered">CREATE TABLE myUserTable (
  user_id STRING,
  user_name STRING
  uv BIGINT,
  pv BIGINT,
  PRIMARY KEY (user_id) NOT ENFORCED
) WITH (
  'connector' = 'elasticsearch-7',
  'hosts' = 'http://localhost:9200',
  'index' = 'users'
);</programlisting>
<simpara><emphasis role="strong">HBase</emphasis></simpara>
<programlisting language="sql" linenumbering="unnumbered">-- register the HBase table 'mytable' in Flink SQL
CREATE TABLE hTable (
 rowkey INT,
 family1 ROW&lt;q1 INT&gt;,
 family2 ROW&lt;q2 STRING, q3 BIGINT&gt;,
 family3 ROW&lt;q4 DOUBLE, q5 BOOLEAN, q6 STRING&gt;,
 PRIMARY KEY (rowkey) NOT ENFORCED
) WITH (
 'connector' = 'hbase-1.4',
 'table-name' = 'mytable',
 'zookeeper.quorum' = 'localhost:2181'
);

-- 假设"T"的 schema 是 [rowkey, f1q1, f2q2, f2q3, f3q4, f3q5, f3q6]
INSERT INTO hTable
SELECT rowkey, ROW(f1q1), ROW(f2q2, f2q3), ROW(f3q4, f3q5, f3q6) FROM T;

-- 查看HBase中的表
SELECT rowkey, family1, family3.q4, family3.q6 FROM hTable;</programlisting>
<simpara><emphasis role="strong">hive</emphasis></simpara>
<programlisting language="text" linenumbering="unnumbered">Flink SQL&gt; create catalog myhive with ('type' = 'hive', 'hive-conf-dir' = '/opt/hive-conf');
[INFO] Execute statement succeed.

Flink SQL&gt; use catalog myhive;
[INFO] Execute statement succeed.

Flink SQL&gt; load module hive;
[INFO] Execute statement succeed.

Flink SQL&gt; use modules hive,core;
[INFO] Execute statement succeed.

Flink SQL&gt; set table.sql-dialect=hive;
[INFO] Session property has been set.

Flink SQL&gt; select explode(array(1,2,3)); -- call hive udtf
+-----+
| col |
+-----+
|   1 |
|   2 |
|   3 |
+-----+
3 rows in set

Flink SQL&gt; create table tbl (key int,value string);
[INFO] Execute statement succeed.

Flink SQL&gt; insert overwrite table tbl values (5,'e'),(1,'a'),(1,'a'),(3,'c'),(2,'b'),(3,'c'),(3,'c'),(4,'d');
[INFO] Submitting SQL update statement to the cluster...
[INFO] SQL update statement has been successfully submitted to the cluster:

Flink SQL&gt; select * from tbl cluster by key; -- run cluster by
2021-04-22 16:13:57,005 INFO  org.apache.hadoop.mapred.FileInputFormat                     [] - Total input paths to process : 1
+-----+-------+
| key | value |
+-----+-------+
|   1 |     a |
|   1 |     a |
|   5 |     e |
|   2 |     b |
|   3 |     c |
|   3 |     c |
|   3 |     c |
|   4 |     d |
+-----+-------+
8 rows in set</programlisting>
</section>
</section>
<section xml:id="_flink_table_api编程简介">
<title>Flink Table API编程简介</title>
<simpara>我们来将之前的一个程序用Table API写一下，感受一下Table API的使用，Table API我们不会详细讲解，具体可以参看官方文档。Table API本质上是一种声明式的DSL（领域专用语言），可以比较好的和Java程序揉和在一起来写。但由于Table API所编写的程序都可以使用SQL来编写，所以在实际生产环境中更加推荐使用SQL来编写程序。</simpara>
<programlisting language="java" linenumbering="unnumbered">import org.apache.flink.api.common.eventtime.SerializableTimestampAssigner;
import org.apache.flink.api.common.eventtime.WatermarkStrategy;
import org.apache.flink.api.java.tuple.Tuple3;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.EnvironmentSettings;
import org.apache.flink.table.api.Table;
import org.apache.flink.table.api.Tumble;
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;
import org.apache.flink.types.Row;

import static org.apache.flink.table.api.Expressions.*;

public class TableAPIExample {

    public static void main(String[] args) throws Exception {
        // set up execution environment
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        SingleOutputStreamOperator&lt;Tuple3&lt;String, String, Long&gt;&gt; stream = env
                .fromElements(
                        Tuple3.of("Mary", "./home", 12 * 60 * 60 * 1000L),
                        Tuple3.of("Bob", "./cart", 12 * 60 * 60 * 1000L),
                        Tuple3.of("Mary", "./prod?id=1", 12 * 60 * 60 * 1000L + 2 * 60 * 1000L),
                        Tuple3.of("Mary", "./prod?id=4", 12 * 60 * 60 * 1000L + 55 * 60 * 1000L),
                        Tuple3.of("Bob", "./prod?id=5", 13 * 60 * 60 * 1000L + 60 * 1000L),
                        Tuple3.of("Liz", "./home", 13 * 60 * 60 * 1000L + 30 * 60 * 1000L),
                        Tuple3.of("Liz", "./prod?id=7", 13 * 60 * 60 * 1000L + 59 * 60 * 1000L),
                        Tuple3.of("Mary", "./cart", 14 * 60 * 60 * 1000L),
                        Tuple3.of("Liz", "./home", 14 * 60 * 60 * 1000L + 2 * 60 * 1000L),
                        Tuple3.of("Bob", "./prod?id=3", 14 * 60 * 60 * 1000L + 30 * 60 * 1000L),
                        Tuple3.of("Bob", "./home", 14 * 60 * 60 * 1000L + 40 * 60 * 1000L)
                )
                .assignTimestampsAndWatermarks(
                        WatermarkStrategy.&lt;Tuple3&lt;String, String, Long&gt;&gt;forMonotonousTimestamps()
                                .withTimestampAssigner(new SerializableTimestampAssigner&lt;Tuple3&lt;String, String, Long&gt;&gt;() {
                                    @Override
                                    public long extractTimestamp(Tuple3&lt;String, String, Long&gt; element, long recordTimestamp) {
                                        return element.f2;
                                    }
                                })
                );

        EnvironmentSettings settings = EnvironmentSettings.newInstance().inStreamingMode().build();

        StreamTableEnvironment tableEnvironment = StreamTableEnvironment.create(env, settings);

        Table table = tableEnvironment
                .fromDataStream(
                        stream,
                        $("f0").as("user"),
                        $("f1").as("url"),
                        $("f2").rowtime().as("cTime"));

        Table result = table
                .window(Tumble.over(lit(1).hours()).on($("cTime")).as("w"))
                .groupBy($("user"), $("w"))
                .select($("user"), $("url").count().as("cnt"), $("w").end().as("endT"));

        tableEnvironment.toRetractStream(result, Row.class).print();

        env.execute();
    }
}</programlisting>
</section>
</section>
<section xml:id="_flink_cep简介">
<title>Flink CEP简介</title>
<simpara><emphasis role="strong">什么是复杂事件CEP？</emphasis></simpara>
<simpara>一个或多个由简单事件构成的事件流通过一定的规则匹配，然后输出用户想得到的数据，满足规则的复杂事件。</simpara>
<simpara><emphasis role="strong">特征</emphasis></simpara>
<itemizedlist>
<listitem>
<simpara>目标：从有序的简单事件流中发现一些高阶特征</simpara>
</listitem>
<listitem>
<simpara>输入：一个或多个由简单事件构成的事件流</simpara>
</listitem>
<listitem>
<simpara>处理：识别简单事件之间的内在联系，多个符合一定规则的简单事件构成复杂事件</simpara>
</listitem>
<listitem>
<simpara>输出：满足规则的复杂事件</simpara>
</listitem>
</itemizedlist>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/flink-cep.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>CEP用于分析低延迟、频繁产生的不同来源的事件流。CEP可以帮助在复杂的、不相关的事件流中找出有意义的模式和复杂的关系，以接近实时或准实时的获得通知并阻止一些行为。</simpara>
<simpara>CEP支持在流上进行模式匹配，根据模式的条件不同，分为连续的条件或不连续的条件；模式的条件允许有时间的限制，当在条件范围内没有达到满足的条件时，会导致模式匹配超时。</simpara>
<simpara>看起来很简单，但是它有很多不同的功能：</simpara>
<itemizedlist>
<listitem>
<simpara>输入的流数据，尽快产生结果</simpara>
</listitem>
<listitem>
<simpara>在2个event流上，基于时间进行聚合类的计算</simpara>
</listitem>
<listitem>
<simpara>提供实时/准实时的警告和通知</simpara>
</listitem>
<listitem>
<simpara>在多样的数据源中产生关联并分析模式</simpara>
</listitem>
<listitem>
<simpara>高吞吐、低延迟的处理</simpara>
</listitem>
</itemizedlist>
<simpara>市场上有多种CEP的解决方案，例如Spark、Samza、Beam等，但他们都没有提供专门的library支持。但是Flink提供了专门的CEP
library。</simpara>
<simpara>Flink为CEP提供了专门的Flink CEP library，它包含如下组件：</simpara>
<itemizedlist>
<listitem>
<simpara>Event Stream</simpara>
</listitem>
<listitem>
<simpara>pattern定义</simpara>
</listitem>
<listitem>
<simpara>pattern检测</simpara>
</listitem>
<listitem>
<simpara>生成Alert</simpara>
</listitem>
</itemizedlist>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/cep6.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>首先，开发人员要在DataStream流上定义出模式条件，之后Flink
CEP引擎进行模式检测，必要时生成告警。</simpara>
<simpara>为了使用Flink CEP，我们需要导入依赖：</simpara>
<programlisting language="xml" linenumbering="unnumbered">&lt;dependency&gt;
  &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;
  &lt;artifactId&gt;flink-cep_${scala.binary.version}&lt;/artifactId&gt;
  &lt;version&gt;${flink.version}&lt;/version&gt;
&lt;/dependency&gt;</programlisting>
<section xml:id="_flink_cep快速上手">
<title>Flink CEP快速上手</title>
<simpara>我们先来看一个案例，检测连续三次登录失败，使用Flink CEP来实现：</simpara>
<simpara><emphasis role="strong">需求</emphasis>：连续三次登录失败，报警</simpara>
<simpara>登录事件POJO类实现：</simpara>
<programlisting language="java" linenumbering="unnumbered">public class LoginEvent {
    public String userId;
    public String ipAddress;
    public String eventType;
    public Long eventTime;

    public LoginEvent(String userId, String ipAddress, String eventType, Long eventTime) {
        this.userId = userId;
        this.ipAddress = ipAddress;
        this.eventType = eventType;
        this.eventTime = eventTime;
    }

    public LoginEvent() {}

    @Override
    public String toString() {
        return "LoginEvent{" +
            "userId='" + userId + '\'' +
            ", ipAddress='" + ipAddress + '\'' +
            ", eventType='" + eventType + '\'' +
            ", eventTime=" + eventTime +
            '}';
    }
}</programlisting>
<simpara>业务逻辑编写</simpara>
<programlisting language="java" linenumbering="unnumbered">public class LoginFailDetect {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        // 登录事件流
        KeyedStream&lt;LoginEvent, String&gt; stream = env
            .fromElements(
                new LoginEvent("user_1", "0.0.0.0", "fail", 2000L),
                new LoginEvent("user_1", "0.0.0.1", "fail", 3000L),
                new LoginEvent("user_1", "0.0.0.2", "fail", 4000L)
            )
            .assignTimestampsAndWatermarks(
                WatermarkStrategy.&lt;LoginEvent&gt;forMonotonousTimestamps()
                .withTimestampAssigner(
                    new SerializableTimestampAssigner&lt;LoginEvent&gt;() {
                        @Override
                        public long extractTimestamp(LoginEvent loginEvent, long l) {
                            return loginEvent.eventTime;
                        }
                    }
                )
            )
            .keyBy(r -&gt; r.userId);

        // 模板定义
        Pattern&lt;LoginEvent, LoginEvent&gt; pattern = Pattern
            .&lt;LoginEvent&gt;begin("first")
            .where(new SimpleCondition&lt;LoginEvent&gt;() {
                @Override
                public boolean filter(LoginEvent loginEvent) throws Exception {
                    return loginEvent.eventType.equals("fail");
                }
            })
            .next("second")
            .where(new SimpleCondition&lt;LoginEvent&gt;() {
                @Override
                public boolean filter(LoginEvent loginEvent) throws Exception {
                    return loginEvent.eventType.equals("fail");
                }
            })
            .next("third")
            .where(new SimpleCondition&lt;LoginEvent&gt;() {
                @Override
                public boolean filter(LoginEvent loginEvent) throws Exception {
                    return loginEvent.eventType.equals("fail");
                }
            });

        // 在流上使用模板来匹配
        PatternStream&lt;LoginEvent&gt; patternedStream = CEP.pattern(stream, pattern);

        // 将匹配到的事件选择出来，然后输出
        patternedStream
            .select(new PatternSelectFunction&lt;LoginEvent, Tuple4&lt;String, String, String, String&gt;&gt;() {
                @Override
                public Tuple4&lt;String, String, String, String&gt; select(Map&lt;String, List&lt;LoginEvent&gt;&gt; map) throws Exception {
                    LoginEvent first = map.get("first").get(0);
                    LoginEvent second = map.get("second").get(0);
                    LoginEvent third = map.get("third").get(0);
                    return Tuple4.of(first.userId, first.ipAddress, second.ipAddress, third.ipAddress);
                }
            })
            .print();

        env.execute();
    }
}</programlisting>
<simpara>在上面的程序中，我们使用了 <literal>.next()</literal> 方法，表示 <emphasis role="strong">严格紧邻</emphasis> ，也就是紧挨着的意思。如果只需要一个事件出现在另一个事件的后面，不需要紧挨着，可以使用 <literal>.followedBy()</literal> 方法。连续三次登录失败事件，由于都是相同类型的事件，所以我们的模板定义也可以如下：</simpara>
<programlisting language="java" linenumbering="unnumbered">Pattern&lt;LoginEvent, LoginEvent&gt; pattern = Pattern
    .&lt;LoginEvent&gt;begin("fail")
    .where(new SimpleCondition&lt;LoginEvent&gt;() {
        @Override
        public boolean filter(LoginEvent loginEvent) throws Exception {
            return loginEvent.eventType.equals("fail");
        }
    })
    .times(3);</programlisting>
<simpara>这里的 <literal>times(3)</literal> 表示三次登录失败，这里是 <emphasis role="strong">非严格紧邻</emphasis> 的。在使用 <literal>.select()</literal> 方法获取匹配的事件时，这里的类型 <literal>Map&lt;String, List&lt;LoginEvent&gt;&gt;</literal> 中的 <literal>List</literal> 中包含了匹配到的三个登录失败事件。</simpara>
</section>
<section xml:id="_flink_cep工作原理">
<title>Flink CEP工作原理</title>
<simpara>Flink CEP底层的工作原理叫做NFA（非确定性有限状态机），这里不打算展开讲解NFA的原理，因为会引入很多的数学内容。所以我们这里用一个例子来说明一下状态机的工作方式，这样更方便读者朋友理解。我们来使用核心API重新实现一遍上面的需求，先来看看状态转移图：</simpara>
<literallayout class="monospaced">digraph finite_state_machine {
    rankdir=LR;
    node [shape=box];
    INITIAL -&gt; SUCCESS [label = "success"];
    INITIAL -&gt; S1 [label = "fail"];
    S1 -&gt; S2 [label = "fail"];
    S2 -&gt; FAIL [label = "fail"];
    S1 -&gt; SUCCESS [label = "success"];
    S2 -&gt; SUCCESS [label = "success"];
    SUCCESS -&gt; INITIAL [label = "重置"];
    FAIL -&gt; INITIAL [label = "重置"];
}</literallayout>
<simpara>代码如下</simpara>
<programlisting language="java" linenumbering="unnumbered">import org.apache.flink.api.common.functions.RichFlatMapFunction;
import org.apache.flink.api.common.state.ValueState;
import org.apache.flink.api.common.state.ValueStateDescriptor;
import org.apache.flink.configuration.Configuration;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.util.Collector;

import java.io.Serializable;

import static org.apache.flink.util.Preconditions.checkNotNull;

public class NFAExample {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        env
                .fromElements(
                        new Event("user-1", EventType.fail),
                        new Event("user-1", EventType.fail),
                        new Event("user-1", EventType.fail),
                        new Event("user-2", EventType.fail),
                        new Event("user-2", EventType.success)
                )
                .keyBy(r -&gt; r.userId)
                .flatMap(new StateMachineMapper())
                .print();

        env.execute();
    }

    @SuppressWarnings("serial")
    public static class StateMachineMapper extends RichFlatMapFunction&lt;Event, Alert&gt; {

        /** The state for the current key. */
        private ValueState&lt;State&gt; currentState;

        @Override
        public void open(Configuration conf) {
            // get access to the state object
            currentState = getRuntimeContext().getState(new ValueStateDescriptor&lt;&gt;("state", State.class));
        }

        @Override
        public void flatMap(Event evt, Collector&lt;Alert&gt; out) throws Exception {
            // get the current state for the key (source address)
            // if no state exists, yet, the state must be the state machine's initial state
            State state = currentState.value();
            if (state == null) {
                state = State.Initial;
            }

            // ask the state machine what state we should go to based on the given event
            State nextState = state.transition(evt.type);

            if (nextState == State.InvalidTransition) {
                // the current event resulted in an invalid transition
                // raise an alert!
                out.collect(new Alert(evt.userId, state, evt.type));
            } else if (nextState.isTerminal()) {
                // we reached a terminal state, clean up the current state
                currentState.clear();
            } else {
                // remember the new state
                currentState.update(nextState);
            }
        }
    }

    public static class Alert {

        private final String userId;

        private final State state;

        private final EventType transition;

        public Alert(String userId, State state, EventType transition) {
            this.userId = userId;
            this.state = checkNotNull(state);
            this.transition = checkNotNull(transition);
        }

        @Override
        public String toString() {
            return "Alert{" +
                    "userId='" + userId + '\'' +
                    ", state=" + state +
                    ", transition=" + transition +
                    '}';
        }
    }

    public static class Event {

        private final EventType type;

        private final String userId;

        public Event(String userId, EventType type) {
            this.type = checkNotNull(type);
            this.userId= userId;
        }
    }

    public enum EventType {
        success, fail;
    }

    public enum State {

        Terminal,

        InvalidTransition,

        S2(new Transition(EventType.fail, InvalidTransition), new Transition(EventType.success, Terminal)),

        S1(new Transition(EventType.fail, S2), new Transition(EventType.success, Terminal)),

        /** The initial state from which all state sequences start. */
        Initial(new Transition(EventType.fail, S1), new Transition(EventType.success, Terminal));

        private final Transition[] transitions;

        State(Transition... transitions) {
            this.transitions = transitions;
        }

        public boolean isTerminal() {
            return transitions.length == 0;
        }

        public State transition(EventType evt) {
            for (Transition t : transitions) {
                if (t.eventType() == evt) {
                    return t.targetState();
                }
            }

            return InvalidTransition;
        }
    }

    public static class Transition implements Serializable {

        // this class is serializable to be able to interact cleanly with enums.
        private static final long serialVersionUID = 1L;

        /** The event that triggers the transition. */
        private final EventType eventType;

        /** The target state after the transition. */
        private final State targetState;

        public Transition(EventType eventType, State targetState) {
            this.eventType = checkNotNull(eventType);
            this.targetState = checkNotNull(targetState);
        }

        public EventType eventType() {
            return eventType;
        }

        public State targetState() {
            return targetState;
        }
    }
}</programlisting>
</section>
<section xml:id="_使用flink_cep处理超时事件">
<title>使用Flink CEP处理超时事件</title>
<simpara>订单超时使用Flink CEP实现：</simpara>
<simpara>在电商平台中，最终创造收入和利润的是用户下单购买的环节；更具体一点，是用户真正完成支付动作的时候。用户下单的行为可以表明用户对商品的需求，但在现实中，并不是每次下单都会被用户立刻支付。当拖延一段时间后，用户支付的意愿会降低。所以为了让用户更有紧迫感从而提高支付转化率，同时也为了防范订单支付环节的安全风险，电商网站往往会对订单状态进行监控，设置一个失效时间（比如15分钟），如果下单后一段时间仍未支付，订单就会被取消。</simpara>
<simpara>我们将会利用CEP库来实现这个功能。我们先将事件流按照订单号orderId分流，然后定义这样的一个事件模式：在15分钟内，事件<literal>create</literal>与<literal>pay</literal>严格紧邻：</simpara>
<programlisting language="java" linenumbering="unnumbered">Pattern&lt;OrderEvent, OrderEvent&gt; pattern = Pattern
    .&lt;OrderEvent&gt;begin("create")
    .where(new SimpleCondition&lt;OrderEvent&gt;() {
        @Override
        public boolean filter(OrderEvent value) throws Exception {
            return value.eventType.equals("create");
        }
    })
    .next("pay")
    .where(new SimpleCondition&lt;OrderEvent&gt;() {
        @Override
        public boolean filter(OrderEvent value) throws Exception {
            return value.eventType.equals("pay");
        }
    })
    .within(Time.seconds(5));</programlisting>
<simpara>这样调用.select方法时，就可以同时获取到匹配出的事件和超时未匹配的事件了。在src/main/java下继续创建OrderTimeout.java文件，新建一个单例对象。定义样例类OrderEvent，这是输入的订单事件流；另外还有OrderResult，这是输出显示的订单状态结果。由于没有现成的数据，我们还是用几条自定义的示例数据来做演示。</simpara>
<simpara>完整代码如下：</simpara>
<simpara>POJO类实现</simpara>
<programlisting language="java" linenumbering="unnumbered">public class OrderEvent {
    public String orderId;
    public String eventType;
    public Long eventTime;

    public OrderEvent() {
    }

    public OrderEvent(String orderId, String eventType, Long eventTime) {
        this.orderId = orderId;
        this.eventType = eventType;
        this.eventTime = eventTime;
    }

    @Override
    public String toString() {
        return "OrderEvent{" +
                "orderId='" + orderId + '\'' +
                ", eventType='" + eventType + '\'' +
                ", eventTime=" + eventTime +
                '}';
    }
}</programlisting>
<simpara>检测程序如下：</simpara>
<programlisting language="java" linenumbering="unnumbered">public class OrderTimeoutDetect {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);
        env.setParallelism(1);

        SingleOutputStreamOperator&lt;OrderEvent&gt; stream = env
            .fromElements(
                new OrderEvent("order_1", "create", 1000L),
                new OrderEvent("order_2", "create", 2000L),
                new OrderEvent("order_1", "pay", 3000L)
            )
            .assignTimestampsAndWatermarks(
                WatermarkStrategy.&lt;OrderEvent&gt;forMonotonousTimestamps()
                    .withTimestampAssigner(new SerializableTimestampAssigner&lt;OrderEvent&gt;() {
                        @Override
                        public long extractTimestamp(OrderEvent element, long recordTimestamp) {
                            return element.eventTime;
                        }
                    })
            );

        Pattern&lt;OrderEvent, OrderEvent&gt; pattern = Pattern
            .&lt;OrderEvent&gt;begin("create")
            .where(new SimpleCondition&lt;OrderEvent&gt;() {
                @Override
                public boolean filter(OrderEvent value) throws Exception {
                    return value.eventType.equals("create");
                }
            })
            .next("pay")
            .where(new SimpleCondition&lt;OrderEvent&gt;() {
                @Override
                public boolean filter(OrderEvent value) throws Exception {
                    return value.eventType.equals("pay");
                }
            })
            .within(Time.seconds(5));

        PatternStream&lt;OrderEvent&gt; patternStream = CEP.pattern(stream.keyBy(r -&gt; r.orderId), pattern);

        SingleOutputStreamOperator&lt;String&gt; result = patternStream
            .select(
                new OutputTag&lt;String&gt;("order-timeout") {
                },
                new PatternTimeoutFunction&lt;OrderEvent, String&gt;() {
                    @Override
                    public String timeout(Map&lt;String, List&lt;OrderEvent&gt;&gt; map, long l) throws Exception {
                        return "订单ID为 " + map.get("create").get(0).orderId + " 没有支付！";
                    }
                },
                new PatternSelectFunction&lt;OrderEvent, String&gt;() {
                    @Override
                    public String select(Map&lt;String, List&lt;OrderEvent&gt;&gt; map) throws Exception {
                        return "订单ID为 " + map.get("pay").get(0).orderId + " 已经支付！";
                    }
                }
            );

        result.print();

        result.getSideOutput(new OutputTag&lt;String&gt;("order-timeout") {}).print();

        env.execute();
    }
}</programlisting>
<simpara>同样的需求我们使用核心API在之前的章节中已经实现过了，可以看到Flink CEP编写起来还是很简洁的。</simpara>
</section>
<section xml:id="_总结">
<title>总结</title>
<simpara>Flink CEP的底层实现使用的NFA。而正则表达式的底层原理同样是NFA。在上面的对CEP的使用中，可以发现，CEP模板的定义和正则表达式模板的定义是非常相似的。而Flink CEP是在事件流上匹配符合模板定义的事件，正则表达式是在字符串上匹配符合正则表达式的字符序列。读者朋友都知道，正则表达式的各种定义方式是非常繁琐的，Flink CEP的模板定义也不遑多让。所以我们在本章节就不一一列举了。上面所编写的代码基本上已经可以满足实际生产的需求了。具体模板的定义方式可以参考官方文档。</simpara>
</section>
</section>
<section xml:id="_案例集">
<title>案例集</title>
<section xml:id="_数据倾斜如何解决">
<title>数据倾斜如何解决</title>
<programlisting language="java" linenumbering="unnumbered">public class DataSkew {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        env
            .fromElements(
                Tuple3.of("a", 1L, 1000L),
                Tuple3.of("a", 1L, 2000L),
                Tuple3.of("a", 1L, 3000L),
                Tuple3.of("a", 1L, 4000L),
                Tuple3.of("a", 1L, 5000L),
                Tuple3.of("a", 1L, 6000L),
                Tuple3.of("a", 1L, 7000L),
                Tuple3.of("a", 1L, 8000L),
                Tuple3.of("a", 1L, 9000L),
                Tuple3.of("a", 1L, 10000L),
                Tuple3.of("b", 1L, 11000L)
            )
            .map(new MapFunction&lt;Tuple3&lt;String, Long, Long&gt;, Tuple3&lt;String, Long, Long&gt;&gt;() {
                @Override
                public Tuple3&lt;String, Long, Long&gt; map(Tuple3&lt;String, Long, Long&gt; value) throws Exception {
                    Random rand = new Random();
                    return Tuple3.of(value.f0 + "-" + rand.nextInt(4), value.f1, value.f2);
                }
            })
            .assignTimestampsAndWatermarks(WatermarkStrategy.&lt;Tuple3&lt;String, Long, Long&gt;&gt;forMonotonousTimestamps()
            .withTimestampAssigner(new SerializableTimestampAssigner&lt;Tuple3&lt;String, Long, Long&gt;&gt;() {
                @Override
                public long extractTimestamp(Tuple3&lt;String, Long, Long&gt; element, long recordTimestamp) {
                    return element.f2;
                }
            }))
            .keyBy(r -&gt; r.f0)
            .process(new KeyedProcessFunction&lt;String, Tuple3&lt;String, Long, Long&gt;, Tuple2&lt;String, Long&gt;&gt;() {
                private ValueState&lt;Tuple2&lt;String, Long&gt;&gt; sum;
                private ValueState&lt;Long&gt; timerTs;
                @Override
                public void open(Configuration parameters) throws Exception {
                    super.open(parameters);
                    sum = getRuntimeContext().getState(new ValueStateDescriptor&lt;Tuple2&lt;String, Long&gt;&gt;("sum", Types.TUPLE(Types.STRING, Types.LONG)));
                    timerTs = getRuntimeContext().getState(new ValueStateDescriptor&lt;Long&gt;("timer", Types.LONG));
                }

                @Override
                public void processElement(Tuple3&lt;String, Long, Long&gt; value, Context ctx, Collector&lt;Tuple2&lt;String, Long&gt;&gt; out) throws Exception {
                    if (sum.value() == null) {
                        sum.update(Tuple2.of(value.f0, value.f1));
                        ctx.timerService().registerEventTimeTimer(value.f2 + 10 * 1000L);
                        timerTs.update(value.f2 + 10 * 1000L);
                    } else {
                        Long cnt = sum.value().f1;
                        sum.update(Tuple2.of(value.f0, cnt + value.f1));
                        if (timerTs.value() == null) {
                            ctx.timerService().registerEventTimeTimer(value.f2 + 10 * 1000L);
                            timerTs.update(value.f2 + 10 * 1000L);
                        }
                    }
                }

                @Override
                public void onTimer(long timestamp, OnTimerContext ctx, Collector&lt;Tuple2&lt;String, Long&gt;&gt; out) throws Exception {
                    super.onTimer(timestamp, ctx, out);
                    out.collect(Tuple2.of(ctx.getCurrentKey(), sum.value().f1));
                    timerTs.clear();
                }
            })
            .map(new MapFunction&lt;Tuple2&lt;String, Long&gt;, Tuple3&lt;String, Integer, Long&gt;&gt;() {
                @Override
                public Tuple3&lt;String, Integer, Long&gt; map(Tuple2&lt;String, Long&gt; value) throws Exception {
                    return Tuple3.of(value.f0.split("-")[0], Integer.parseInt(value.f0.split("-")[1]), value.f1);
                }
            })
            .keyBy(r -&gt; r.f0)
            .process(new KeyedProcessFunction&lt;String, Tuple3&lt;String, Integer, Long&gt;, Tuple2&lt;String, Long&gt;&gt;() {
                private MapState&lt;Long, Long&gt; mapState;

                @Override
                public void open(Configuration parameters) throws Exception {
                    super.open(parameters);
                    mapState = getRuntimeContext().getMapState(
                            new MapStateDescriptor&lt;Long, Long&gt;("map", Types.LONG, Types.LONG)
                    );
                }

                @Override
                public void processElement(Tuple3&lt;String, Integer, Long&gt; value, Context ctx, Collector&lt;Tuple2&lt;String, Long&gt;&gt; out) throws Exception {
                    mapState.put((long)value.f1, value.f2);
                    long sum = 0L;
                    for (Long v : mapState.values()) {
                        sum += v;
                    }
                    out.collect(Tuple2.of(value.f0, sum));
                }
            })
            .print();

        env.execute();
    }
}</programlisting>
</section>
<section xml:id="_独立访客计算">
<title>独立访客计算</title>
<section xml:id="_原始版">
<title>原始版</title>
<programlisting language="java" linenumbering="unnumbered">public class UV {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        env
            .readTextFile("UserBehavior.csv")
            .map(new MapFunction&lt;String, UserBehavior&gt;() {
                @Override
                public UserBehavior map(String value) throws Exception {
                    String[] arr = value.split(",");
                    return new UserBehavior(
                            arr[0],arr[1],arr[2],arr[3],
                            Long.parseLong(arr[4]) * 1000L
                    );
                }
            })
            .filter(r -&gt; r.behaviorType.equals("pv"))
            .assignTimestampsAndWatermarks(WatermarkStrategy.&lt;UserBehavior&gt;forMonotonousTimestamps()
            .withTimestampAssigner(new SerializableTimestampAssigner&lt;UserBehavior&gt;() {
                @Override
                public long extractTimestamp(UserBehavior element, long recordTimestamp) {
                    return element.timestamp;
                }
            }))
            .map(new MapFunction&lt;UserBehavior, Tuple2&lt;String, String&gt;&gt;() {
                @Override
                public Tuple2&lt;String, String&gt; map(UserBehavior value) throws Exception {
                    return Tuple2.of("key", value.userId);
                }
            })
            .keyBy(r -&gt; r.f0)
            .window(TumblingEventTimeWindows.of(Time.hours(1)))
            .process(new ProcessWindowFunction&lt;Tuple2&lt;String, String&gt;, String, String, TimeWindow&gt;() {
                @Override
                public void process(String s, Context context, Iterable&lt;Tuple2&lt;String, String&gt;&gt; elements, Collector&lt;String&gt; out) throws Exception {
                    HashSet&lt;String&gt; set = new HashSet&lt;&gt;();
                    for (Tuple2&lt;String, String&gt; e : elements) {
                        set.add(e.f1);
                    }
                    String start = new Timestamp(context.window().getStart()).toString();
                    String end = new Timestamp(context.window().getEnd()).toString();

                    out.collect("窗口 " + start + " ~~~ " + end + " 的UV数据是：" + set.size());
                }
            })
            .print();

        env.execute();
    }
}</programlisting>
</section>
<section xml:id="_改进版">
<title>改进版</title>
<programlisting language="java" linenumbering="unnumbered">public class UVImproved {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        env
            .readTextFile("UserBehavior.csv")
            .map(new MapFunction&lt;String, UserBehavior&gt;() {
                @Override
                public UserBehavior map(String value) throws Exception {
                    String[] arr = value.split(",");
                    return new UserBehavior(
                            arr[0],arr[1],arr[2],arr[3],
                            Long.parseLong(arr[4]) * 1000L
                    );
                }
            })
            .filter(r -&gt; r.behaviorType.equals("pv"))
            .assignTimestampsAndWatermarks(WatermarkStrategy.&lt;UserBehavior&gt;forMonotonousTimestamps()
                    .withTimestampAssigner(new SerializableTimestampAssigner&lt;UserBehavior&gt;() {
                        @Override
                        public long extractTimestamp(UserBehavior element, long recordTimestamp) {
                            return element.timestamp;
                        }
                    }))
            .map(new MapFunction&lt;UserBehavior, Tuple2&lt;String, String&gt;&gt;() {
                @Override
                public Tuple2&lt;String, String&gt; map(UserBehavior value) throws Exception {
                    return Tuple2.of("key", value.userId);
                }
            })
            .keyBy(r -&gt; r.f0)
            .window(TumblingEventTimeWindows.of(Time.hours(1)))
            .aggregate(new Agg(), new WindowResult())
            .print();

        env.execute();
    }

    public static class Agg implements AggregateFunction&lt;Tuple2&lt;String, String&gt;, HashSet&lt;String&gt;, Long&gt; {
        @Override
        public HashSet&lt;String&gt; createAccumulator() {
            return new HashSet&lt;&gt;();
        }

        @Override
        public HashSet&lt;String&gt; add(Tuple2&lt;String, String&gt; value, HashSet&lt;String&gt; accumulator) {
            accumulator.add(value.f1);
            return accumulator;
        }

        @Override
        public Long getResult(HashSet&lt;String&gt; accumulator) {
            return (long) accumulator.size();
        }

        @Override
        public HashSet&lt;String&gt; merge(HashSet&lt;String&gt; a, HashSet&lt;String&gt; b) {
            return null;
        }
    }

    public static class WindowResult extends ProcessWindowFunction&lt;Long, String, String, TimeWindow&gt; {
        @Override
        public void process(String s, Context context, Iterable&lt;Long&gt; elements, Collector&lt;String&gt; out) throws Exception {
            String start = new Timestamp(context.window().getStart()).toString();
            String end = new Timestamp(context.window().getEnd()).toString();

            out.collect("窗口 " + start + " ~~~ " + end + " 的UV数据是：" + elements.iterator().next());
        }
    }
}</programlisting>
</section>
<section xml:id="_布隆过滤器理论">
<title>布隆过滤器理论</title>
<simpara><emphasis role="strong">什么是布隆过滤器</emphasis></simpara>
<simpara>本质上布隆过滤器是一种数据结构，比较巧妙的概率型数据结构（probabilistic
data structure），特点是高效地插入和查询，可以用来告诉你
<literal>某样东西一定不存在或者可能存在</literal>。</simpara>
<simpara>相比于传统的 List、Set、Map
等数据结构，它更高效、占用空间更少，但是缺点是其返回的结果是概率性的，而不是确切的。</simpara>
<simpara><emphasis role="strong">实现原理</emphasis></simpara>
<simpara><emphasis>HashMap的问题</emphasis></simpara>
<simpara>讲述布隆过滤器的原理之前，我们先思考一下，通常你判断某个元素是否存在用的是什么？应该蛮多人回答
HashMap 吧，确实可以将值映射到 HashMap 的 Key，然后可以在 O(1)
的时间复杂度内返回结果，效率奇高。但是 HashMap
的实现也有缺点，例如存储容量占比高，考虑到负载因子的存在，通常空间是不能被用满的，而一旦你的值很多例如上亿的时候，那
HashMap 占据的内存大小就变得很可观了。</simpara>
<simpara>还比如说你的数据集存储在远程服务器上，本地服务接受输入，而数据集非常大不可能一次性读进内存构建
HashMap 的时候，也会存在问题。</simpara>
<simpara><emphasis>布隆过滤器数据结构</emphasis></simpara>
<simpara>布隆过滤器是一个 bit 向量或者说 bit 数组，长这样：</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/bloomfilter-array-index.svg"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>如果我们要映射一个值到布隆过滤器中，我们需要使用多个不同的哈希函数生成多个哈希值，并对每个生成的哈希值指向的
bit 位置 1，例如针对值 <literal>baidu</literal> 和三个不同的哈希函数分别生成了哈希值
1、4、7，则上图转变为：</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/bloomfilter-baidu.svg"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>Ok，我们现在再存一个值 <literal>tencent</literal>，如果哈希函数返回 3、4、8
的话，图继续变为：</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/bloomfilter-tencent.svg"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>值得注意的是，4 这个 bit 位由于两个值的哈希函数都返回了这个 bit
位，因此它被覆盖了。现在我们如果想查询 <literal>dianping</literal>
这个值是否存在，哈希函数返回了 1、5、8三个值，结果我们发现 5 这个 bit
位上的值为 0，说明没有任何一个值映射到这个 bit
位上，因此我们可以很确定地说 <literal>dianping</literal> 这个值不存在。而当我们需要查询
<literal>baidu</literal> 这个值是否存在的话，那么哈希函数必然会返回
1、4、7，然后我们检查发现这三个 bit 位上的值均为 1，那么我们可以说
<literal>baidu</literal> 存在了么？答案是不可以，只能是 <literal>baidu</literal> 这个值可能存在。</simpara>
<simpara>这是为什么呢？答案跟简单，因为随着增加的值越来越多，被置为 1 的 bit
位也会越来越多，这样某个值 <literal>taobao</literal>
即使没有被存储过，但是万一哈希函数返回的三个 bit 位都被其他值置位了 1
，那么程序还是会判断 <literal>taobao</literal> 这个值存在。</simpara>
<simpara><emphasis role="strong">如何选择哈希函数个数和布隆过滤器长度</emphasis></simpara>
<simpara>很显然，过小的布隆过滤器很快所有的 bit 位均为
1，那么查询任何值都会返回<literal>可能存在</literal>，起不到过滤的目的了。布隆过滤器的长度会直接影响误报率，布隆过滤器越长其误报率越小。</simpara>
<simpara>另外，哈希函数的个数也需要权衡，个数越多则布隆过滤器 bit 位置为 1
的速度越快，且布隆过滤器的效率越低；但是如果太少的话，那我们的误报率会变高。</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/bloomfilter.png"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara>如何选择适合业务的<inlineequation><alt><![CDATA[k]]></alt><mathphrase><![CDATA[k]]></mathphrase></inlineequation>和<inlineequation><alt><![CDATA[m]]></alt><mathphrase><![CDATA[m]]></mathphrase></inlineequation>值呢，这里直接贴一个公式：</simpara>
<informalequation>
<alt><![CDATA[\[
\begin{aligned}
m = - \frac{n \ln p}{(\ln 2)^2}
\\
k = \frac{m}{n} \ln 2
\end{aligned}
\]]]></alt>
<mathphrase><![CDATA[\[
\begin{aligned}
m = - \frac{n \ln p}{(\ln 2)^2}
\\
k = \frac{m}{n} \ln 2
\end{aligned}
\]]]></mathphrase>
</informalequation>
<simpara>如何推导这个公式这里只是提一句，因为对于使用来说并没有太大的意义，你让一个高中生来推会推得很快。<inlineequation><alt><![CDATA[k]]></alt><mathphrase><![CDATA[k]]></mathphrase></inlineequation>次哈希函数某一bit位未被置为<inlineequation><alt><![CDATA[1]]></alt><mathphrase><![CDATA[1]]></mathphrase></inlineequation>的概率为：</simpara>
<informalequation>
<alt><![CDATA[\[
(1 - \frac{1}{m})^k
\]]]></alt>
<mathphrase><![CDATA[\[
(1 - \frac{1}{m})^k
\]]]></mathphrase>
</informalequation>
<simpara>插入<inlineequation><alt><![CDATA[n]]></alt><mathphrase><![CDATA[n]]></mathphrase></inlineequation>个元素后依旧为<inlineequation><alt><![CDATA[0]]></alt><mathphrase><![CDATA[0]]></mathphrase></inlineequation>的概率和为<inlineequation><alt><![CDATA[1]]></alt><mathphrase><![CDATA[1]]></mathphrase></inlineequation>的概率分别是：</simpara>
<simpara><inlineequation><alt><![CDATA[(1 - \frac{1}{m})^{nk}]]></alt><mathphrase><![CDATA[(1 - \frac{1}{m})^{nk}]]></mathphrase></inlineequation>和<inlineequation><alt><![CDATA[1-(1-\frac{1}{m})^{nk}]]></alt><mathphrase><![CDATA[1-(1-\frac{1}{m})^{nk}]]></mathphrase></inlineequation>。</simpara>
<simpara>标明某个元素是否在集合中所需的 k 个位置都按照如上的方法设置为
1，但是该方法可能会使算法错误的认为某一原本不在集合中的元素却被检测为在该集合中（False
Positives），该概率由以下公式确定</simpara>
<informalequation>
<alt><![CDATA[\[
[1-(1-\frac{1}{m})^{nk}]^k \approx (1 - e^{\frac{-kn}{m}})^k
\]]]></alt>
<mathphrase><![CDATA[\[
[1-(1-\frac{1}{m})^{nk}]^k \approx (1 - e^{\frac{-kn}{m}})^k
\]]]></mathphrase>
</informalequation>
<simpara><emphasis role="strong">最佳实践</emphasis></simpara>
<simpara>常见的使用场景有，利用布隆过滤器减少磁盘 IO
或者网络请求，因为一旦一个值必定不存在的话，我们可以不用进行后续昂贵的查询请求。</simpara>
<simpara>另外，既然你使用布隆过滤器来加速查找和判断是否存在，那么性能很低的哈希函数不是个好选择，推荐
MurmurHash、Fnv 这些。</simpara>
<simpara><emphasis>大Value拆分</emphasis></simpara>
<simpara>Redis 因其支持 setbit 和 getbit
操作，且纯内存性能高等特点，因此天然就可以作为布隆过滤器来使用。但是布隆过滤器的不当使用极易产生大
Value，增加 Redis
阻塞风险，因此生成环境中建议对体积庞大的布隆过滤器进行拆分。</simpara>
<simpara>拆分的形式方法多种多样，但是本质是不要将 Hash(Key)
之后的请求分散在多个节点的多个小 bitmap 上，而是应该拆分成多个小 bitmap
之后，对一个 Key 的所有哈希函数都落在这一个小 bitmap 上。</simpara>
</section>
<section xml:id="_布隆过滤器版">
<title>布隆过滤器版</title>
<programlisting language="java" linenumbering="unnumbered">public class UVBloomFilter {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        env
            .readTextFile("UserBehavior.csv")
            .map(new MapFunction&lt;String, UserBehavior&gt;() {
                @Override
                public UserBehavior map(String value) throws Exception {
                    String[] arr = value.split(",");
                    return new UserBehavior(
                            arr[0],arr[1],arr[2],arr[3],
                            Long.parseLong(arr[4]) * 1000L
                    );
                }
            })
            .filter(r -&gt; r.behaviorType.equals("pv"))
            .assignTimestampsAndWatermarks(WatermarkStrategy.&lt;UserBehavior&gt;forMonotonousTimestamps()
                    .withTimestampAssigner(new SerializableTimestampAssigner&lt;UserBehavior&gt;() {
                        @Override
                        public long extractTimestamp(UserBehavior element, long recordTimestamp) {
                            return element.timestamp;
                        }
                    }))
            .map(new MapFunction&lt;UserBehavior, Tuple2&lt;String, String&gt;&gt;() {
                @Override
                public Tuple2&lt;String, String&gt; map(UserBehavior value) throws Exception {
                    return Tuple2.of("key", value.userId);
                }
            })
            .keyBy(r -&gt; r.f0)
            .window(TumblingEventTimeWindows.of(Time.hours(1)))
            .aggregate(new Agg(), new WindowResult())
            .print();

        env.execute();
    }

    public static class Agg implements AggregateFunction&lt;Tuple2&lt;String, String&gt;, Tuple2&lt;BloomFilter&lt;Long&gt;, Long&gt;, Long&gt; {
        @Override
        public Tuple2&lt;BloomFilter&lt;Long&gt;, Long&gt; createAccumulator() {
            // 假设独立用户数量是一百万，误判率是0.01
            return Tuple2.of(BloomFilter.create(Funnels.longFunnel(), 1000000, 0.01), 0L);
        }

        @Override
        public Tuple2&lt;BloomFilter&lt;Long&gt;, Long&gt; add(Tuple2&lt;String, String&gt; value, Tuple2&lt;BloomFilter&lt;Long&gt;, Long&gt; accumulator) {
            if (!accumulator.f0.mightContain(Long.parseLong(value.f1))) {
                // 如果userID没来过，那么执行put操作
                accumulator.f0.put(Long.parseLong(value.f1));
                accumulator.f1 += 1L; // UV数量加一
                return accumulator;
            }
            return accumulator;
        }

        @Override
        public Long getResult(Tuple2&lt;BloomFilter&lt;Long&gt;, Long&gt; accumulator) {
            return accumulator.f1;
        }

        @Override
        public Tuple2&lt;BloomFilter&lt;Long&gt;, Long&gt; merge(Tuple2&lt;BloomFilter&lt;Long&gt;, Long&gt; a, Tuple2&lt;BloomFilter&lt;Long&gt;, Long&gt; b) {
            return null;
        }
    }

    public static class WindowResult extends ProcessWindowFunction&lt;Long, String, String, TimeWindow&gt; {
        @Override
        public void process(String s, Context context, Iterable&lt;Long&gt; elements, Collector&lt;String&gt; out) throws Exception {
            String start = new Timestamp(context.window().getStart()).toString();
            String end = new Timestamp(context.window().getEnd()).toString();

            out.collect("窗口 " + start + " ~~~ " + end + " 的UV数据是：" + elements.iterator().next());
        }
    }
}</programlisting>
</section>
</section>
<section xml:id="_订单超时检测">
<title>订单超时检测</title>
<programlisting language="java" linenumbering="unnumbered">public class OrderTimeoutDetectWithoutCEP {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        SingleOutputStreamOperator&lt;Tuple3&lt;String, String, Long&gt;&gt; orderStream = env
            .fromElements(
                Tuple3.of("order-1", "create", 1000L),
                Tuple3.of("order-2", "create", 2000L),
                Tuple3.of("order-1", "pay", 3000L)
            )
            .assignTimestampsAndWatermarks(WatermarkStrategy.&lt;Tuple3&lt;String, String, Long&gt;&gt;forMonotonousTimestamps()
                .withTimestampAssigner(new SerializableTimestampAssigner&lt;Tuple3&lt;String, String, Long&gt;&gt;() {
                    @Override
                    public long extractTimestamp(Tuple3&lt;String, String, Long&gt; element, long recordTimestamp) {
                        return element.f2;
                    }
                }));

        orderStream
            .keyBy(r -&gt; r.f0)
            .process(new KeyedProcessFunction&lt;String, Tuple3&lt;String, String, Long&gt;, String&gt;() {
                private ValueState&lt;Tuple3&lt;String, String, Long&gt;&gt; orderState;
                @Override
                public void open(Configuration parameters) throws Exception {
                    super.open(parameters);
                    orderState = getRuntimeContext().getState(
                        new ValueStateDescriptor&lt;Tuple3&lt;String, String, Long&gt;&gt;("order-state", Types.TUPLE(Types.STRING, Types.STRING, Types.LONG))
                    );
                }

                @Override
                public void processElement(Tuple3&lt;String, String, Long&gt; value, Context ctx, Collector&lt;String&gt; out) throws Exception {
                    if (value.f1.equals("pay")) {
                        out.collect("订单ID：" + value.f0 + " 已经支付");
                        orderState.update(value);
                    } else if (value.f1.equals("create")) {
                        if (orderState.value() == null) {
                            // 说明pay事件没有在create事件之前到达
                            ctx.timerService().registerEventTimeTimer(value.f2 + 5 * 1000L);
                            orderState.update(value);
                        }
                    }

                }

                @Override
                public void onTimer(long timestamp, OnTimerContext ctx, Collector&lt;String&gt; out) throws Exception {
                    super.onTimer(timestamp, ctx, out);
                    if (orderState.value() != null &amp;&amp; orderState.value().f1.equals("create")) {
                        out.collect("订单ID：" + ctx.getCurrentKey() + " 支付超时");
                        orderState.clear();
                    }
                }
            })
            .print();

        env.execute();
    }
}</programlisting>
</section>
<section xml:id="_实时对帐">
<title>实时对帐</title>
<programlisting language="java" linenumbering="unnumbered">public class TwoStreamJoin {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        SingleOutputStreamOperator&lt;Tuple3&lt;String, String, Long&gt;&gt; orderStream = env
                .fromElements(
                        Tuple3.of("order-1", "pay", 1000L),
                        Tuple3.of("order-2", "pay", 2000L)
                )
                .assignTimestampsAndWatermarks(WatermarkStrategy.&lt;Tuple3&lt;String, String, Long&gt;&gt;forMonotonousTimestamps()
                        .withTimestampAssigner(new SerializableTimestampAssigner&lt;Tuple3&lt;String, String, Long&gt;&gt;() {
                            @Override
                            public long extractTimestamp(Tuple3&lt;String, String, Long&gt; element, long recordTimestamp) {
                                return element.f2;
                            }
                        }));

        SingleOutputStreamOperator&lt;Tuple3&lt;String, String, Long&gt;&gt; payStream = env
                .fromElements(
                        Tuple3.of("order-1", "weixin", 3000L),
                        Tuple3.of("order-3", "weixin", 4000L)
                )
                .assignTimestampsAndWatermarks(WatermarkStrategy.&lt;Tuple3&lt;String, String, Long&gt;&gt;forMonotonousTimestamps()
                        .withTimestampAssigner(new SerializableTimestampAssigner&lt;Tuple3&lt;String, String, Long&gt;&gt;() {
                            @Override
                            public long extractTimestamp(Tuple3&lt;String, String, Long&gt; element, long recordTimestamp) {
                                return element.f2;
                            }
                        }));

        SingleOutputStreamOperator&lt;String&gt; result = orderStream
                .keyBy(r -&gt; r.f0)
                .connect(payStream.keyBy(r -&gt; r.f0))
                .process(new MatchFunction());

        result.print();

        env.execute();
    }

    public static class MatchFunction extends CoProcessFunction&lt;Tuple3&lt;String, String, Long&gt;, Tuple3&lt;String, String, Long&gt;, String&gt; {
        private ValueState&lt;Tuple3&lt;String, String, Long&gt;&gt; orderState;
        private ValueState&lt;Tuple3&lt;String, String, Long&gt;&gt; payState;
        @Override
        public void open(Configuration parameters) throws Exception {
            super.open(parameters);
            orderState = getRuntimeContext().getState(new ValueStateDescriptor&lt;Tuple3&lt;String, String, Long&gt;&gt;("order", Types.TUPLE(Types.STRING, Types.STRING, Types.LONG)));
            payState = getRuntimeContext().getState(new ValueStateDescriptor&lt;Tuple3&lt;String, String, Long&gt;&gt;("pay", Types.TUPLE(Types.STRING, Types.STRING, Types.LONG)));
        }

        @Override
        public void processElement1(Tuple3&lt;String, String, Long&gt; value, Context ctx, Collector&lt;String&gt; out) throws Exception {
            if (payState.value() != null) {
                out.collect("订单" + value.f0 + "对账成功");
                payState.clear();
            } else {
                orderState.update(value); // 说明app端支付事件先到了
                ctx.timerService().registerEventTimeTimer(value.f2 + 5000L); // order事件等待pay事件5秒钟
            }
        }

        @Override
        public void processElement2(Tuple3&lt;String, String, Long&gt; value, Context ctx, Collector&lt;String&gt; out) throws Exception {
            if (orderState.value() != null) {
                out.collect("订单" + value.f0 + "对账成功");
                orderState.clear();
            } else {
                payState.update(value);
                ctx.timerService().registerEventTimeTimer(value.f2 + 5000L); // pay事件等待order事件5秒钟
            }
        }

        @Override
        public void onTimer(long timestamp, OnTimerContext ctx, Collector&lt;String&gt; out) throws Exception {
            super.onTimer(timestamp, ctx, out);
            if (orderState.value() != null) {
                out.collect("订单" + orderState.value().f0 + "对账失败");
                orderState.clear();
            }
            if (payState.value() != null) {
                out.collect("订单" + payState.value().f0 + "对账失败");
                payState.clear();
            }
        }
    }
}</programlisting>
</section>
<section xml:id="_实时热门商品">
<title>实时热门商品</title>
<section xml:id="_实现思路">
<title>实现思路</title>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/userbehavior-overall.svg"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara><emphasis role="strong">先来做分流</emphasis></simpara>
<programlisting language="java" linenumbering="unnumbered">.keyBy(r -&gt; r.itemId)</programlisting>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/userbehavior-keyby.svg"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara><emphasis role="strong">开窗</emphasis></simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/usebehavior-window.svg"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara><emphasis role="strong">聚合</emphasis></simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/usebehavior-agg.svg"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara><emphasis role="strong">使用窗口结束时间分流</emphasis></simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/userbehavior-windowend.svg"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/usebehavior-liststate.svg"/>
</imageobject>
<textobject><phrase>image</phrase></textobject>
</inlinemediaobject></simpara>
</section>
<section xml:id="_底层api实现方式">
<title>底层API实现方式</title>
<simpara><emphasis>UserBehavior POJO类</emphasis></simpara>
<programlisting language="java" linenumbering="unnumbered">// 用户行为的每一条数据就是一个POJO类
public class UserBehavior {
    public String userId;
    public String itemId;
    public String categoryId;
    public String behaviorType;
    public Long timestamp;

    public UserBehavior() {
    }

    public UserBehavior(String userId, String itemId, String categoryId, String behaviorType, Long timestamp) {
        this.userId = userId;
        this.itemId = itemId;
        this.categoryId = categoryId;
        this.behaviorType = behaviorType;
        this.timestamp = timestamp;
    }

    @Override
    public String toString() {
        return "UserBehavior{" +
                "userId='" + userId + '\'' +
                ", itemId='" + itemId + '\'' +
                ", categoryId='" + categoryId + '\'' +
                ", behaviorType='" + behaviorType + '\'' +
                ", timestamp=" + new Timestamp(timestamp) +
                '}';
    }
}</programlisting>
<simpara><emphasis>ItemViewCount POJO类</emphasis></simpara>
<programlisting language="java" linenumbering="unnumbered">// 每个窗口里面，商品ID的PV数据
public class ItemViewCount {
    public String itemId;
    public Long count; // pv 数据
    public Long windowStart;
    public Long windowEnd;

    public ItemViewCount() {
    }

    public ItemViewCount(String itemId, Long count, Long windowStart, Long windowEnd) {
        this.itemId = itemId;
        this.count = count;
        this.windowStart = windowStart;
        this.windowEnd = windowEnd;
    }

    @Override
    public String toString() {
        return "ItemViewCount{" +
                "itemId='" + itemId + '\'' +
                ", count=" + count +
                ", windowStart=" + new Timestamp(windowStart) +
                ", windowEnd=" + new Timestamp(windowEnd) +
                '}';
    }
}</programlisting>
<simpara><emphasis>主体程序</emphasis></simpara>
<programlisting language="java" linenumbering="unnumbered">public class TopNHotItems {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        SingleOutputStreamOperator&lt;UserBehavior&gt; userBehaviorStream = env
                .readTextFile("/Users/yuanzuo/Desktop/Flink1125SH/src/main/resources/UserBehavior.csv")
                .map(new MapFunction&lt;String, UserBehavior&gt;() {
                    @Override
                    public UserBehavior map(String value) throws Exception {
                        String[] arr = value.split(",");
                        return new UserBehavior(
                                arr[0], arr[1], arr[2], arr[3],
                                Long.parseLong(arr[4]) * 1000L
                        );
                    }
                })
                .filter(r -&gt; r.behaviorType.equals("pv"))
                // 对于离线数据集来讲，flink只会插入连个水位线，开始的负无穷大，末尾的正无穷大
                .assignTimestampsAndWatermarks(WatermarkStrategy.&lt;UserBehavior&gt;forMonotonousTimestamps()
                        .withTimestampAssigner(new SerializableTimestampAssigner&lt;UserBehavior&gt;() {
                            @Override
                            public long extractTimestamp(UserBehavior element, long recordTimestamp) {
                                return element.timestamp;
                            }
                        }));

        SingleOutputStreamOperator&lt;ItemViewCount&gt; itemViewCountStream = userBehaviorStream
                .keyBy(r -&gt; r.itemId)
                // 进到了窗口的数据有什么特点？
                // 相同的itemId
                .window(SlidingEventTimeWindows.of(Time.hours(1), Time.minutes(5)))
                .aggregate(new CountAgg(), new WindowResult());

        SingleOutputStreamOperator&lt;String&gt; result = itemViewCountStream
                // 每一条支流，属于同一个窗口的不同itemid的pv数量
                .keyBy(r -&gt; r.windowEnd)
                .process(new TopN(3L));

        result.print();

        env.execute();
    }

    public static class CountAgg implements AggregateFunction&lt;UserBehavior, Long, Long&gt; {
        @Override
        public Long createAccumulator() {
            return 0L;
        }

        @Override
        public Long add(UserBehavior value, Long accumulator) {
            return accumulator + 1L;
        }

        @Override
        public Long getResult(Long accumulator) {
            return accumulator;
        }

        @Override
        public Long merge(Long a, Long b) {
            return null;
        }
    }

    public static class WindowResult extends ProcessWindowFunction&lt;Long, ItemViewCount, String, TimeWindow&gt; {
        @Override
        public void process(String itemId, Context ctx, Iterable&lt;Long&gt; elements, Collector&lt;ItemViewCount&gt; out) throws Exception {
            out.collect(new ItemViewCount(itemId, elements.iterator().next(), ctx.window().getStart(), ctx.window().getEnd()));
        }
    }

    public static class TopN extends KeyedProcessFunction&lt;Long, ItemViewCount, String&gt; {
        public Long threshold;

        public ListState&lt;ItemViewCount&gt; itemViewCountListState;

        public TopN(Long threshold) {
            this.threshold = threshold;
        }

        @Override
        public void open(Configuration parameters) throws Exception {
            super.open(parameters);
            itemViewCountListState = getRuntimeContext().getListState(
                    new ListStateDescriptor&lt;ItemViewCount&gt;("list", Types.POJO(ItemViewCount.class))
            );
        }

        @Override
        public void processElement(ItemViewCount value, Context ctx, Collector&lt;String&gt; out) throws Exception {
            itemViewCountListState.add(value);
            // 某一个时间戳只能注册一个定时器
            // 所以这里只会在第一条数据到来的时候注册一个定时器
            ctx.timerService().registerEventTimeTimer(value.windowEnd + 1L);
        }

        @Override
        public void onTimer(long timestamp, OnTimerContext ctx, Collector&lt;String&gt; out) throws Exception {
            super.onTimer(timestamp, ctx, out);
            // 定时器用来排序
            ArrayList&lt;ItemViewCount&gt; itemViewCountArrayList = new ArrayList&lt;&gt;();
            for (ItemViewCount ivc : itemViewCountListState.get()) {
                itemViewCountArrayList.add(ivc);
            }
            // 清空列表状态变量
            itemViewCountListState.clear();

            // 排序
            itemViewCountArrayList.sort(new Comparator&lt;ItemViewCount&gt;() {
                @Override
                public int compare(ItemViewCount o1, ItemViewCount o2) {
                    return o2.count.intValue() - o1.count.intValue();
                }
            });

            StringBuilder result = new StringBuilder();

            result
                    .append("====================================\n")
                    .append("窗口结束时间：")
                    .append(new Timestamp(timestamp - 1L)) // 恢复窗口结束时间
                    .append("\n");

            for (int i = 0; i &lt; this.threshold; i++) {
                ItemViewCount itemViewCount = itemViewCountArrayList.get(i);
                result
                        .append("第" + (i + 1) + "名商品的ID是：" + itemViewCount.itemId)
                        .append("\n")
                        .append("PV数量是：" + itemViewCount.count)
                        .append("\n");
            }

            result
                    .append("====================================\n");
            out.collect(result.toString());
        }
    }
}</programlisting>
</section>
<section xml:id="_flink_sql实现方式">
<title>Flink SQL实现方式</title>
<programlisting language="java" linenumbering="unnumbered">public class TopNSQL {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        SingleOutputStreamOperator&lt;UserBehavior&gt; stream = env
                .readTextFile("UserBehavior.csv")
                .map(new MapFunction&lt;String, UserBehavior&gt;() {
                    @Override
                    public UserBehavior map(String value) throws Exception {
                        String[] arr = value.split(",");
                        return new UserBehavior(arr[0], arr[1], arr[2], arr[3], Long.parseLong(arr[4]) * 1000L);
                    }
                })
                .filter(r -&gt; r.behaviorType.equals("pv"))
                .assignTimestampsAndWatermarks(WatermarkStrategy.&lt;UserBehavior&gt;forMonotonousTimestamps()
                        .withTimestampAssigner(new SerializableTimestampAssigner&lt;UserBehavior&gt;() {
                            @Override
                            public long extractTimestamp(UserBehavior element, long recordTimestamp) {
                                return element.timestamp;
                            }
                        }));

        // SQL
        EnvironmentSettings settings = EnvironmentSettings.newInstance().inStreamingMode().build();

        StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, settings);

        tEnv.createTemporaryView("t", stream, $("itemId"), $("timestamp").rowtime().as("ts"));

        // 按照itemid分流开窗聚合，结果是每一个itemid在每一个窗口的浏览量
        String innerSQL = "SELECT itemId, COUNT(itemId) as itemCount, HOP_END(ts, INTERVAL '5' MINUTE, INTERVAL '1' HOUR) as windowEnd" +
                " FROM t GROUP BY HOP(ts, INTERVAL '5' MINUTE, INTERVAL '1' HOUR), itemId";

        // 按照windowEnd分区，然后按照浏览量降序排列
        String midSQL = "SELECT *, ROW_NUMBER() OVER (PARTITION BY windowEnd ORDER BY itemCount DESC) as row_num" +
                " FROM (" + innerSQL + ")";

        // 取出前三名
        String outerSQL = "SELECT * FROM (" + midSQL + ") WHERE row_num &lt;= 3";

        Table result = tEnv.sqlQuery(outerSQL);

        tEnv.toRetractStream(result, Row.class).print();

        env.execute();
    }
}</programlisting>
</section>
</section>
</section>
<section xml:id="_逻辑时钟_如何刻画分布式中的事件顺序">
<title>逻辑时钟 - 如何刻画分布式中的事件顺序</title>
<simpara>逻辑时钟是描述分布式系统中时序和因果关系的一种机制。
由于网络延迟、时钟漂移等现实问题，我们无法建立一个全局物理时钟来描述时序，
因此区别于物理时钟的「逻辑时钟」机制应运而生。
第一个逻辑时钟算法是由分布式领域的大神 Lamport 在1978年提出的 lamport
时钟算法。</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/repr-clock.jpeg"/>
</imageobject>
<textobject><phrase>时钟</phrase></textobject>
</mediaobject>
</informalfigure>
<simpara>本文将从一个基础问题的讨论切入，逐步介绍：</simpara>
<itemizedlist>
<listitem>
<simpara>为什么需要逻辑时钟 – 物理时钟是否可行？</simpara>
</listitem>
<listitem>
<simpara>相对论是带来了什么启示 – 事件的相对性</simpara>
</listitem>
<listitem>
<simpara>最早期的逻辑时钟算法什么特点 – Lamport逻辑时钟</simpara>
</listitem>
<listitem>
<simpara>后续改进的算法有哪些 – 「向量时钟」和「版本向量」等等</simpara>
</listitem>
</itemizedlist>
<simpara>逻辑时钟的思想有趣而深刻，值得探究。</simpara>
<section xml:id="_如何确定分布式系统中事件的发生顺序">
<title>如何确定分布式系统中事件的发生顺序？</title>
<simpara>首先，我们来观察一个「如何确定分布式系统中事件的发生顺序」的问题：</simpara>
<simpara>在这个分布式系统中，有三个独立的进程A、B、C：</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>首先，进程A中发生了一个事件a，并把这个事件消息同步给另外两个进程B和C。</simpara>
</listitem>
<listitem>
<simpara>B收到消息后，发生了一个事件b，并把这个事件消息同步给进程C。</simpara>
</listitem>
<listitem>
<simpara>但是由于无法确定的网络延迟原因，导致进程A发出的消息到达C晚于进程B发出的消息到达C，这样
进程C的视角上，最终看到的事件顺序是b,a，但是这与事实是相悖的。</simpara>
</listitem>
</orderedlist>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/appendix-a-01.jpeg"/>
</imageobject>
<textobject><phrase>由于网络延迟，进程B发出的消息晚于进程A发出的消息到达进程C</phrase></textobject>
</mediaobject>
</informalfigure>
<simpara>可以看到，在分布式通信中，由于网络延迟的不确定性，<emphasis role="strong">仅仅以接收顺序作为整个分布式系统中事件的发生顺序是不可取的</emphasis>。</simpara>
<simpara>下面我们再观察一个稍微现实点的例子。</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>假设朋友圈有三个数据中心，分别在北京、维也纳和纽约。</simpara>
</listitem>
<listitem>
<simpara>北京小明在朋友圈中发了一张风景图，并问到「猜猜这里是哪里?」，这条消息被扩散到其他数据中心。</simpara>
</listitem>
<listitem>
<simpara>维也纳的小红看到这个消息后，回复说她知道。这条回复也被扩散到其他数据中心。</simpara>
</listitem>
<listitem>
<simpara>但是由于无法确定的网络延迟的原因，导致纽约的数据中心先收到小红的回复，而后收到了原始的提问消息。
这样，导致最终小李看到的问答顺序是不符合问答的因果一致性的。</simpara>
</listitem>
</orderedlist>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/appendix-a-02.jpeg"/>
</imageobject>
<textobject><phrase>由于网络延迟，小李看到的事件顺序并不符合问答的因果关系</phrase></textobject>
</mediaobject>
</informalfigure>
<simpara>以上的例子，是腾讯微信朋友圈真实碰到并解决的问题。</simpara>
<simpara>下面是微信朋友圈架构设计的两个关于因果一致性算法设计的PPT截图：</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/appendix-a-03.jpeg"/>
</imageobject>
<textobject><phrase>《微信朋友圈技术之道》中分享的关于因果性的PPT</phrase></textobject>
</mediaobject>
</informalfigure>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/appendix-a-04.jpeg"/>
</imageobject>
<textobject><phrase>《微信朋友圈技术之道》中分享的关于向量时钟的PPT</phrase></textobject>
</mediaobject>
</informalfigure>
<simpara>在腾讯的分享PPT中，提到了「向量时钟」， 不过我们稍后再揭开它的神秘面纱。</simpara>
<simpara>而这两个例子中，我们面临的要解决的问题，
其实是分布式系统中的因果一致性，
换句话说，就是如何准确刻画分布式中的事件顺序，<emphasis role="strong">显然仅仅简单地依靠进程接收到的(看到的)事件发生顺序是不准确的</emphasis>。</simpara>
</section>
<section xml:id="_全局物理时钟">
<title>全局物理时钟</title>
<simpara>对于上面提到的两个问题，我们试图解决的一个天然想法是，记录每个分布式进程中发生事件的原始时间戳，并把它连同事件本身扩散到其他节点，这样其他节点的视角上就可以观察到完整的因果顺序了？</simpara>
<blockquote>
<simpara>If you have one clock, you know what the time is. If you have two, you
are not sure.</simpara>
</blockquote>
<simpara>大家都清楚的一点是，<emphasis role="strong">不同节点的物理时钟其实是不一致的，而且无法做到精确一致</emphasis>。</simpara>
<simpara>其原因：</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>仍然是由于网络延迟的不确定，我们无法通过网络同步时间来获取一个全局一致的物理时钟。</simpara>
</listitem>
<listitem>
<simpara>现实中的多个时钟，即使时间已经调成一致，但是由于日积月累的计时速率的差异，会导致时钟漂移而显示不同的时间。</simpara>
</listitem>
</orderedlist>
<simpara>如此看来，<emphasis role="strong">寄希望于一个全局的时钟来对事件顺序做全局标定也是不现实的</emphasis>。</simpara>
</section>
<section xml:id="_全序和偏序">
<title>全序和偏序</title>
<simpara>在数学上，「顺序」是如何描述的？</simpara>
<simpara>我们看下序理论中的两种序关系：偏序(partial ordering) 和 全序 (total
ordering).</simpara>
<simpara>偏序：
假设<inlineequation><alt><![CDATA[\leq]]></alt><mathphrase><![CDATA[\leq]]></mathphrase></inlineequation>是集合<inlineequation><alt><![CDATA[S]]></alt><mathphrase><![CDATA[S]]></mathphrase></inlineequation>上的一个二元关系，如果<inlineequation><alt><![CDATA[\leq]]></alt><mathphrase><![CDATA[\leq]]></mathphrase></inlineequation>满足：</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>自反性：
对<inlineequation><alt><![CDATA[S]]></alt><mathphrase><![CDATA[S]]></mathphrase></inlineequation>中任意的元素<inlineequation><alt><![CDATA[a]]></alt><mathphrase><![CDATA[a]]></mathphrase></inlineequation>，都有<inlineequation><alt><![CDATA[a \leq a]]></alt><mathphrase><![CDATA[a \leq a]]></mathphrase></inlineequation></simpara>
</listitem>
<listitem>
<simpara>反对称性：
如果对于<inlineequation><alt><![CDATA[S]]></alt><mathphrase><![CDATA[S]]></mathphrase></inlineequation>中的两个元素<inlineequation><alt><![CDATA[a]]></alt><mathphrase><![CDATA[a]]></mathphrase></inlineequation>和<inlineequation><alt><![CDATA[b]]></alt><mathphrase><![CDATA[b]]></mathphrase></inlineequation>，<inlineequation><alt><![CDATA[a \leq b]]></alt><mathphrase><![CDATA[a \leq b]]></mathphrase></inlineequation>且<inlineequation><alt><![CDATA[b \leq a]]></alt><mathphrase><![CDATA[b \leq a]]></mathphrase></inlineequation>，那么<inlineequation><alt><![CDATA[a = b]]></alt><mathphrase><![CDATA[a = b]]></mathphrase></inlineequation></simpara>
</listitem>
<listitem>
<simpara>传递性：
如果对于<inlineequation><alt><![CDATA[S]]></alt><mathphrase><![CDATA[S]]></mathphrase></inlineequation>中的三个元素，有<inlineequation><alt><![CDATA[a \leq b]]></alt><mathphrase><![CDATA[a \leq b]]></mathphrase></inlineequation>且<inlineequation><alt><![CDATA[b \leq c]]></alt><mathphrase><![CDATA[b \leq c]]></mathphrase></inlineequation>，那么<inlineequation><alt><![CDATA[a \leq c]]></alt><mathphrase><![CDATA[a \leq c]]></mathphrase></inlineequation></simpara>
</listitem>
</orderedlist>
<simpara>以上的数学内容其实不那么重要~ 关键理解：
<emphasis role="strong">偏序关系是一种序关系，但只是部分元素有序，并不是全部元素都可以比较</emphasis>。</simpara>
<simpara>全序则比偏序的要求更为严格一些，在偏序的基础上，多了一个完全性的条件：</simpara>
<itemizedlist>
<listitem>
<simpara>完全性：对于<inlineequation><alt><![CDATA[S]]></alt><mathphrase><![CDATA[S]]></mathphrase></inlineequation>中的任意<inlineequation><alt><![CDATA[a]]></alt><mathphrase><![CDATA[a]]></mathphrase></inlineequation>和<inlineequation><alt><![CDATA[b]]></alt><mathphrase><![CDATA[b]]></mathphrase></inlineequation>元素，必然有<inlineequation><alt><![CDATA[a \leq b]]></alt><mathphrase><![CDATA[a \leq b]]></mathphrase></inlineequation>或<inlineequation><alt><![CDATA[b \leq a]]></alt><mathphrase><![CDATA[b \leq a]]></mathphrase></inlineequation></simpara>
</listitem>
</itemizedlist>
<simpara>可以看出，实际上，<emphasis role="strong">全序就是在偏序的基础上，要求全部元素都必须可以比较</emphasis>。</simpara>
<simpara>总结来看，简短说：<emphasis role="strong">偏序是部分可比较的序关系，全序是全部可比较的序关系</emphasis>。</simpara>
<simpara>举例来看：</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>自然数集合中的比较大小的关系，就是一种全序关系。</simpara>
</listitem>
<listitem>
<simpara>集合之间的包含关系，则是一种偏序关系（集合之间可以有包含关系，也可以没有包含关系）。</simpara>
</listitem>
<listitem>
<simpara>复数之间的大小关系，是一种偏序关系，而复数的模的大小关系，则是一种全序关系。</simpara>
</listitem>
</orderedlist>
<simpara>下面用简单的有向图(哈斯图)来描述下全序和偏序的区别：</simpara>
<simpara>下图中<inlineequation><alt><![CDATA[S_1]]></alt><mathphrase><![CDATA[S_1]]></mathphrase></inlineequation>上的图示关系，描述的是整数之间的大小顺序，是一种全序关系，可以看到任意两个元素之间都可以比较顺序。</simpara>
<simpara>而<inlineequation><alt><![CDATA[S_2]]></alt><mathphrase><![CDATA[S_2]]></mathphrase></inlineequation>上的关系，描述的是集合之间的包含关系，是一种偏序关系，其中<inlineequation><alt><![CDATA[v_2]]></alt><mathphrase><![CDATA[v_2]]></mathphrase></inlineequation>和<inlineequation><alt><![CDATA[v_3]]></alt><mathphrase><![CDATA[v_3]]></mathphrase></inlineequation>是不可比较的。</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/appendix-a-05.jpeg"/>
</imageobject>
<textobject><phrase>latexmath:[$S_1$]描述了全序关系；latexmath:[$S_2$]描述了偏序关系</phrase></textobject>
</mediaobject>
</informalfigure>
<simpara>现在我们回头看，寻求全局时钟为分布式节点中的时间做顺序标定的方式，
其实是在寻求一种全序关系来描述分布式中的事件顺序，
而且是严格对齐真实的物理时钟的全序关系。</simpara>
</section>
<section xml:id="_事件先后的相对性">
<title>事件先后的相对性</title>
<simpara>逻辑时钟的概念是由著名的分布式系统科学家 Leslie Lamport
(2013年图灵奖得主) 提出的， 在他的那篇著名的论文「Time, Clocks and the
Ordering of Events in a Distributed System」
的介绍上，Lamport提到了著名的狭义相对论：</simpara>
<blockquote>
<simpara>Special relativity teaches us that there is no invariant total ordering
of events in space-time; different observers can disagree about which of
two events happened first. There is only a partial order in which an
event e1 precedes an event e2 iff e1 can causally affect e2. - Leslie
Lamport 《Time, Clocks and the Ordering of Events in a Distributed
System》</simpara>
</blockquote>
<simpara>爱因斯坦的狭义相对论告诉我们，<emphasis role="strong">时空中不存在绝对的全序事件顺序，
不同的观察者可能对哪个事件是先发生的无法达成一致。
但是有偏序关系存在，当事件<inlineequation><alt><![CDATA[e_2]]></alt><mathphrase><![CDATA[e_2]]></mathphrase></inlineequation>是由事件<inlineequation><alt><![CDATA[e_1]]></alt><mathphrase><![CDATA[e_1]]></mathphrase></inlineequation>引起的时候，<inlineequation><alt><![CDATA[e_1]]></alt><mathphrase><![CDATA[e_1]]></mathphrase></inlineequation>和<inlineequation><alt><![CDATA[e_2]]></alt><mathphrase><![CDATA[e_2]]></mathphrase></inlineequation>之间才有先后关系</emphasis>。</simpara>
<simpara>对于「不同的观察者可能对哪个事件是先发生的无法达成一致」这个说法，
我们从同时的相对性开始说起：</simpara>
<blockquote>
<simpara>根据狭义相对论，发生在空间中不同位置的两个事件，它们的同时性并不具有绝对的意义，
我们没办法肯定地说它们是否为同时发生。若在某一参考系中此两事件是同时的，则在另一相对于原参考系等速运动的新参考系中，此两事件将不再同时。
- 维基百科「同时的相对性」</simpara>
</blockquote>
<simpara>因为狭义相对论最基本的假设，光速不变原理：无论在何种惯性参考系下，光在真空中的传播速度相对观察者都是一个常数。所以「同时」这个概念也是相对的。</simpara>
<simpara>关于同时的相对性，有一个著名的火车思想实验：</simpara>
<simpara>有一个观察者<inlineequation><alt><![CDATA[A]]></alt><mathphrase><![CDATA[A]]></mathphrase></inlineequation>在移动的火车中间，有另一位观察者<inlineequation><alt><![CDATA[B]]></alt><mathphrase><![CDATA[B]]></mathphrase></inlineequation>在地面上的月台上，
当两个观察者相遇时，一道闪光从火车的中央发出。</simpara>
<simpara>对于火车上的观察者<inlineequation><alt><![CDATA[A]]></alt><mathphrase><![CDATA[A]]></mathphrase></inlineequation>而言，由于火车头和火车尾距离光源的距离是相同的，
因此观察到了光同时到达了车头和车尾。</simpara>
<simpara>但是对于月台的观察者<inlineequation><alt><![CDATA[B]]></alt><mathphrase><![CDATA[B]]></mathphrase></inlineequation>而言，火车的尾部会迎向光移动，而车头会远离光移动，而且光速是有限的，且相对于两个观察者都是相同的常数，所以<inlineequation><alt><![CDATA[B]]></alt><mathphrase><![CDATA[B]]></mathphrase></inlineequation>认为光会先到达车尾，后到达车头。</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/appendix-a-06.jpeg"/>
</imageobject>
<textobject><phrase>狭义相对论中著名的火车思想实验</phrase></textobject>
</mediaobject>
</informalfigure>
<simpara>这样，对于不同参照系的观察者而言，事件的顺序并没有一个一致性的结论。之所以得出这样神奇的结论，仍然是因为关键的「光速不变原理」。但是，这并不意味着发生了因果上的逻辑矛盾，
我们在这种情况下，只是无法在不同的参照系下观察到一致性的事件顺序。</simpara>
<simpara>我们经验上所说的「同时发生」，是因为光速太大，或者我们生活的尺度太小，所以同时是一种近乎同时。</simpara>
<simpara>相对论告诉我们，光速是物质移动的最大速度，信息传播的速度不可能超过这个速度。假如太阳消失了，地球上的我们也要在8分钟之后感知到太阳消失了。</simpara>
<simpara>也就是说，一个事件<inlineequation><alt><![CDATA[A]]></alt><mathphrase><![CDATA[A]]></mathphrase></inlineequation>发生后，载有这个事件信息的光（引力、无论什么，快不过光速）到达观察者之前，
观察者是无法没有任何感知的， 这时我们就无法定义事件的顺序。
而当信息最终传播到达观察者时，这个事件也就对观察者发生了影响，造成了一个新的事件<inlineequation><alt><![CDATA[B]]></alt><mathphrase><![CDATA[B]]></mathphrase></inlineequation>，叫做「观察到了事件<inlineequation><alt><![CDATA[A]]></alt><mathphrase><![CDATA[A]]></mathphrase></inlineequation>的事件<inlineequation><alt><![CDATA[B]]></alt><mathphrase><![CDATA[B]]></mathphrase></inlineequation>」。这时我们才有<inlineequation><alt><![CDATA[A]]></alt><mathphrase><![CDATA[A]]></mathphrase></inlineequation>
happened before <inlineequation><alt><![CDATA[B]]></alt><mathphrase><![CDATA[B]]></mathphrase></inlineequation>的因果关系。</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/appendix-a-07.jpeg"/>
</imageobject>
<textobject><phrase>物理时空中因果关系建立了事件顺序</phrase></textobject>
</mediaobject>
</informalfigure>
<simpara>可以看到，时空对于描述事件顺序的「happened before」同样是偏序关系。</simpara>
<simpara>因为事件<inlineequation><alt><![CDATA[A]]></alt><mathphrase><![CDATA[A]]></mathphrase></inlineequation>的发生，造成了事件<inlineequation><alt><![CDATA[B]]></alt><mathphrase><![CDATA[B]]></mathphrase></inlineequation>的发生（包括<inlineequation><alt><![CDATA[B]]></alt><mathphrase><![CDATA[B]]></mathphrase></inlineequation>这种观察<inlineequation><alt><![CDATA[A]]></alt><mathphrase><![CDATA[A]]></mathphrase></inlineequation>而发生的事件），那么<inlineequation><alt><![CDATA[A]]></alt><mathphrase><![CDATA[A]]></mathphrase></inlineequation>和<inlineequation><alt><![CDATA[B]]></alt><mathphrase><![CDATA[B]]></mathphrase></inlineequation>就存在因果关系。</simpara>
<simpara>Lamport受相对论中事件顺序的相对性的启发，创造了Lamport Logical Clocks。</simpara>
<simpara>我们的分布式系统，和相对论有很多相似之处：</simpara>
<itemizedlist>
<listitem>
<simpara>在物理时空中，信息是通过光速传播的，而在分布式系统中，信息是通过网络传播的。</simpara>
</listitem>
<listitem>
<simpara>在物理时空中，不同参照系下的观察者可能对于事件顺序无法达成一致，
而在分布式系统中，由于全局物理时钟无法实现，不存在进程拥有全局视角。
而如果进程间的事件没有因果关系，那么就无法达成顺序上的一致性。</simpara>
</listitem>
<listitem>
<simpara>在物理时空中，由于光速限制，观察者在观察到事件<inlineequation><alt><![CDATA[A]]></alt><mathphrase><![CDATA[A]]></mathphrase></inlineequation>的时候，才确定了事件<inlineequation><alt><![CDATA[B]]></alt><mathphrase><![CDATA[B]]></mathphrase></inlineequation>和<inlineequation><alt><![CDATA[A]]></alt><mathphrase><![CDATA[A]]></mathphrase></inlineequation>的因果关系。那么在分布式系统中，我们同样可以通过消息传递来创造因果关系。</simpara>
</listitem>
</itemizedlist>
<simpara>总体来说，
逻辑时钟尝试用「通过进程间创造通信以添加因果关系」的方式来对分布式中的事件顺序做描述。</simpara>
<simpara>下面，我们来看下，「通过通信创造因果关系」
这个设计对于刻画分布式系统中的事件顺序有多么重要。</simpara>
<simpara>观察下图中的两个图：</simpara>
<itemizedlist>
<listitem>
<simpara>红色的点表示自发性事件。
黑色的表示『观察到其他进程事件』而发生的事件。</simpara>
</listitem>
<listitem>
<simpara>横向黑色实线代表物理时钟。</simpara>
</listitem>
<listitem>
<simpara>带箭头的线表示进程中一个事件发生时，向另外一个进程传播这个事件。</simpara>
</listitem>
</itemizedlist>
<simpara>我们试着从每个进程的视角，依次对图<inlineequation><alt><![CDATA[S_1]]></alt><mathphrase><![CDATA[S_1]]></mathphrase></inlineequation>和<inlineequation><alt><![CDATA[S_2]]></alt><mathphrase><![CDATA[S_2]]></mathphrase></inlineequation>进行推导一下，会发现，其实两个图所描述的事件顺序，在进程的相对视角中，是一样的。</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/appendix-a-08.jpeg"/>
</imageobject>
<textobject><phrase>latexmath:[$S_1$]和latexmath:[$S_2$]虽然在物理时序上不一样，但是在各个进程的视角上推导出来却是一样的</phrase></textobject>
</mediaobject>
</informalfigure>
<simpara>我们的逻辑时序应该越接近物理时序越好，然而两个图对时序的刻画，
出现了歧义（比如无法确定<inlineequation><alt><![CDATA[a_3]]></alt><mathphrase><![CDATA[a_3]]></mathphrase></inlineequation>和<inlineequation><alt><![CDATA[b_2]]></alt><mathphrase><![CDATA[b_2]]></mathphrase></inlineequation>的顺序）。根本上是没有做充分的消息传递来添加因果关系。</simpara>
</section>
<section xml:id="_逻辑时钟lamports_timestamp">
<title>逻辑时钟(Lamport’s Timestamp)</title>
<simpara>现在，我们开始讨论Lamport的逻辑时钟算法。</simpara>
<simpara>首先我们需要明确一点：<emphasis role="strong">逻辑时钟并不度量时间本身，仅区分事件发生的前后顺序</emphasis>。</simpara>
<simpara>那么，「事件」是如何分类的：</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>进程内自发的事件（如下图中的红色标记的事件）。</simpara>
</listitem>
<listitem>
<simpara>发送一个消息，是一个事件（如下图中的蓝色标记的事件）。</simpara>
</listitem>
<listitem>
<simpara>接收一个消息，是一个事件 （如下图中的黑灰色标记的事件）。</simpara>
</listitem>
</orderedlist>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/appendix-a-09.jpeg"/>
</imageobject>
<textobject><phrase>分布式中事件的分类</phrase></textobject>
</mediaobject>
</informalfigure>
<simpara>我们上面有提到「happened
before」的关系，我们知道，因果关系是推导「happened
before」关系的重要一环：</simpara>
<blockquote>
<simpara>if <inlineequation><alt><![CDATA[e_i]]></alt><mathphrase><![CDATA[e_i]]></mathphrase></inlineequation> causes <inlineequation><alt><![CDATA[e_j]]></alt><mathphrase><![CDATA[e_j]]></mathphrase></inlineequation>, then <inlineequation><alt><![CDATA[e_i]]></alt><mathphrase><![CDATA[e_i]]></mathphrase></inlineequation>
must happen before <inlineequation><alt><![CDATA[e_j]]></alt><mathphrase><![CDATA[e_j]]></mathphrase></inlineequation> – Logical Time</simpara>
</blockquote>
<simpara>道理是显然的：如果事件<inlineequation><alt><![CDATA[e_i]]></alt><mathphrase><![CDATA[e_i]]></mathphrase></inlineequation>导致了事件<inlineequation><alt><![CDATA[e_j]]></alt><mathphrase><![CDATA[e_j]]></mathphrase></inlineequation>，那么一定<inlineequation><alt><![CDATA[e_i]]></alt><mathphrase><![CDATA[e_i]]></mathphrase></inlineequation>发生在<inlineequation><alt><![CDATA[e_j]]></alt><mathphrase><![CDATA[e_j]]></mathphrase></inlineequation>之前。</simpara>
<simpara>我们现在给「happened
before」这个关系一个记号：<inlineequation><alt><![CDATA[\rightarrow]]></alt><mathphrase><![CDATA[\rightarrow]]></mathphrase></inlineequation>，事件<inlineequation><alt><![CDATA[a]]></alt><mathphrase><![CDATA[a]]></mathphrase></inlineequation>在事件<inlineequation><alt><![CDATA[b]]></alt><mathphrase><![CDATA[b]]></mathphrase></inlineequation>之前发生则表示为<inlineequation><alt><![CDATA[a \rightarrow b]]></alt><mathphrase><![CDATA[a \rightarrow b]]></mathphrase></inlineequation>，那么我们有：</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>如果<inlineequation><alt><![CDATA[a]]></alt><mathphrase><![CDATA[a]]></mathphrase></inlineequation>和<inlineequation><alt><![CDATA[b]]></alt><mathphrase><![CDATA[b]]></mathphrase></inlineequation>是同一个进程内的事件，并且<inlineequation><alt><![CDATA[a]]></alt><mathphrase><![CDATA[a]]></mathphrase></inlineequation>在<inlineequation><alt><![CDATA[b]]></alt><mathphrase><![CDATA[b]]></mathphrase></inlineequation>之前发生，则<inlineequation><alt><![CDATA[a \rightarrow b]]></alt><mathphrase><![CDATA[a \rightarrow b]]></mathphrase></inlineequation>。</simpara>
</listitem>
<listitem>
<simpara>如果事件<inlineequation><alt><![CDATA[a]]></alt><mathphrase><![CDATA[a]]></mathphrase></inlineequation>是「发送了一个消息」，而事件<inlineequation><alt><![CDATA[b]]></alt><mathphrase><![CDATA[b]]></mathphrase></inlineequation>是接收了这个消息，则<inlineequation><alt><![CDATA[a \rightarrow b]]></alt><mathphrase><![CDATA[a \rightarrow b]]></mathphrase></inlineequation>。</simpara>
</listitem>
<listitem>
<simpara>如果<inlineequation><alt><![CDATA[a \rightarrow b]]></alt><mathphrase><![CDATA[a \rightarrow b]]></mathphrase></inlineequation>并且<inlineequation><alt><![CDATA[b \rightarrow c]]></alt><mathphrase><![CDATA[b \rightarrow c]]></mathphrase></inlineequation>，那么<inlineequation><alt><![CDATA[a \rightarrow c]]></alt><mathphrase><![CDATA[a \rightarrow c]]></mathphrase></inlineequation>（即传递性）。</simpara>
</listitem>
</orderedlist>
<simpara>那么，是否存在两个事件并无顺序关系吗？经过前面的讨论，答案当然是肯定的。</simpara>
<simpara>如果两个事件无法推导出顺序关系的话，我们称两个事件是并发的，记作<inlineequation><alt><![CDATA[a \Vert b]]></alt><mathphrase><![CDATA[a \Vert b]]></mathphrase></inlineequation>。</simpara>
<simpara>这样，我们可以这样描述上面的图中存在的事件顺序：</simpara>
<itemizedlist>
<listitem>
<simpara><inlineequation><alt><![CDATA[a \rightarrow b \rightarrow c \rightarrow d]]></alt><mathphrase><![CDATA[a \rightarrow b \rightarrow c \rightarrow d]]></mathphrase></inlineequation></simpara>
</listitem>
<listitem>
<simpara><inlineequation><alt><![CDATA[e \rightarrow c \rightarrow d]]></alt><mathphrase><![CDATA[e \rightarrow c \rightarrow d]]></mathphrase></inlineequation></simpara>
</listitem>
<listitem>
<simpara><inlineequation><alt><![CDATA[a \Vert e]]></alt><mathphrase><![CDATA[a \Vert e]]></mathphrase></inlineequation></simpara>
</listitem>
<listitem>
<simpara><inlineequation><alt><![CDATA[b \Vert e]]></alt><mathphrase><![CDATA[b \Vert e]]></mathphrase></inlineequation></simpara>
</listitem>
</itemizedlist>
<simpara>可以看出，「happened before」<inlineequation><alt><![CDATA[\rightarrow]]></alt><mathphrase><![CDATA[\rightarrow]]></mathphrase></inlineequation>是一个偏序关系。</simpara>
<simpara><emphasis role="strong">Lamport的时钟算法</emphasis>：</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>每个进程<inlineequation><alt><![CDATA[P_i]]></alt><mathphrase><![CDATA[P_i]]></mathphrase></inlineequation>内维护本地一个计数器<inlineequation><alt><![CDATA[C_i]]></alt><mathphrase><![CDATA[C_i]]></mathphrase></inlineequation>，初始为<inlineequation><alt><![CDATA[0]]></alt><mathphrase><![CDATA[0]]></mathphrase></inlineequation>.</simpara>
</listitem>
<listitem>
<simpara>每次执行一个事件，计数器<inlineequation><alt><![CDATA[C_i]]></alt><mathphrase><![CDATA[C_i]]></mathphrase></inlineequation>自增（假设自增量为<inlineequation><alt><![CDATA[1]]></alt><mathphrase><![CDATA[1]]></mathphrase></inlineequation>）.</simpara>
</listitem>
<listitem>
<simpara>进程<inlineequation><alt><![CDATA[P_i]]></alt><mathphrase><![CDATA[P_i]]></mathphrase></inlineequation>发消息给进程<inlineequation><alt><![CDATA[P_j]]></alt><mathphrase><![CDATA[P_j]]></mathphrase></inlineequation>时，需要在消息上附带自己的计数器<inlineequation><alt><![CDATA[C_i]]></alt><mathphrase><![CDATA[C_i]]></mathphrase></inlineequation>.</simpara>
</listitem>
<listitem>
<simpara>当进程<inlineequation><alt><![CDATA[P_j]]></alt><mathphrase><![CDATA[P_j]]></mathphrase></inlineequation>接收到消息时，更新自己的计数器<inlineequation><alt><![CDATA[C_j = \max (C_i,C_j) + 1]]></alt><mathphrase><![CDATA[C_j = \max (C_i,C_j) + 1]]></mathphrase></inlineequation>.</simpara>
</listitem>
</orderedlist>
<simpara>下面的图是这个算法的示意图，可以推算最后的时钟计数器的值：<inlineequation><alt><![CDATA[C_i = 3, C_j = 5]]></alt><mathphrase><![CDATA[C_i = 3, C_j = 5]]></mathphrase></inlineequation></simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/appendix-a-10.jpeg"/>
</imageobject>
<textobject><phrase>Lamport时钟算法</phrase></textobject>
</mediaobject>
</informalfigure>
<simpara>下面，将证明：如果<inlineequation><alt><![CDATA[a \rightarrow b]]></alt><mathphrase><![CDATA[a \rightarrow b]]></mathphrase></inlineequation>，那么一定有<inlineequation><alt><![CDATA[C_a < C_b]]></alt><mathphrase><![CDATA[C_a < C_b]]></mathphrase></inlineequation>。</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>假设<inlineequation><alt><![CDATA[a]]></alt><mathphrase><![CDATA[a]]></mathphrase></inlineequation>和<inlineequation><alt><![CDATA[b]]></alt><mathphrase><![CDATA[b]]></mathphrase></inlineequation>发生在同一个进程内，显然<inlineequation><alt><![CDATA[C_a < C_b]]></alt><mathphrase><![CDATA[C_a < C_b]]></mathphrase></inlineequation>.</simpara>
</listitem>
<listitem>
<simpara>假设<inlineequation><alt><![CDATA[a]]></alt><mathphrase><![CDATA[a]]></mathphrase></inlineequation>和<inlineequation><alt><![CDATA[b]]></alt><mathphrase><![CDATA[b]]></mathphrase></inlineequation>分别处在不同的进程内，如<inlineequation><alt><![CDATA[P_a]]></alt><mathphrase><![CDATA[P_a]]></mathphrase></inlineequation>和<inlineequation><alt><![CDATA[P_b]]></alt><mathphrase><![CDATA[P_b]]></mathphrase></inlineequation>，根据事件先后的定义，必然存在一个不早于<inlineequation><alt><![CDATA[a]]></alt><mathphrase><![CDATA[a]]></mathphrase></inlineequation>且不晚于<inlineequation><alt><![CDATA[b]]></alt><mathphrase><![CDATA[b]]></mathphrase></inlineequation>的由<inlineequation><alt><![CDATA[P_a]]></alt><mathphrase><![CDATA[P_a]]></mathphrase></inlineequation>到<inlineequation><alt><![CDATA[P_b]]></alt><mathphrase><![CDATA[P_b]]></mathphrase></inlineequation>的通信（否则<inlineequation><alt><![CDATA[a \Vert b]]></alt><mathphrase><![CDATA[a \Vert b]]></mathphrase></inlineequation>，矛盾）。那么假设两个进程在<inlineequation><alt><![CDATA[a]]></alt><mathphrase><![CDATA[a]]></mathphrase></inlineequation>和<inlineequation><alt><![CDATA[b]]></alt><mathphrase><![CDATA[b]]></mathphrase></inlineequation>之间最近一次通信是由<inlineequation><alt><![CDATA[P_a]]></alt><mathphrase><![CDATA[P_a]]></mathphrase></inlineequation>向<inlineequation><alt><![CDATA[P_b]]></alt><mathphrase><![CDATA[P_b]]></mathphrase></inlineequation>发送了消息<inlineequation><alt><![CDATA[a \rightarrow b]]></alt><mathphrase><![CDATA[a \rightarrow b]]></mathphrase></inlineequation>：易得<inlineequation><alt><![CDATA[a \rightarrow c \rightarrow d \rightarrow b]]></alt><mathphrase><![CDATA[a \rightarrow c \rightarrow d \rightarrow b]]></mathphrase></inlineequation>（其中可能<inlineequation><alt><![CDATA[a = c]]></alt><mathphrase><![CDATA[a = c]]></mathphrase></inlineequation>或者<inlineequation><alt><![CDATA[d = b]]></alt><mathphrase><![CDATA[d = b]]></mathphrase></inlineequation>）。根据算法定义，得：</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara><inlineequation><alt><![CDATA[C_a \leq C_c]]></alt><mathphrase><![CDATA[C_a \leq C_c]]></mathphrase></inlineequation>（进程内计数器自增）.</simpara>
</listitem>
<listitem>
<simpara><inlineequation><alt><![CDATA[C_d \leq C_b]]></alt><mathphrase><![CDATA[C_d \leq C_b]]></mathphrase></inlineequation>（进程内计数器自增）.</simpara>
</listitem>
<listitem>
<simpara><inlineequation><alt><![CDATA[C_c < C_d]]></alt><mathphrase><![CDATA[C_c < C_d]]></mathphrase></inlineequation>（进程间通信，观察者事件已经严格大于发生者事件的计数器）.</simpara>
<simpara>那么，最终推导出<inlineequation><alt><![CDATA[C_a < C_b]]></alt><mathphrase><![CDATA[C_a < C_b]]></mathphrase></inlineequation>（严格小于）。</simpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/appendix-a-11.jpeg"/>
</imageobject>
<textobject><phrase>事件latexmath:[$a$]和latexmath:[$b$]在不同进程的情况下，中间一定有消息传递，否则两个事件并发</phrase></textobject>
</mediaobject>
</informalfigure>
<simpara>以上，得证<inlineequation><alt><![CDATA[a \rightarrow b \Rightarrow C_a < C_b]]></alt><mathphrase><![CDATA[a \rightarrow b \Rightarrow C_a < C_b]]></mathphrase></inlineequation>。</simpara>
<simpara>但是，如果我们已知<inlineequation><alt><![CDATA[C_a < C_b]]></alt><mathphrase><![CDATA[C_a < C_b]]></mathphrase></inlineequation>的话，是否可以推导出<inlineequation><alt><![CDATA[a \rightarrow b]]></alt><mathphrase><![CDATA[a \rightarrow b]]></mathphrase></inlineequation>呢？</simpara>
<simpara>悲哀的是，不能。下面的图是个反例：</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/appendix-a-12.jpeg"/>
</imageobject>
<textobject><phrase>Lamport时钟无法反向推导事件顺序的反例：红色latexmath:[$a$]和蓝色latexmath:[$b$]是并发的</phrase></textobject>
</mediaobject>
</informalfigure>
<simpara>这样，我们反证了<inlineequation><alt><![CDATA[C_a < C_b \nRightarrow a \rightarrow b]]></alt><mathphrase><![CDATA[C_a < C_b \nRightarrow a \rightarrow b]]></mathphrase></inlineequation>。</simpara>
<simpara>我们无法推导出<inlineequation><alt><![CDATA[C_a < C_b \Rightarrow a \rightarrow b]]></alt><mathphrase><![CDATA[C_a < C_b \Rightarrow a \rightarrow b]]></mathphrase></inlineequation>的原因，在于<inlineequation><alt><![CDATA[a]]></alt><mathphrase><![CDATA[a]]></mathphrase></inlineequation>可能和<inlineequation><alt><![CDATA[b]]></alt><mathphrase><![CDATA[b]]></mathphrase></inlineequation>并发。</simpara>
<simpara>但是，
如果<inlineequation><alt><![CDATA[C_a < C_b]]></alt><mathphrase><![CDATA[C_a < C_b]]></mathphrase></inlineequation>，一定不会有<inlineequation><alt><![CDATA[b \rightarrow a]]></alt><mathphrase><![CDATA[b \rightarrow a]]></mathphrase></inlineequation>的关系存在。</simpara>
<simpara>Lamport的逻辑时钟算法构建了一个全序(total ordering)时钟来描述事件顺序，
但是我们知道「happened before」是一个偏序关系，
用全序关系的一维计数器来描述「happened before」的话，
就会导致无法等价化描述的结果，Lamport时钟的缺陷在于：<emphasis role="strong">如果两个事件并不相关，那么这个时钟给出的大小关系则没有意义，这个缺陷其实恰好就是全序和偏序的不同点而已</emphasis>。</simpara>
<simpara>所以，要准确描述事件顺序，我们终究要寻求偏序方法。</simpara>
<simpara>于是，我们继续探讨向量时钟。</simpara>
</section>
<section xml:id="_向量时钟_vector_clocks">
<title>向量时钟 (Vector Clocks)</title>
<simpara>向量时钟，其实是对Lamport的时钟的一个延伸思考，<emphasis role="strong">算法结构一致，只是多传了一部分信息</emphasis>。</simpara>
<simpara>对每个进程，定义一个向量<inlineequation><alt><![CDATA[VC]]></alt><mathphrase><![CDATA[VC]]></mathphrase></inlineequation>，向量的长度是<inlineequation><alt><![CDATA[n]]></alt><mathphrase><![CDATA[n]]></mathphrase></inlineequation>，<inlineequation><alt><![CDATA[n]]></alt><mathphrase><![CDATA[n]]></mathphrase></inlineequation>是进程数目。</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>初始化各个进程<inlineequation><alt><![CDATA[P_i]]></alt><mathphrase><![CDATA[P_i]]></mathphrase></inlineequation>的向量，全部抹零：<inlineequation><alt><![CDATA[VC_i = [0,...,0]]></alt><mathphrase><![CDATA[VC_i = [0,...,0]]></mathphrase></inlineequation>]。</simpara>
</listitem>
<listitem>
<simpara>进程<inlineequation><alt><![CDATA[P_i]]></alt><mathphrase><![CDATA[P_i]]></mathphrase></inlineequation>每发生一个事件时，其向量的第<inlineequation><alt><![CDATA[i]]></alt><mathphrase><![CDATA[i]]></mathphrase></inlineequation>个元素自增：<inlineequation><alt><![CDATA[VC_i [i] \mathrel{+}= 1]]></alt><mathphrase><![CDATA[VC_i [i] \mathrel{+}= 1]]></mathphrase></inlineequation>。</simpara>
</listitem>
<listitem>
<simpara>当进程<inlineequation><alt><![CDATA[P_i]]></alt><mathphrase><![CDATA[P_i]]></mathphrase></inlineequation>发消息给进程<inlineequation><alt><![CDATA[P_j]]></alt><mathphrase><![CDATA[P_j]]></mathphrase></inlineequation>时，需要在消息上附带自己的向量<inlineequation><alt><![CDATA[VC_i]]></alt><mathphrase><![CDATA[VC_i]]></mathphrase></inlineequation>。</simpara>
</listitem>
<listitem>
<simpara>当进程<inlineequation><alt><![CDATA[P_j]]></alt><mathphrase><![CDATA[P_j]]></mathphrase></inlineequation>接收到消息时，对齐对方的时钟，并在自己的时钟上自增：对<inlineequation><alt><![CDATA[[0,n)]]></alt><mathphrase><![CDATA[[0,n)]]></mathphrase></inlineequation>上的任意一个整数<inlineequation><alt><![CDATA[k]]></alt><mathphrase><![CDATA[k]]></mathphrase></inlineequation>执行<inlineequation><alt><![CDATA[VC_j[k] = \max (VC_j[k],VC_i[k])]]></alt><mathphrase><![CDATA[VC_j[k] = \max (VC_j[k],VC_i[k])]]></mathphrase></inlineequation>，接着，对应第2点：<inlineequation><alt><![CDATA[VC_j[j] \mathrel{+}= 1]]></alt><mathphrase><![CDATA[VC_j[j] \mathrel{+}= 1]]></mathphrase></inlineequation>。</simpara>
</listitem>
</orderedlist>
<simpara>下面图是一个向量时钟算法的示意图：</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>和Lamport时钟算法示意图一样：（红色点、蓝色点、黑灰色点…）</simpara>
</listitem>
<listitem>
<simpara>图的右方部分，总结了这个算法对不同事件的操作：</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>对于红色和蓝色，也就是进程内自发性事件和发送消息的事件，向量内相应的计数器自增。</simpara>
</listitem>
<listitem>
<simpara>发送消息的时候，需要传播出去自己的整个向量（也就是广播自己对整个系统的视角）。</simpara>
</listitem>
<listitem>
<simpara>接收到消息的时候，也就是蓝色事件，需要对齐对方的向量，并应用第一条规则，即自己的向量内相应计数器自增。</simpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/appendix-a-13.jpeg"/>
</imageobject>
<textobject><phrase>向量时钟示意图</phrase></textobject>
</mediaobject>
</informalfigure>
<simpara>可以发现，向量时钟和Lamport的时钟算法结构一样，
不同的点在于：Lamport时钟只是在对齐时钟的计数器，而<emphasis role="strong">向量时钟是在对齐各自对整个系统的视角</emphasis>。</simpara>
<simpara>我们可以推导出来关于向量时钟比较大小的几个性质：</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara><emphasis role="strong">向量的各维相等，则向量相等</emphasis>。这个是显而易见的。</simpara>
</listitem>
<listitem>
<simpara>向量时钟是有序的
(充要)，即：<inlineequation><alt><![CDATA[VC_i]]></alt><mathphrase><![CDATA[VC_i]]></mathphrase></inlineequation>的各维上的值不大于<inlineequation><alt><![CDATA[VC_j]]></alt><mathphrase><![CDATA[VC_j]]></mathphrase></inlineequation>对应维上的值，则认为<inlineequation><alt><![CDATA[VC_i]]></alt><mathphrase><![CDATA[VC_i]]></mathphrase></inlineequation>不大于<inlineequation><alt><![CDATA[VC_j]]></alt><mathphrase><![CDATA[VC_j]]></mathphrase></inlineequation>。</simpara>
</listitem>
<listitem>
<simpara>向量时钟有序性质的进一步细化，定义了严格小于：如果<inlineequation><alt><![CDATA[VC_i]]></alt><mathphrase><![CDATA[VC_i]]></mathphrase></inlineequation>不大于<inlineequation><alt><![CDATA[VC_j]]></alt><mathphrase><![CDATA[VC_j]]></mathphrase></inlineequation>，并且至少存在一个维，在这个维上<inlineequation><alt><![CDATA[VC_i]]></alt><mathphrase><![CDATA[VC_i]]></mathphrase></inlineequation>的值严格小于<inlineequation><alt><![CDATA[VC_j]]></alt><mathphrase><![CDATA[VC_j]]></mathphrase></inlineequation>在这个维上的值，则认为<inlineequation><alt><![CDATA[VC_i]]></alt><mathphrase><![CDATA[VC_i]]></mathphrase></inlineequation>小于<inlineequation><alt><![CDATA[VC_j]]></alt><mathphrase><![CDATA[VC_j]]></mathphrase></inlineequation>。</simpara>
</listitem>
<listitem>
<simpara>如果两个向量不存在大小关系，则认为两个向量平行，记作<inlineequation><alt><![CDATA[VC_i \Vert VC_j]]></alt><mathphrase><![CDATA[VC_i \Vert VC_j]]></mathphrase></inlineequation>.</simpara>
</listitem>
</orderedlist>
<simpara>这几个性质看起来复杂，其实都是在定义向量时钟之间的大小关系。
我们可以得出一个结论：</simpara>
<simpara><emphasis role="strong">向量时钟之间的大小关系是一种偏序关系</emphasis>。</simpara>
<simpara>和Lamport时钟一样，我们可以利用类似的推导方式，证明<inlineequation><alt><![CDATA[a \rightarrow b \Rightarrow VC_a < VC_b]]></alt><mathphrase><![CDATA[a \rightarrow b \Rightarrow VC_a < VC_b]]></mathphrase></inlineequation>。这里不再描述证明思路。</simpara>
<simpara>我们接下来要花一定篇幅论证下<inlineequation><alt><![CDATA[VC_a < VC_b \Rightarrow a \rightarrow b]]></alt><mathphrase><![CDATA[VC_a < VC_b \Rightarrow a \rightarrow b]]></mathphrase></inlineequation>。</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>如果<inlineequation><alt><![CDATA[a]]></alt><mathphrase><![CDATA[a]]></mathphrase></inlineequation>和<inlineequation><alt><![CDATA[b]]></alt><mathphrase><![CDATA[b]]></mathphrase></inlineequation>两个事件处在同一个进程中，显而易见<inlineequation><alt><![CDATA[a \rightarrow b]]></alt><mathphrase><![CDATA[a \rightarrow b]]></mathphrase></inlineequation>。</simpara>
</listitem>
<listitem>
<simpara>假设<inlineequation><alt><![CDATA[a]]></alt><mathphrase><![CDATA[a]]></mathphrase></inlineequation>和<inlineequation><alt><![CDATA[b]]></alt><mathphrase><![CDATA[b]]></mathphrase></inlineequation>分别处在不同的进程内，如<inlineequation><alt><![CDATA[P_a]]></alt><mathphrase><![CDATA[P_a]]></mathphrase></inlineequation>和<inlineequation><alt><![CDATA[P_b]]></alt><mathphrase><![CDATA[P_b]]></mathphrase></inlineequation>，设<inlineequation><alt><![CDATA[VC_a = [m,n], VC_b = [s,t]]]></alt><mathphrase><![CDATA[VC_a = [m,n], VC_b = [s,t]]]></mathphrase></inlineequation>。因为<inlineequation><alt><![CDATA[VC_a < VC_b]]></alt><mathphrase><![CDATA[VC_a < VC_b]]></mathphrase></inlineequation>，所以<inlineequation><alt><![CDATA[m≤s]]></alt><mathphrase><![CDATA[m≤s]]></mathphrase></inlineequation>，所以必然在不早于<inlineequation><alt><![CDATA[a]]></alt><mathphrase><![CDATA[a]]></mathphrase></inlineequation>之前和不晚于<inlineequation><alt><![CDATA[b]]></alt><mathphrase><![CDATA[b]]></mathphrase></inlineequation>之后的时间内，<inlineequation><alt><![CDATA[P_a]]></alt><mathphrase><![CDATA[P_a]]></mathphrase></inlineequation>向<inlineequation><alt><![CDATA[P_b]]></alt><mathphrase><![CDATA[P_b]]></mathphrase></inlineequation>发送了消息（否则<inlineequation><alt><![CDATA[P_b]]></alt><mathphrase><![CDATA[P_b]]></mathphrase></inlineequation>对<inlineequation><alt><![CDATA[P_a]]></alt><mathphrase><![CDATA[P_a]]></mathphrase></inlineequation>的计数器得不到及时刷新，<inlineequation><alt><![CDATA[s]]></alt><mathphrase><![CDATA[s]]></mathphrase></inlineequation>就不会不小于<inlineequation><alt><![CDATA[m]]></alt><mathphrase><![CDATA[m]]></mathphrase></inlineequation>）。</simpara>
</listitem>
</orderedlist>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/appendix-a-14.jpeg"/>
</imageobject>
<textobject><phrase>中间必然存在latexmath:[$P_a$]向latexmath:[$P_b$]发送了消息</phrase></textobject>
</mediaobject>
</informalfigure>
<simpara>实际上，可以分为以下几种情况：</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/appendix-a-15.jpeg"/>
</imageobject>
<textobject><phrase>可能出现的4种情况</phrase></textobject>
</mediaobject>
</informalfigure>
<orderedlist numeration="arabic">
<listitem>
<simpara>当<inlineequation><alt><![CDATA[a = c]]></alt><mathphrase><![CDATA[a = c]]></mathphrase></inlineequation>且<inlineequation><alt><![CDATA[d = b]]></alt><mathphrase><![CDATA[d = b]]></mathphrase></inlineequation>,
易得<inlineequation><alt><![CDATA[a \rightarrow b]]></alt><mathphrase><![CDATA[a \rightarrow b]]></mathphrase></inlineequation>.</simpara>
</listitem>
<listitem>
<simpara>当<inlineequation><alt><![CDATA[a = c]]></alt><mathphrase><![CDATA[a = c]]></mathphrase></inlineequation>且<inlineequation><alt><![CDATA[d \rightarrow b]]></alt><mathphrase><![CDATA[d \rightarrow b]]></mathphrase></inlineequation>，由传递性，得<inlineequation><alt><![CDATA[a \rightarrow b]]></alt><mathphrase><![CDATA[a \rightarrow b]]></mathphrase></inlineequation>.</simpara>
</listitem>
<listitem>
<simpara>同样对于<inlineequation><alt><![CDATA[d = b]]></alt><mathphrase><![CDATA[d = b]]></mathphrase></inlineequation>且<inlineequation><alt><![CDATA[a \rightarrow c]]></alt><mathphrase><![CDATA[a \rightarrow c]]></mathphrase></inlineequation>的情况.</simpara>
</listitem>
<listitem>
<simpara>当<inlineequation><alt><![CDATA[a \rightarrow c]]></alt><mathphrase><![CDATA[a \rightarrow c]]></mathphrase></inlineequation>且<inlineequation><alt><![CDATA[d \rightarrow b]]></alt><mathphrase><![CDATA[d \rightarrow b]]></mathphrase></inlineequation>，根据进程内的算法逻辑和传递性，也很容易得出结论。</simpara>
</listitem>
</orderedlist>
<simpara>综上：<inlineequation><alt><![CDATA[VC_a < VC_b \Rightarrow a \rightarrow b]]></alt><mathphrase><![CDATA[VC_a < VC_b \Rightarrow a \rightarrow b]]></mathphrase></inlineequation>得证。</simpara>
<simpara>进一步的，我们可以得出这样的结论：</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>向量有序，则事件有序（充要）：<inlineequation><alt><![CDATA[VC_a < VC_b \leftrightarrow a \rightarrow b]]></alt><mathphrase><![CDATA[VC_a < VC_b \leftrightarrow a \rightarrow b]]></mathphrase></inlineequation></simpara>
</listitem>
<listitem>
<simpara>向量平行，则事件并发（充要）：<inlineequation><alt><![CDATA[VC_a \Vert VC_b \leftrightarrow a \Vert b]]></alt><mathphrase><![CDATA[VC_a \Vert VC_b \leftrightarrow a \Vert b]]></mathphrase></inlineequation></simpara>
</listitem>
</orderedlist>
<simpara>是的，向量时钟可以准确刻画事件顺序。</simpara>
<simpara>其本质在于将Lamport时钟的全序计数器的方式改造成向量时钟的偏序大小关系。</simpara>
</section>
<section xml:id="_向量时钟看前面的问题">
<title>向量时钟看前面的问题</title>
<simpara>现在我们回到文中一开始提到的问题。</simpara>
<simpara>由于网络延迟的不确定性，
我们在文章开始提出了一个<inlineequation><alt><![CDATA[A,B,C]]></alt><mathphrase><![CDATA[A,B,C]]></mathphrase></inlineequation>三个进程中如何确定事件顺序的问题。</simpara>
<simpara>我们可以从下面的图中看出，<inlineequation><alt><![CDATA[VC_a < VC_b < VC_c]]></alt><mathphrase><![CDATA[VC_a < VC_b < VC_c]]></mathphrase></inlineequation>，那么可以确定<inlineequation><alt><![CDATA[a \rightarrow b \rightarrow c]]></alt><mathphrase><![CDATA[a \rightarrow b \rightarrow c]]></mathphrase></inlineequation>。也就是我们找到了一种方法来描述这种情况下的事件顺序。进程<inlineequation><alt><![CDATA[C]]></alt><mathphrase><![CDATA[C]]></mathphrase></inlineequation>的视角下观察<inlineequation><alt><![CDATA[a]]></alt><mathphrase><![CDATA[a]]></mathphrase></inlineequation>和<inlineequation><alt><![CDATA[b]]></alt><mathphrase><![CDATA[b]]></mathphrase></inlineequation>的顺序的问题也有了明确的答案。</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/appendix-a-16.jpeg"/>
</imageobject>
<textobject><phrase>用向量时钟的方法解答文章开始提出的问题</phrase></textobject>
</mediaobject>
</informalfigure>
<simpara>最后，我们回到朋友圈的例子。下面图中可以看出，<inlineequation><alt><![CDATA[VC_a < VC_b < VC_c < VC_d < VC_e]]></alt><mathphrase><![CDATA[VC_a < VC_b < VC_c < VC_d < VC_e]]></mathphrase></inlineequation>，显然可以确定<inlineequation><alt><![CDATA[a \rightarrow b \rightarrow c \rightarrow d \rightarrow e]]></alt><mathphrase><![CDATA[a \rightarrow b \rightarrow c \rightarrow d \rightarrow e]]></mathphrase></inlineequation>。</simpara>
<simpara>在小李看到小明的朋友圈和小红的评论的时候，也收到了这两条数据的向量，
我们可以根据向量时钟来确定事件的先后关系，从而不会显示出因果矛盾。</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/appendix-a-17.jpeg"/>
</imageobject>
<textobject><phrase>用向量时钟的方法看朋友圈问题</phrase></textobject>
</mediaobject>
</informalfigure>
<simpara>上图中还有一个事件<inlineequation><alt><![CDATA[f]]></alt><mathphrase><![CDATA[f]]></mathphrase></inlineequation>，它的发生可能是小红又发了一条评论。我们可以看到<inlineequation><alt><![CDATA[VC_f \Vert VC_e]]></alt><mathphrase><![CDATA[VC_f \Vert VC_e]]></mathphrase></inlineequation>。这时候，无法确定事件<inlineequation><alt><![CDATA[f]]></alt><mathphrase><![CDATA[f]]></mathphrase></inlineequation>和<inlineequation><alt><![CDATA[e]]></alt><mathphrase><![CDATA[e]]></mathphrase></inlineequation>的先后关系，也就是说<inlineequation><alt><![CDATA[f \Vert e]]></alt><mathphrase><![CDATA[f \Vert e]]></mathphrase></inlineequation>。</simpara>
<simpara>但是，这时候小李还没有看见这个事情。所谓「因果」：有因才有果。看见也是一个事件的果。
这样说，<inlineequation><alt><![CDATA[f]]></alt><mathphrase><![CDATA[f]]></mathphrase></inlineequation>和<inlineequation><alt><![CDATA[e]]></alt><mathphrase><![CDATA[e]]></mathphrase></inlineequation>没有因果关系，因为小李还没有看见这个事件。所以讨论也就没有意义。</simpara>
</section>
<section xml:id="_向量时钟的不足">
<title>向量时钟的不足</title>
<orderedlist numeration="arabic">
<listitem>
<simpara>只考虑了固定数量的节点，没有考虑节点的动态添加和销毁。</simpara>
</listitem>
<listitem>
<simpara>假设节点数量是<inlineequation><alt><![CDATA[N]]></alt><mathphrase><![CDATA[N]]></mathphrase></inlineequation>，那么每个节点需要维护的空间复杂度是<inlineequation><alt><![CDATA[O(N)]]></alt><mathphrase><![CDATA[O(N)]]></mathphrase></inlineequation>。通信的信息量的复杂度也是<inlineequation><alt><![CDATA[O(N)]]></alt><mathphrase><![CDATA[O(N)]]></mathphrase></inlineequation>。</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="_总结与感想">
<title>总结与感想</title>
<itemizedlist>
<listitem>
<simpara>现实中，无法构建精确的全局时钟来描述事件顺序。</simpara>
</listitem>
<listitem>
<simpara>受狭义相对论的启发，我们用因果关系来描述事件顺序。</simpara>
</listitem>
<listitem>
<simpara>因果关系是一种偏序关系。</simpara>
</listitem>
<listitem>
<simpara>Lamport时钟构造的计数器之间的大小关系是一种全序关系，无法准确刻画事件顺序的偏序关系。</simpara>
</listitem>
<listitem>
<simpara>向量时钟是一种对Lamport时钟的延伸，以偏序关系准确刻画了事件的因果顺序。</simpara>
</listitem>
</itemizedlist>
<simpara>此外，向量时钟给我一种感想，对每个分布式节点来说：</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>我把我的视角分享给其他节点。</simpara>
</listitem>
<listitem>
<simpara>我对齐我看到的其他节点的视角。</simpara>
</listitem>
</orderedlist>
<simpara>本质上，是在做<emphasis role="strong">视角对齐</emphasis>。</simpara>
<simpara>这自然地，让我想起了Gossip谣言传播算法。
Gossip算法如其名，每个分布式参与者都在散播自己视角的信息，以达到谣言扩散的效果。
同样是在做<emphasis role="strong">视角对齐</emphasis>。</simpara>
<simpara>对于分布式系统中的事件顺序的刻画，就讨论到这里。</simpara>
<literallayout class="monospaced">digraph G {
    node[shape=record,color=blue];
    TaskManager[label="&lt;f0&gt;TaskManager | &lt;f1&gt; TaskManager | &lt;f2&gt; TaskManager"];
    App -&gt; Dispatcher [label="提交应用"];
    Dispatcher -&gt; JobManager [label="启动并提交应用"];
    JobManager -&gt; ResourceManager [label="请求slots"];
    JobManager -&gt; TaskManager [label="提交要在slots中执行的任务"];
    "TaskManager" -&gt; JobManager [label="提供slots"];
    TaskManager -&gt; ResourceManager [label="注册slots"];
    ResourceManager -&gt; TaskManager [label="发出提供slots的命令"];
}</literallayout>
</section>
</section>
<section xml:id="_api列表">
<title>API列表</title>
<section xml:id="_word_count">
<title>WORD COUNT</title>
<literallayout class="monospaced">digraph G {
    aa -&gt; a -&gt; b -&gt; cc -&gt; dd -&gt; c -&gt; d -&gt; e;
    a [shape=rectangle,label="hello world"];
    aa [shape=rectangle,label="hello world"];
    b [shape=rectangle,label="flatMap"];
    c [shape=rectangle,label="(world,1)"];
    d [shape=rectangle,label="(hello,1)"];
    cc [shape=rectangle,label="(world,1)"];
    dd [shape=rectangle,label="(hello,1)"];
    e [shape=rectangle,label="keyBy"];
    e -&gt; ff -&gt; f;
    e -&gt; gg -&gt; g;
    f [shape=rectangle,label="(hello,1)"];
    g [shape=rectangle,label="(world,1)"];
    ff [shape=rectangle,label="(hello,1)"];
    gg [shape=rectangle,label="(world,1)"];
    f -&gt; h;
    g -&gt; i;
    h [shape=circle,label="reduce"];
    i [shape=circle,label="reduce"];
    h -&gt; jj;
    i -&gt; kk;
    h -&gt; h;
    i -&gt; i;
    j [shape=rectangle,label="(hello,1)"];
    k [shape=rectangle,label="(world,1)"];
    jj [shape=rectangle,label="(hello,2)"];
    kk [shape=rectangle,label="(world,2)"];
    jj -&gt; j -&gt; l;
    kk -&gt; k -&gt; m;
    l [shape=rectangle,label="print"];
    m [shape=rectangle,label="print"];
}</literallayout>
<programlisting language="java" linenumbering="unnumbered">public class WordCountFromBatch {
    // 不要忘记抛出异常
    public static void main(String[] args) throws Exception {
        // 获取执行环境上下文
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        // 设置并行任务的数量
//        env.setParallelism(1);

        // 数据源
        DataStreamSource&lt;String&gt; stream = env
                .fromElements("hello world", "hello world");

        // map操作：string =&gt; (string, 1)
        // flatMap的语义：将列表中的每一个元素转化成0个，1个或者多个元素
        SingleOutputStreamOperator&lt;WordWithCount&gt; mappedStream = stream
                // 匿名类
                // 第一个泛型是输入类型
                // 第二个泛型是输出类型
                .flatMap(new FlatMapFunction&lt;String, WordWithCount&gt;() {
                    @Override
                    public void flatMap(String value, Collector&lt;WordWithCount&gt; out) throws Exception {
                        // 使用空格进行切分
                        String[] arr = value.split(" ");
                        // 使用collect方法向下游发送数据
                        for (String e : arr) {
                            out.collect(new WordWithCount(e, 1L));
                        }
                    }
                });

        // shuffle操作
        KeyedStream&lt;WordWithCount, String&gt; keyedStream = mappedStream
                // 第一个泛型是流中数据的类型
                // 第二个泛型是key的类型
                .keyBy(new KeySelector&lt;WordWithCount, String&gt;() {
                    @Override
                    public String getKey(WordWithCount value) throws Exception {
                        return value.word;
                    }
                });

        // reduce操作
        // 累加器的类型和元素类型是一样的
        SingleOutputStreamOperator&lt;WordWithCount&gt; reducedStream = keyedStream
                .reduce(new ReduceFunction&lt;WordWithCount&gt;() {
                    @Override
                    public WordWithCount reduce(WordWithCount value1, WordWithCount value2) throws Exception {
                        return new WordWithCount(value1.word, value1.count + value2.count);
                    }
                });

        // 输出聚合的累加器结果
        reducedStream.print();

        // 不要忘记执行
        env.execute();
    }

    // POJO Class
    // 类必须是公有类
    // 所有字段必须是public
    // 必须有空构造器
    public static class WordWithCount {
        public String word;
        public Long count;

        public WordWithCount() {
        }

        public WordWithCount(String word, Long count) {
            this.word = word;
            this.count = count;
        }

        @Override
        public String toString() {
            return "WordWithCount{" +
                    "word='" + word + '\'' +
                    ", count=" + count +
                    '}';
        }
    }
}</programlisting>
</section>
<section xml:id="_pojo_class">
<title>POJO CLASS</title>
<programlisting language="java" linenumbering="unnumbered">public static class UserBehavior {
    public String userId;
    public String itemId;
    public String categoryId;
    public String behaviorType;
    public Long timestamp;

    public UserBehavior() {
    }

    public UserBehavior(String userId, String itemId, String categoryId, String behaviorType, Long timestamp) {
        this.userId = userId;
        this.itemId = itemId;
        this.categoryId = categoryId;
        this.behaviorType = behaviorType;
        this.timestamp = timestamp;
    }

    @Override
    public String toString() {
        return "UserBehavior{" +
                "userId='" + userId + '\'' +
                ", itemId='" + itemId + '\'' +
                ", categoryId='" + categoryId + '\'' +
                ", behaviorType='" + behaviorType + '\'' +
                ", timestamp=" + new Timestamp(timestamp) +
                '}';
    }
}</programlisting>
</section>
<section xml:id="_从集合读取">
<title>从集合读取</title>
<programlisting language="java" linenumbering="unnumbered">public static void main(String[] args) throws Exception {
    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
    env.setParallelism(1);

    env.fromElements(
        new UserBehavior("543462","1715","1464116","pv",1511658000 * 1000L),
        new UserBehavior("662867","2244074","1575622","pv",1511658000 * 1000L)
    );

    ArrayList&lt;UserBehavior&gt; userBehaviors = new ArrayList&lt;&gt;();
    userBehaviors.add(new UserBehavior("543462","1715","1464116","pv",1511658000 * 1000L));
    userBehaviors.add(new UserBehavior("662867","2244074","1575622","pv",1511658000 * 1000L));

    env.fromCollection(userBehaviors);

    env.execute();
}</programlisting>
</section>
<section xml:id="_从文件读取">
<title>从文件读取</title>
<programlisting language="java" linenumbering="unnumbered">env.readTextFile("UserBehavior.csv")</programlisting>
</section>
<section xml:id="_从socket读取">
<title>从socket读取</title>
<programlisting language="java" linenumbering="unnumbered">env.socketTextStream("localhost", 9999);</programlisting>
</section>
<section xml:id="_从kafka读取">
<title>从Kafka读取</title>
<simpara>添加驱动</simpara>
<programlisting language="xml" linenumbering="unnumbered">&lt;dependency&gt;
    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;
    &lt;artifactId&gt;flink-connector-kafka_${scala.binary.version}&lt;/artifactId&gt;
    &lt;version&gt;${flink.version}&lt;/version&gt;
&lt;/dependency&gt;</programlisting>
<simpara>代码</simpara>
<programlisting language="java" linenumbering="unnumbered">Properties properties = new Properties();
properties.setProperty("bootstrap.servers", "localhost:9092");
properties.setProperty("group.id", "consumer-group");
properties.setProperty("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
properties.setProperty("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
properties.setProperty("auto.offset.reset", "latest");

env
    .addSource(new FlinkKafkaConsumer&lt;String&gt;(
        "userbehavior",
        new SimpleStringSchema(),
        properties
    ));</programlisting>
</section>
<section xml:id="_自定义数据源">
<title>自定义数据源</title>
<simpara>数据源</simpara>
<programlisting language="java" linenumbering="unnumbered">public class UserBehaviorSource extends RichParallelSourceFunction&lt;UserBehavior&gt; {
    private Boolean running = true;
    @Override
    public void run(SourceContext&lt;UserBehavior&gt; sourceContext) throws Exception {
        String strCurrentLine;
        BufferedReader reader = new BufferedReader(new FileReader("UserBehavior.csv"));
        while (running &amp;&amp; (strCurrentLine = reader.readLine()) != null) {
            String[] arr = strCurrentLine.split(",");
            sourceContext.collect(new UserBehavior(arr[0], arr[1], arr[2], arr[3], Long.parseLong(arr[4]) * 1000L));
            Thread.sleep(100L);
        }
    }

    @Override
    public void cancel() {
        running = false;
    }
}</programlisting>
<simpara>数据源的使用方法</simpara>
<programlisting language="java" linenumbering="unnumbered">env.addSource(new UserBehaviorSource()).print();</programlisting>
</section>
<section xml:id="_map">
<title>MAP</title>
<programlisting language="java" linenumbering="unnumbered">readings.map(r -&gt; r.itemId);

readings.map(new MapFunction&lt;UserBehavior, String&gt;() {
    @Override
    public String map(UserBehavior r) throws Exception {
        return r.itemId;
    }
});

readings.map(new IdExtractor());

public static class IdExtractor implements MapFunction&lt;UserBehavior, String&gt; {
    @Override
    public String map(UserBehavior r) throws Exception {
        return r.itemId;
    }
}</programlisting>
</section>
<section xml:id="_filter">
<title>FILTER</title>
<programlisting language="java" linenumbering="unnumbered">readings.filter(r -&gt; r.behaviorType.equals("pv"));

readings.filter(new FilterFunction&lt;UserBehavior&gt;() {
    @Override
    public Boolean filter(UserBehavior r) throws Exception {
        return r.behaviorType.equals("pv");
    }
});

readings.filter(new PvExtractor());

public static class IdExtractor implements FilterFunction&lt;UserBehavior&gt; {
    @Override
    public Boolean filter(UserBehavior r) throws Exception {
        return r.behaviorType.equals("pv");
    }
}</programlisting>
</section>
<section xml:id="_flatmap">
<title>FLATMAP</title>
<programlisting language="java" linenumbering="unnumbered">DataStreamSource&lt;String&gt; stream = env.fromElements("white", "black", "gray");

stream
    .flatMap(
        (String s, Collector&lt;String&gt; out) -&gt; {
            if (s.equals("white")) {
                out.collect(s);
            } else if (s.equals("black")) {
                out.collect(s);
                out.collect(s);
            }
    })
    .returns(Types.STRING)
    .print();

stream.flatMap(new MyFlatMap()).print();

public static class MyFlatMap implements FlatMapFunction&lt;String, String&gt; {
    @Override
    public void flatMap(String value, Collector&lt;String&gt; out) throws Exception {
        if (value.equals("white")) {
            out.collect(value);
        } else if (value.equals("black")) {
            out.collect(value);
            out.collect(value);
        }
    }
}</programlisting>
</section>
<section xml:id="_keyby">
<title>KEYBY</title>
<programlisting language="java" linenumbering="unnumbered">KeyedStream&lt;UserBehavior, String&gt; keyed = stream.keyBy(r -&gt; r.itemId);

stream
    .keyBy(new KeySelector&lt;UserBehavior, String&gt;() {
        @Override
        public String getKey(UserBehavior value) throws Exception {
            return value.itemId;
        }
    });</programlisting>
</section>
<section xml:id="_sum_min_max_minby_maxby">
<title>SUM, MIN, MAX, MINBY, MAXBY</title>
<programlisting language="java" linenumbering="unnumbered">DataStreamSource&lt;Tuple2&lt;String, Integer&gt;&gt; stream = env.fromElements(
    Tuple2.of("a", 1),
    Tuple2.of("a", 3),
    Tuple2.of("b", 3),
    Tuple2.of("b", 4)
);

stream.keyBy(r -&gt; r.f0).sum(1).print();
stream.keyBy(r -&gt; r.f0).sum("f1").print();
stream.keyBy(r -&gt; r.f0).max(1).print();
stream.keyBy(r -&gt; r.f0).max("f1").print();
stream.keyBy(r -&gt; r.f0).min(1).print();
stream.keyBy(r -&gt; r.f0).min("f1").print();
stream.keyBy(r -&gt; r.f0).maxBy(1).print();
stream.keyBy(r -&gt; r.f0).maxBy("f1").print();
stream.keyBy(r -&gt; r.f0).minBy(1).print();
stream.keyBy(r -&gt; r.f0).minBy("f1").print();</programlisting>
</section>
<section xml:id="_reduce">
<title>REDUCE</title>
<programlisting language="java" linenumbering="unnumbered">env
    .addSource(new UserBehaviorSource())
    .map(new MapFunction&lt;UserBehavior, Tuple2&lt;String, Long&gt;&gt;() {
        @Override
        public Tuple2&lt;String, Long&gt; map(UserBehavior value) throws Exception {
            return Tuple2.of(value.itemId, 1L);
        }
    })
    .keyBy(r -&gt; r.f0)
    .reduce(new ReduceFunction&lt;Tuple2&lt;String, Long&gt;&gt;() {
        @Override
        public Tuple2&lt;String, Long&gt; reduce(Tuple2&lt;String, Long&gt; value1, Tuple2&lt;String, Long&gt; value2) throws Exception {
            return Tuple2.of(value1.f0, value1.f1 + value2.f1);
        }
    })
    .keyBy(r -&gt; true)
    .reduce(new ReduceFunction&lt;Tuple2&lt;String, Long&gt;&gt;() {
        @Override
        public Tuple2&lt;String, Long&gt; reduce(Tuple2&lt;String, Long&gt; value1, Tuple2&lt;String, Long&gt; value2) throws Exception {
            return value1.f1 &gt; value2.f1 ? value1 : value2;
        }
    })
    .print();</programlisting>
</section>
<section xml:id="_重分区">
<title>重分区</title>
<programlisting language="java" linenumbering="unnumbered">DataStreamSource&lt;Integer&gt; stream = env.fromElements(1, 2, 3, 4).setParallelism(1);

stream
    .shuffle()
    .print()
    .setParallelism(2);

stream
    .rebalance()
    .print()
    .setParallelism(2);

stream
    .rescale()
    .print()
    .setParallelism(2);

env
    .fromElements(1,2,3,4,5,6,7,8)
    .map(new MapFunction&lt;Integer, Tuple1&lt;Integer&gt;&gt;() {
        @Override
        public Tuple1&lt;Integer&gt; map(Integer value) throws Exception {
            return Tuple1.of(value);
        }
    })
    .partitionCustom(new Partitioner&lt;Integer&gt;() {
        @Override
        public int partition(Integer key, int numPartitions) {
            return key % 2;
        }
    }, new KeySelector&lt;Tuple1&lt;Integer&gt;, Integer&gt;() {
        @Override
        public Integer getKey(Tuple1&lt;Integer&gt; value) throws Exception {
            return value.f0;
        }
    })
    .map(new MapFunction&lt;Tuple1&lt;Integer&gt;, Tuple1&lt;Integer&gt;&gt;() {
        @Override
        public Tuple1&lt;Integer&gt; map(Tuple1&lt;Integer&gt; value) throws Exception {
            System.out.println("获取当前线程id" + Thread.currentThread().getId() + ",value" + value);
            return value;
        }
    })
    .print()
    .setParallelism(1);</programlisting>
</section>
<section xml:id="_类型系统">
<title>类型系统</title>
<programlisting language="java" linenumbering="unnumbered">public class Person {
    public String name;
    public int age;

    public Person() {}

    public Person(String name, int age) {
        this.name = name;
        this.age = age;
    }
}

DataStream&lt;Person&gt; persons = env.fromElements(
    new Person("Alex", 42),
    new Person("Wendy", 23)
);

TypeInformation&lt;Integer&gt; intType = Types.INT;

TypeInformation&lt;Tuple2&lt;Long, String&gt;&gt; tupleType = Types.TUPLE(Types.LONG, Types.STRING);

TypeInformation&lt;Person&gt; personType = Types.POJO(Person.class);</programlisting>
</section>
<section xml:id="_java_lambda_表达式">
<title>Java Lambda 表达式</title>
<simpara>Java 8 引入了几种新的语言特性，旨在实现更快、更清晰的编码。 作为最重要的特性，即所谓的“Lambda 表达式”，它开启了函数式编程的大门。Lambda 表达式允许以简捷的方式实现和传递函数，而无需声明额外的（匿名）类。</simpara>
<blockquote>
<simpara>Flink 支持对 Java API 的所有算子使用 Lambda 表达式，但是，当 Lambda 表达式使用 Java 泛型时，你需要 显式 声明类型信息。</simpara>
</blockquote>
<simpara>下例演示了如何实现一个简单的行内 map() 函数，它使用 Lambda 表达式计算输入的平方。不需要声明 map() 函数的输入 i 和输出参数的数据类型，因为 Java 编译器会对它们做出推断。</simpara>
<programlisting language="java" linenumbering="unnumbered">env
    .fromElements(1, 2, 3)
    .map(i -&gt; i*i)
    .print();</programlisting>
<simpara>由于 OUT 是 Integer 而不是泛型，Flink 可以由方法签名 OUT map(IN value) 的实现中自动提取出结果的类型信息。</simpara>
<simpara>不幸的是，flatMap() 这样的函数，它的签名 void flatMap(IN value, Collector&lt;OUT&gt; out) 被 Java 编译器编译为 void flatMap(IN value, Collector out)。这样 Flink 就无法自动推断输出的类型信息了。</simpara>
<simpara>Flink 很可能抛出类似如下的异常：</simpara>
<programlisting language="text" linenumbering="unnumbered">org.apache.flink.api.common.functions.InvalidTypesException: The generic type parameters of 'Collector' are missing.
In many cases lambda methods don't provide enough information for automatic type extraction when Java generics are involved.
An easy workaround is to use an (anonymous) class instead that implements the 'org.apache.flink.api.common.functions.FlatMapFunction' interface.
Otherwise the type has to be specified explicitly using type information.</programlisting>
<simpara>在这种情况下，需要 显式 指定类型信息，否则输出将被视为 Object 类型，这会导致低效的序列化。</simpara>
<programlisting language="java" linenumbering="unnumbered">import org.apache.flink.api.common.typeinfo.Types;
import org.apache.flink.api.java.DataSet;
import org.apache.flink.util.Collector;

DataSet&lt;Integer&gt; input = env.fromElements(1, 2, 3);

// 必须声明 collector 类型
input.flatMap((Integer number, Collector&lt;String&gt; out) -&gt; {
    StringBuilder builder = new StringBuilder();
    for(int i = 0; i &lt; number; i++) {
        builder.append("a");
        out.collect(builder.toString());
    }
})
// 显式提供类型信息
.returns(Types.STRING)
// 打印 "a", "a", "aa", "a", "aa", "aaa"
.print();</programlisting>
<simpara>当使用 map() 函数返回泛型类型的时候也会发生类似的问题。下例中的方法签名 Tuple2&lt;Integer, Integer&gt; map(Integer value) 被擦除为 Tuple2 map(Integer value)。</simpara>
<programlisting language="java" linenumbering="unnumbered">import org.apache.flink.api.common.functions.MapFunction;
import org.apache.flink.api.java.tuple.Tuple2;

env.fromElements(1, 2, 3)
.map(i -&gt; Tuple2.of(i, i))    // 没有关于 Tuple2 字段的信息
.print();</programlisting>
<simpara>一般来说，这些问题可以通过多种方式解决：</simpara>
<programlisting language="java" linenumbering="unnumbered">import org.apache.flink.api.common.typeinfo.Types;
import org.apache.flink.api.java.tuple.Tuple2;

// 使用显式的 ".returns(...)"
env.fromElements(1, 2, 3)
.map(i -&gt; Tuple2.of(i, i))
.returns(Types.TUPLE(Types.INT, Types.INT))
.print();

// 使用类来替代
env.fromElements(1, 2, 3)
.map(new MyTuple2Mapper())
.print();

public static class MyTuple2Mapper extends MapFunction&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt; {
    @Override
    public Tuple2&lt;Integer, Integer&gt; map(Integer i) {
        return Tuple2.of(i, i);
    }
}

// 使用匿名类来替代
env.fromElements(1, 2, 3)
.map(new MapFunction&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt; {
    @Override
    public Tuple2&lt;Integer, Integer&gt; map(Integer i) {
        return Tuple2.of(i, i);
    }
})
.print();

// 或者在这个例子中用 Tuple 的子类来替代
env.fromElements(1, 2, 3)
.map(i -&gt; new DoubleTuple(i, i))
.print();

public static class DoubleTuple extends Tuple2&lt;Integer, Integer&gt; {
    public DoubleTuple(int f0, int f1) {
        this.f0 = f0;
        this.f1 = f1;
    }
}</programlisting>
</section>
<section xml:id="_富函数_2">
<title>富函数</title>
<programlisting language="java" linenumbering="unnumbered">public static class MyFlatMap extends RichFlatMapFunction&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt; {
  private int subTaskIndex = 0;

  @Override
  public void open(Configuration configuration) {
    int subTaskIndex = getRuntimeContext().getIndexOfThisSubtask();
    // 做一些初始化工作
    // 例如建立一个和HDFS的连接
    System.out.println("enter life cycle");
  }

  @Override
  public void flatMap(Integer in, Collector&lt;Tuple2&lt;Integer, Integer&gt;&gt; out) {
    if (in % 2 == subTaskIndex) {
      out.collect((subTaskIndex, in));
    }
  }

  @Override
  public void close() {
    // 清理工作，断开和HDFS的连接。
    System.out.println("exit life cycle");
  }
}</programlisting>
</section>
<section xml:id="_写入fs">
<title>写入FS</title>
<programlisting language="java" linenumbering="unnumbered">DataStream&lt;String&gt; input = env.fromElements("hello world", "hello world");

final StreamingFileSink&lt;String&gt; sink = StreamingFileSink
    .forRowFormat(new Path("./output"), new SimpleStringEncoder&lt;String&gt;("UTF-8"))
    .withRollingPolicy(
        DefaultRollingPolicy.builder()
            .withRolloverInterval(TimeUnit.MINUTES.toMillis(15))
            .withInactivityInterval(TimeUnit.MINUTES.toMillis(5))
            .withMaxPartSize(1024 * 1024 * 1024)
            .build())
    .build();

input.addSink(sink);</programlisting>
</section>
<section xml:id="_写入kafka">
<title>写入KAFKA</title>
<programlisting language="java" linenumbering="unnumbered">Properties properties = new Properties();
properties.put("bootstrap.servers", "localhost:9092");

DataStreamSource&lt;String&gt; stream = env.readTextFile("UserBehavior.csv");

stream
    .addSink(new FlinkKafkaProducer&lt;String&gt;(
            "userbehavior",
            new SimpleStringSchema(),
            properties
    ));</programlisting>
</section>
<section xml:id="_写入redis">
<title>写入REDIS</title>
<programlisting language="java" linenumbering="unnumbered">FlinkJedisPoolConfig conf = new FlinkJedisPoolConfig.Builder().setHost("localhost").build();

env
   .addSource(new UserBehaviorSource())
   .addSink(new RedisSink&lt;UserBehavior&gt;(conf, new MyRedisSink()));

public static class MyRedisSink implements RedisMapper&lt;UserBehavior&gt; {
    @Override
    public String getKeyFromData(UserBehavior r) {
        return r.userId;
    }

    @Override
    public String getValueFromData(UserBehavior r) {
        return r.itemId;
    }

    @Override
    public RedisCommandDescription getCommandDescription() {
        return new RedisCommandDescription(RedisCommand.HSET, "userbehavior");
    }
}</programlisting>
</section>
<section xml:id="_写入es">
<title>写入ES</title>
<programlisting language="java" linenumbering="unnumbered">DataStreamSource&lt;UserBehavior&gt; stream = env.addSource(new UserBehaviorSource());

ArrayList&lt;HttpHost&gt; httpHosts = new ArrayList&lt;&gt;();
httpHosts.add(new HttpHost("127.0.0.1", 9200, "http"));

ElasticsearchSink.Builder&lt;UserBehavior&gt; esBuilder = new ElasticsearchSink.Builder&lt;&gt;(
        httpHosts,
        new ElasticsearchSinkFunction&lt;UserBehavior&gt;() {
            @Override
            public void process(UserBehavior r, RuntimeContext runtimeContext, RequestIndexer requestIndexer) {
                HashMap&lt;String, String&gt; data = new HashMap&lt;&gt;();
                data.put(r.userId, r.itemId);

                IndexRequest indexRequest = Requests
                        .indexRequest()
                        .index("user-behavior")
                        .source(data);

                requestIndexer.add(indexRequest);
            }
        }
);

esBuilder.setBulkFlushMaxActions(1);

stream.addSink(esBuilder.build());</programlisting>
</section>
<section xml:id="_写入mysql">
<title>写入MYSQL</title>
<programlisting language="java" linenumbering="unnumbered">env.addSource(new UserBehaviorSource()).addSink(new MyJDBC());

public static class MyJDBC extends RichSinkFunction&lt;SensorReading&gt; {
    private Connection conn;
    private PreparedStatement insertStmt;
    private PreparedStatement updateStmt;
    @Override
    public void open(Configuration parameters) throws Exception {
        super.open(parameters);
        conn = DriverManager.getConnection(
                "jdbc:mysql://localhost:3306/database",
                "username",
                "password"
        );
        insertStmt = conn.prepareStatement("INSERT INTO table (userId, itemId) VALUES (?, ?)");
        updateStmt = conn.prepareStatement("UPDATE table SET itemId = ? WHERE userId = ?");
    }

    @Override
    public void invoke(UserBehavior value, Context context) throws Exception {
        updateStmt.setString(1, value.itemId);
        updateStmt.setString(2, value.userId);
        updateStmt.execute();

        if (updateStmt.getUpdateCount() == 0) {
            insertStmt.setString(1, value.userId);
            insertStmt.setString(2, value.itemId);
            insertStmt.execute();
        }
    }

    @Override
    public void close() throws Exception {
        super.close();
        insertStmt.close();
        updateStmt.close();
        conn.close();
    }
}</programlisting>
</section>
<section xml:id="_自定义写入">
<title>自定义写入</title>
<programlisting language="java" linenumbering="unnumbered">env.addSource(new SimpleSocketSink("localhost", 9999));

public static class SimpleSocketSink extends RichSinkFunction&lt;Example1.UserBehavior&gt; {
    private Socket socket;
    private PrintStream writer;
    private String host;
    private Integer port;

    public SimpleSocketSink(String host, Integer port) {
        this.host = host;
        this.port = port;
    }

    @Override
    public void open(Configuration parameters) throws Exception {
        super.open(parameters);
        socket = new Socket(InetAddress.getByName(host), port);
        writer = new PrintStream(socket.getOutputStream());
    }

    @Override
    public void invoke(Example1.UserBehavior value, Context context) throws Exception {
        writer.println(value);
        writer.flush();
    }

    @Override
    public void close() throws Exception {
        super.close();
        writer.close();
        socket.close();
    }
}</programlisting>
</section>
<section xml:id="_水位线生成方式">
<title>水位线生成方式</title>
<simpara>正常设置方式</simpara>
<programlisting language="java" linenumbering="unnumbered">public class Example {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);
        // 每隔1分钟插入一次水位线
        // env.getConfig().setAutoWatermarkInterval(60 * 1000L);

        env
                .addSource(new UserBehaviorSource())
                // 插入水位线的逻辑
                .assignTimestampsAndWatermarks(
                        // 针对乱序流插入水位线
                        // 最大延迟时间设置为5s
                        // 默认200ms的机器时间插入一个水位线
                        // 水位线 = 观察到的元素中的最大时间戳 - 最大延迟时间 - 1ms
                        WatermarkStrategy.&lt;UserBehavior&gt;forBoundedOutOfOrderness(Duration.ofSeconds(5))
                        .withTimestampAssigner(new SerializableTimestampAssigner&lt;UserBehavior&gt;() {
                            // 抽取时间戳的逻辑
                            // 为什么要抽取时间戳？
                            // 如果不指定元素中时间戳的字段，程序就不知道事件时间是哪个字段
                            // 时间戳的单位必须是毫秒
                            @Override
                            public long extractTimestamp(UserBehavior element, long recordTimestamp) {
                                return element.timestamp;
                            }
                        })
                )
                .print();

        env.execute();
    }
}</programlisting>
<simpara>升序时间戳设置</simpara>
<programlisting language="java" linenumbering="unnumbered">.assignTimestampsAndWatermarks(
    // 等价于forBoundedOutOfOrderness(Duration.ofSeconds(0))
    WatermarkStrategy.&lt;UserBehavior&gt;forMonotonousTimestamps()
    .withTimestampAssigner(new SerializableTimestampAssigner&lt;UserBehavior&gt;() {
        @Override
        public long extractTimestamp(UserBehavior element, long recordTimestamp) {
            return element.timestamp;
        }
    })
)</programlisting>
<simpara>自定义周期性插入水位线</simpara>
<programlisting language="java" linenumbering="unnumbered">// 自定义水位线的产生
public class Example {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        env
                .addSource(new UserBehaviorSource())
                .filter(r -&gt; r.behaviorType.equals("pv"))
                .assignTimestampsAndWatermarks(new CustomWatermarkStrategy())
                .print();

        env.execute();
    }

    public static class CustomWatermarkStrategy implements WatermarkStrategy&lt;UserBehavior&gt; {
        @Override
        public TimestampAssigner&lt;UserBehavior&gt; createTimestampAssigner(TimestampAssignerSupplier.Context context) {
            return new SerializableTimestampAssigner&lt;UserBehavior&gt;() {
                @Override
                public long extractTimestamp(UserBehavior element, long recordTimestamp) {
                    return element.timestamp; // 告诉程序数据源里的时间戳是哪一个字段
                }
            };
        }

        @Override
        public WatermarkGenerator&lt;UserBehavior&gt; createWatermarkGenerator(WatermarkGeneratorSupplier.Context context) {
            return new CustomBoundedOutOfOrdernessAssigner();
        }
    }

    public static class CustomBoundedOutOfOrdernessAssigner implements WatermarkGenerator&lt;UserBehavior&gt; {
        private Long delayTime = 5000L; // 最大延迟时间
        private Long maxTs = -Long.MAX_VALUE + delayTime + 1L; // 观察到的最大时间戳

        @Override
        public void onEvent(UserBehavior event, long eventTimestamp, WatermarkOutput output) {
            // 每来一条数据就调用一次
            maxTs = Math.max(event.timestamp, maxTs); // 更新最大时间戳
        }

        @Override
        public void onPeriodicEmit(WatermarkOutput output) {
            // 发射水位线
            // 默认200ms调用一次
            output.emitWatermark(new Watermark(maxTs - delayTime - 1L));
        }
    }
}</programlisting>
<simpara>自定义非周期性水位线</simpara>
<programlisting language="java" linenumbering="unnumbered">public class PunctuatedAssigner implements WatermarkGenerator&lt;UserBehavior&gt; {

    @Override
    public void onEvent(UserBehavior r, long eventTimestamp, WatermarkOutput output) {
        if (r.itemId.equals("someItemId")) {
            output.emitWatermark(new Watermark(r.timestamp - 1));
        }
    }

    @Override
    public void onPeriodicEmit(WatermarkOutput output) {
        // 不需要做任何事情，因为我们在onEvent方法中发射了水位线
    }
}</programlisting>
</section>
<section xml:id="_window_api">
<title>WINDOW API</title>
<programlisting language="java" linenumbering="unnumbered">.keyBy(...)
.window(TumblingProcessingTimeWindows.of(Time.seconds(5)))
.aggregate(...)

.keyBy(...)
.window(SlidingProcessingTimeWindows.of(Time.seconds(10), Time.seconds(5)))
.aggregate(...)

.keyBy(...)
.window(ProcessingTimeSessionWindows.withGap(Time.seconds(10)))
.aggregate(...)

.keyBy(...)
.window(TumblingEventTimeWindows.of(Time.seconds(5)))
.aggregate(...)

.keyBy(...)
.window(SlidingEventTimeWindows.of(Time.seconds(10), Time.seconds(5)))
.aggregate(...)

.keyBy(...)
.window(EventTimeSessionWindows.withGap(Time.seconds(10)))
.aggregate(...)</programlisting>
</section>
<section xml:id="_window_reduce">
<title>WINDOW REDUCE</title>
<programlisting language="java" linenumbering="unnumbered">stream
    .map(new MapFunction&lt;UserBehavior, Tuple2&lt;String, Long&gt;&gt;() {
        @Override
        public Tuple2&lt;String, Long&gt; map(UserBehavior value) throws Exception {
            return Tuple2.of(value.userId, 1L);
        }
    })
    .keyBy(r -&gt; r.f0)
    .window(TumblingProcessingTimeWindows.of(Time.seconds(5)))
    .reduce(new ReduceFunction&lt;Tuple2&lt;String, Long&gt;&gt;() {
        @Override
        public Tuple2&lt;String, Long&gt; reduce(Tuple2&lt;String, Long&gt; value1, Tuple2&lt;String, Long&gt; value2) throws Exception {
            return Tuple2.of(value1.f0, value1.f1 + value2.f1);
        }
    });</programlisting>
</section>
<section xml:id="_window_aggregatefunction">
<title>WINDOW AGGREGATEFUNCTIOn</title>
<programlisting language="java" linenumbering="unnumbered">stream
    .keyBy(
        new KeySelector&lt;UserBehavior, String&gt;() {
            @Override
            public String getKey(UserBehavior value) throws Exception {
                return "key";
            }
        }
    )
    .window(TumblingProcessingTimeWindows.of(Time.seconds(5)))
    .aggregate(new AvgPv())
    .print();

public static class AvgPv implements AggregateFunction&lt;UserBehavior, Tuple2&lt;HashSet&lt;String&gt;, Long&gt;, Double&gt; {
    @Override
    public Tuple2&lt;HashSet&lt;String&gt;, Long&gt; createAccumulator() {
        return Tuple3.of(new HashSet&lt;String&gt;(), 0L);
    }

    @Override
    public Tuple2&lt;HashSet&lt;String&gt;, Long&gt; add(UserBehavior value, Tuple2&lt;HashSet&lt;String&gt;, Long&gt; accumulator) {
        return Tuple2.of(accumulator.f0.add(value.userId), accumulator.f1 + 1L);
    }

    @Override
    public Double getResult(Tuple2&lt;HashSet&lt;String&gt;, Long&gt; accumulator) {
        return accumulator.f0.size() / accumulator.f1;
    }

    @Override
    public Tuple2&lt;HashSet&lt;String&gt;, Long&gt; merge(Tuple2&lt;HashSet&lt;String&gt;, Long&gt; a, Tuple2&lt;HashSet&lt;String&gt;, Long&gt; b) {
        return null;
    }
}</programlisting>
</section>
<section xml:id="_window_processfunction">
<title>WINDOW PROCESSFUNCTION</title>
<programlisting language="java" linenumbering="unnumbered">env
    .addSource(new UserBehaviorSource())
    .assignTimestampsAndWatermarks(
            WatermarkStrategy.&lt;Example1.UserBehavior&gt;forMonotonousTimestamps()
            .withTimestampAssigner(new SerializableTimestampAssigner&lt;Example1.UserBehavior&gt;() {
                @Override
                public long extractTimestamp(Example1.UserBehavior element, long recordTimestamp) {
                    return element.timestamp;
                }
            })
    )
    .keyBy(new KeySelector&lt;Example1.UserBehavior, String&gt;() {
        @Override
        public String getKey(Example1.UserBehavior value) throws Exception {
            return "key";
        }
    })
    .window(TumblingEventTimeWindows.of(Time.hours(1)))
    .apply(new WindowFunction&lt;Example1.UserBehavior, String, String, TimeWindow&gt;() {
        @Override
        public void apply(String s, TimeWindow timeWindow, Iterable&lt;Example1.UserBehavior&gt; iterable, Collector&lt;String&gt; collector) throws Exception {
            HashSet&lt;String&gt; set = new HashSet&lt;&gt;();
            for (Example1.UserBehavior e : iterable) {
                set.add(e.userId);
            }
            collector.collect(set.size() + "");
        }
    })
    .print();</programlisting>
</section>
<section xml:id="_aggregatefunction_with_windowfunction">
<title>AGGREGATEFUNCTION WITH WINDOWFUNCTION</title>
<programlisting language="java" linenumbering="unnumbered">SingleOutputStreamOperator&lt;ItemViewCount&gt; ivcStream = pvStream
    .keyBy(r -&gt; r.itemId)
    .window(SlidingEventTimeWindows.of(Time.hours(1), Time.minutes(5)))
    .aggregate(new CountAgg(), new WindowResult());

public static class CountAgg implements AggregateFunction&lt;UserBehavior, Long, Long&gt; {
    @Override
    public Long createAccumulator() {
        return 0L;
    }

    @Override
    public Long add(UserBehavior value, Long accumulator) {
        return accumulator + 1L;
    }

    @Override
    public Long getResult(Long accumulator) {
        return accumulator;
    }

    @Override
    public Long merge(Long a, Long b) {
        return null;
    }
}

public static class WindowResult implements WindowFunction&lt;Long, ItemViewCount, String, TimeWindow&gt; {
    @Override
    public void apply(String s, TimeWindow timeWindow, Iterable&lt;Long&gt; iterable, Collector&lt;ItemViewCount&gt; collector) throws Exception {
        collector.collect(new ItemViewCount(s, iterable.iterator().next(), timeWindow.getStart(), timeWindow.getEnd()));
    }
}

public static class ItemViewCount {
    public String itemId;
    public Long count;
    public Long windowStart;
    public Long windowEnd;

    public ItemViewCount() {
    }

    public ItemViewCount(String itemId, Long count, Long windowStart, Long windowEnd) {
        this.itemId = itemId;
        this.count = count;
        this.windowStart = windowStart;
        this.windowEnd = windowEnd;
    }

    @Override
    public String toString() {
        return "ItemViewCount{" +
                "itemId='" + itemId + '\'' +
                ", count=" + count +
                ", windowStart=" + new Timestamp(windowStart) +
                ", windowEnd=" + new Timestamp(windowEnd) +
                '}';
    }
}</programlisting>
</section>
<section xml:id="_处理函数_processfunction">
<title>处理函数: PROCESSFUNCTION</title>
<programlisting language="java" linenumbering="unnumbered">env
    .addSource(new UserBehaviorSource())
    .process(new ProcessFunction&lt;Example1.UserBehavior, String&gt;() {
        @Override
        public void processElement(Example1.UserBehavior value, Context ctx, Collector&lt;String&gt; out) throws Exception {
            if (value.itemId.equals("someItemId1")) {
                out.collect(value.itemId);
            } else if (value.itemId.equals("someItemId2")) {
                out.collect(value.itemId);
                out.collect(value.itemId);
            }
            System.out.println(ctx.timerService().currentWatermark());
        }
    })
    .print();</programlisting>
</section>
<section xml:id="_处理函数_keyedprocessfunction">
<title>处理函数: KEYEDPROCESSFUNCTION</title>
<simpara>处理时间</simpara>
<programlisting language="java" linenumbering="unnumbered">public class Example {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        env
                .addSource(new CustomSource())
                .keyBy(r -&gt; true)
                .process(new KeyedProcessFunction&lt;Boolean, String, String&gt;() {
                    @Override
                    public void processElement(String s, Context context, Collector&lt;String&gt; collector) throws Exception {
                        // 每来一条数据，调用一次
                        long currTs = context.timerService().currentProcessingTime();
                        collector.collect("数据到达，到达时间是：" + new Timestamp(currTs));
                        // 注册10s之后的定时器，定时器是onTimer
                        context.timerService().registerProcessingTimeTimer(currTs + 10 * 1000L);
                    }

                    @Override
                    public void onTimer(long timestamp, OnTimerContext ctx, Collector&lt;String&gt; out) throws Exception {
                        super.onTimer(timestamp, ctx, out);
                        out.collect("定时器触发了，触发时间是：" + new Timestamp(timestamp));
                    }
                })
                .print();

        env.execute();
    }

    public static class CustomSource implements SourceFunction&lt;String&gt; {
        @Override
        public void run(SourceContext&lt;String&gt; ctx) throws Exception {
            ctx.collect("a");
            Thread.sleep(20 * 1000L);
        }

        @Override
        public void cancel() {
        }
    }
}</programlisting>
<simpara>事件时间</simpara>
<programlisting language="java" linenumbering="unnumbered">public class Example {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        env
                .addSource(new CustomSource())
                .assignTimestampsAndWatermarks(
                        WatermarkStrategy.&lt;Tuple2&lt;String, Long&gt;&gt;forMonotonousTimestamps()
                        .withTimestampAssigner(new SerializableTimestampAssigner&lt;Tuple2&lt;String, Long&gt;&gt;() {
                            @Override
                            public long extractTimestamp(Tuple2&lt;String, Long&gt; element, long recordTimestamp) {
                                return element.f1;
                            }
                        })
                )
                .keyBy(r -&gt; r.f0)
                .process(new KeyedProcessFunction&lt;String, Tuple2&lt;String, Long&gt;, String&gt;() {
                    @Override
                    public void processElement(Tuple2&lt;String, Long&gt; element, Context context, Collector&lt;String&gt; collector) throws Exception {
                        // 每来一条数据，调用一次
                        collector.collect("数据到达，当前数据的事件时间是：" + new Timestamp(element.f1));
                        // 注册10s之后的定时器，定时器是onTimer
                        context.timerService().registerEventTimeTimer(element.f1 + 10 * 1000L);
                    }

                    @Override
                    public void onTimer(long timestamp, OnTimerContext ctx, Collector&lt;String&gt; out) throws Exception {
                        super.onTimer(timestamp, ctx, out);
                        out.collect("定时器触发了，触发时间是：" + new Timestamp(timestamp));
                    }
                })
                .print();

        env.execute();
    }

    public static class CustomSource implements SourceFunction&lt;Tuple2&lt;String, Long&gt;&gt; {
        @Override
        public void run(SourceContext&lt;Tuple2&lt;String, Long&gt;&gt; ctx) throws Exception {
            ctx.collect(Tuple2.of("a", 1000L));
            Thread.sleep(1000L);
            ctx.collect(Tuple2.of("a", 12 * 1000L));
        }

        @Override
        public void cancel() {

        }
    }
}</programlisting>
</section>
<section xml:id="_处理函数_processwindowfunction">
<title>处理函数: PROCESSWINDOWFUNCTION</title>
<programlisting language="java" linenumbering="unnumbered">// 实时热门商品
public class Example {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        SingleOutputStreamOperator&lt;UserBehavior&gt; pvStream = env
                .addSource(new UserBehaviorSource())
                .filter(r -&gt; r.behaviorType.equals("pv"))
                .assignTimestampsAndWatermarks(
                        WatermarkStrategy.&lt;UserBehavior&gt;forBoundedOutOfOrderness(Duration.ofSeconds(0))
                                .withTimestampAssigner(new SerializableTimestampAssigner&lt;UserBehavior&gt;() {
                                    @Override
                                    public long extractTimestamp(UserBehavior element, long recordTimestamp) {
                                        return element.timestamp;
                                    }
                                })
                );

        // 先求出每件商品在每个窗口的访问量
        SingleOutputStreamOperator&lt;ItemViewCount&gt; ivcStream = pvStream
                .keyBy(r -&gt; r.itemId)
                .window(SlidingEventTimeWindows.of(Time.hours(1), Time.minutes(5)))
                .aggregate(new CountAgg(), new WindowResult());

        // 每一条支流都是同一个窗口中的不同商品的ItemViewCount次数
        KeyedStream&lt;ItemViewCount, Long&gt; ivcKeyedStream = ivcStream
                .keyBy(r -&gt; r.windowEnd);

        ivcKeyedStream
                .process(new TopN(3))
                .print();

        env.execute();
    }

    public static class TopN extends KeyedProcessFunction&lt;Long, ItemViewCount, String&gt; {
        // 列表状态变量
        private ListState&lt;ItemViewCount&gt; itemViewCountListState;
        private Integer threshold;

        public TopN(Integer threshold) {
            this.threshold = threshold;
        }
        @Override
        public void open(Configuration parameters) throws Exception {
            super.open(parameters);
            itemViewCountListState = getRuntimeContext().getListState(
                    new ListStateDescriptor&lt;ItemViewCount&gt;("list-state", Types.POJO(ItemViewCount.class))
            );
        }

        @Override
        public void processElement(ItemViewCount value, Context ctx, Collector&lt;String&gt; out) throws Exception {
            itemViewCountListState.add(value); // 添加到列表状态变量中
            // 水位线超过窗口结束时间+1毫秒时触发定时器来进行排序
            ctx.timerService().registerEventTimeTimer(value.windowEnd + 1);
        }

        @Override
        public void onTimer(long timestamp, OnTimerContext ctx, Collector&lt;String&gt; out) throws Exception {
            super.onTimer(timestamp, ctx, out);
            // 将数据从列表状态变量中取出
            ArrayList&lt;ItemViewCount&gt; itemViewCountArrayList = new ArrayList&lt;&gt;();
            for (ItemViewCount ivc : itemViewCountListState.get()) {
                itemViewCountArrayList.add(ivc);
            }
            itemViewCountListState.clear();

            itemViewCountArrayList.sort(new Comparator&lt;ItemViewCount&gt;() {
                @Override
                public int compare(ItemViewCount o1, ItemViewCount o2) {
                    return o2.count.intValue() - o1.count.intValue();
                }
            });

            StringBuilder result = new StringBuilder();
            result
                    .append("========================================\n");
            for (int i = 0; i &lt; this.threshold; i++) {
                ItemViewCount itemViewCount = itemViewCountArrayList.get(i);
                result
                        .append("浏览量No." + (i + 1) + " ")
                        .append("商品ID：" + itemViewCount.itemId + " ")
                        .append("浏览量：" + itemViewCount.count + " ")
                        .append("窗口结束时间：" + new Timestamp(timestamp - 1) + "\n");
            }
            result
                    .append("========================================\n\n\n");
            out.collect(result.toString());
        }
    }

    public static class CountAgg implements AggregateFunction&lt;UserBehavior, Long, Long&gt; {
        @Override
        public Long createAccumulator() {
            return 0L;
        }

        @Override
        public Long add(UserBehavior value, Long accumulator) {
            return accumulator + 1L;
        }

        @Override
        public Long getResult(Long accumulator) {
            return accumulator;
        }

        @Override
        public Long merge(Long a, Long b) {
            return null;
        }
    }

    public static class WindowResult extends ProcessWindowFunction&lt;Long, ItemViewCount, String, TimeWindow&gt; {
        @Override
        public void process(String s, Context context, Iterable&lt;Long&gt; elements, Collector&lt;ItemViewCount&gt; out) throws Exception {
            out.collect(new ItemViewCount(s, elements.iterator().next(), context.window().getStart(), context.window().getEnd()));
        }
    }
}</programlisting>
</section>
<section xml:id="_处理函数_processallwindowfunction">
<title>处理函数: PROCESSALLWINDOWFUNCTION</title>
<simpara>计算实时热门商品</simpara>
<programlisting language="java" linenumbering="unnumbered">// 用最简单的方式实现实时TopN
public class Example {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        SingleOutputStreamOperator&lt;UserBehavior&gt; pvStream = env
                .addSource(new UserBehaviorSource())
                .filter(r -&gt; r.behaviorType.equals("pv"))
                .assignTimestampsAndWatermarks(
                        WatermarkStrategy.&lt;UserBehavior&gt;forBoundedOutOfOrderness(Duration.ofSeconds(0))
                                .withTimestampAssigner(new SerializableTimestampAssigner&lt;UserBehavior&gt;() {
                                    @Override
                                    public long extractTimestamp(UserBehavior element, long recordTimestamp) {
                                        return element.timestamp;
                                    }
                                })
                );

        pvStream
                .map(new MapFunction&lt;UserBehavior, Tuple2&lt;String, String&gt;&gt;() {
                    @Override
                    public Tuple2&lt;String, String&gt; map(UserBehavior value) throws Exception {
                        return Tuple2.of("key", value.itemId);
                    }
                })
                .windowAll(SlidingEventTimeWindows.of(Time.hours(1), Time.minutes(5)))
                .process(new ProcessAllWindowFunction&lt;Tuple2&lt;String, String&gt;, String, TimeWindow&gt;() {
                    @Override
                    public void process(Context context, Iterable&lt;Tuple2&lt;String, String&gt;&gt; elements, Collector&lt;String&gt; out) throws Exception {
                        HashMap&lt;String, Long&gt; itemCountMap = new HashMap&lt;&gt;();
                        for (Tuple2&lt;String, String&gt; e : elements) {
                            if (itemCountMap.containsKey(e.f1)) {
                                long count = itemCountMap.get(e.f1);
                                itemCountMap.put(e.f1, count + 1L);
                            } else {
                                itemCountMap.put(e.f1, 1L);
                            }
                        }
                        ArrayList&lt;Tuple2&lt;String, Long&gt;&gt; mapList = new ArrayList&lt;Tuple2&lt;String, Long&gt;&gt;();
                        for (String key : itemCountMap.keySet()) {
                            mapList.add(Tuple2.of(key, itemCountMap.get(key)));
                        }
                        mapList.sort(new Comparator&lt;Tuple2&lt;String, Long&gt;&gt;() {
                            @Override
                            public int compare(Tuple2&lt;String, Long&gt; o1, Tuple2&lt;String, Long&gt; o2) {
                                return o2.f1.intValue() - o1.f1.intValue();
                            }
                        });
                        StringBuilder result = new StringBuilder();
                        result
                                .append("========================================\n");
                        for (int i = 0; i &lt; 3; i++) {
                            Tuple2&lt;String, Long&gt; temp = mapList.get(i);
                            result
                                    .append("浏览量No." + (i + 1) + " ")
                                    .append("商品ID：" + temp.f0 + " ")
                                    .append("浏览量：" + temp.f1 + " ")
                                    .append("窗口结束时间：" + new Timestamp(context.window().getEnd()) + "\n");
                        }
                        result
                                .append("========================================\n\n\n");
                        out.collect(result.toString());
                    }

                })
                .print();


        env.execute();
    }
}</programlisting>
</section>
<section xml:id="_多流转换_union">
<title>多流转换: UNION</title>
<programlisting language="java" linenumbering="unnumbered">// 合流水位线传播
public class Example {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        SingleOutputStreamOperator&lt;Tuple2&lt;String, Long&gt;&gt; stream1 = env
            .socketTextStream("localhost", 9999)
            .map(new MapFunction&lt;String, Tuple2&lt;String, Long&gt;&gt;() {
                @Override
                public Tuple2&lt;String, Long&gt; map(String value) throws Exception {
                    String[] arr = value.split(" ");
                    return Tuple2.of(arr[0], Long.parseLong(arr[1]) * 1000L);
                }
            })
            .assignTimestampsAndWatermarks(
                WatermarkStrategy.&lt;Tuple2&lt;String, Long&gt;&gt;forMonotonousTimestamps()
                    .withTimestampAssigner(new SerializableTimestampAssigner&lt;Tuple2&lt;String, Long&gt;&gt;() {
                        @Override
                        public long extractTimestamp(Tuple2&lt;String, Long&gt; element, long recordTimestamp) {
                            return element.f1;
                        }
                    })
            );

        SingleOutputStreamOperator&lt;Tuple2&lt;String, Long&gt;&gt; stream2 = env
            .socketTextStream("localhost", 9998)
            .map(new MapFunction&lt;String, Tuple2&lt;String, Long&gt;&gt;() {
                @Override
                public Tuple2&lt;String, Long&gt; map(String value) throws Exception {
                    String[] arr = value.split(" ");
                    return Tuple2.of(arr[0], Long.parseLong(arr[1]) * 1000L);
                }
            })
            .assignTimestampsAndWatermarks(
                WatermarkStrategy.&lt;Tuple2&lt;String, Long&gt;&gt;forMonotonousTimestamps()
                    .withTimestampAssigner(new SerializableTimestampAssigner&lt;Tuple2&lt;String, Long&gt;&gt;() {
                        @Override
                        public long extractTimestamp(Tuple2&lt;String, Long&gt; element, long recordTimestamp) {
                            return element.f1;
                        }
                    })
            );

        stream1
            .union(stream2)
            .process(new ProcessFunction&lt;Tuple2&lt;String, Long&gt;, String&gt;() {
                @Override
                public void processElement(Tuple2&lt;String, Long&gt; stringLongTuple2, Context context, Collector&lt;String&gt; collector) throws Exception {
                    // 数据到来立即调用，发送当前process函数的水位线的水平
                    collector.collect("当前水位线是：" + context.timerService().currentWatermark());
                }
            })
            .print();

        env.execute();
    }
}</programlisting>
</section>
<section xml:id="_多流转换_connect">
<title>多流转换: CONNECT</title>
<programlisting language="java" linenumbering="unnumbered">DataStreamSource&lt;String&gt; behaviorStream = env.addSource(new CustomSource());

DataStreamSource&lt;UserBehavior&gt; stream = env.addSource(new UserBehaviorSource());

stream
    .keyBy(r -&gt; r.userId)
    .connect(behaviorStream.broadcast())
    .flatMap(new CoFlatMapFunction&lt;UserBehavior, String, UserBehavior&gt;() {
        private String behaviorType = "pv";
        @Override
        public void flatMap1(UserBehavior value, Collector&lt;UserBehavior&gt; out) throws Exception {
            if (value.behaviorType.equals(behaviorType)) {
                out.collect(value);
            }
        }

        @Override
        public void flatMap2(String value, Collector&lt;UserBehavior&gt; out) throws Exception {
            behaviorType = value;
        }
    })
    .print();

public static class CustomSource implements SourceFunction&lt;String&gt; {
    @Override
    public void run(SourceContext&lt;String&gt; ctx) throws Exception {
        ctx.collect("pv");
        Thread.sleep(10 * 1000L);
        ctx.collect("buy");
        Thread.sleep(10 * 1000L);
        ctx.collect("cart");
    }

    @Override
    public void cancel() {}
}</programlisting>
</section>
<section xml:id="_基于间隔的join_2">
<title>基于间隔的JOIN</title>
<programlisting language="java" linenumbering="unnumbered">SingleOutputStreamOperator&lt;UserBehavior&gt; buyStream = env
    .addSource(new UserBehaviorSource())
    .filter(r -&gt; r.behaviorType.equals("buy"))
    .assignTimestampsAndWatermarks(
        WatermarkStrategy.&lt;UserBehavior&gt;forMonotonousTimestamps()
            .withTimestampAssigner(new SerializableTimestampAssigner&lt;UserBehavior&gt;() {
                @Override
                public long extractTimestamp(UserBehavior element, long recordTimestamp) {
                    return element.timestamp;
                }
            })
    );

SingleOutputStreamOperator&lt;UserBehavior&gt; pvStream = env
    .addSource(new UserBehaviorSource())
    .filter(r -&gt; r.behaviorType.equals("pv"))
    .assignTimestampsAndWatermarks(
        WatermarkStrategy.&lt;UserBehavior&gt;forMonotonousTimestamps()
            .withTimestampAssigner(new SerializableTimestampAssigner&lt;UserBehavior&gt;() {
                @Override
                public long extractTimestamp(UserBehavior element, long recordTimestamp) {
                    return element.timestamp;
                }
            })
    );

buyStream
    .keyBy(r -&gt; r.userId)
    .intervalJoin(pvStream.keyBy(r -&gt; r.userId))
    .between(Time.minutes(-10), Time.minutes(10))
    .process(new ProcessJoinFunction&lt;UserBehavior, UserBehavior, String&gt;() {
        @Override
        public void processElement(UserBehavior left, UserBehavior right, Context ctx, Collector&lt;String&gt; out) throws Exception {
            out.collect(left + " =&gt; " + right);
        }
    })
    .print();</programlisting>
</section>
<section xml:id="_基于窗口的join_2">
<title>基于窗口的JOIN</title>
<programlisting language="java" linenumbering="unnumbered">SingleOutputStreamOperator&lt;UserBehavior&gt; buyStream = env
    .addSource(new UserBehaviorSource())
    .filter(r -&gt; r.behaviorType.equals("buy"))
    .assignTimestampsAndWatermarks(
        WatermarkStrategy.&lt;UserBehavior&gt;forMonotonousTimestamps()
            .withTimestampAssigner(new SerializableTimestampAssigner&lt;UserBehavior&gt;() {
                @Override
                public long extractTimestamp(UserBehavior element, long recordTimestamp) {
                    return element.timestamp;
                }
            })
    );

SingleOutputStreamOperator&lt;UserBehavior&gt; pvStream = env
    .addSource(new UserBehaviorSource())
    .filter(r -&gt; r.behaviorType.equals("pv"))
    .assignTimestampsAndWatermarks(
        WatermarkStrategy.&lt;UserBehavior&gt;forMonotonousTimestamps()
            .withTimestampAssigner(new SerializableTimestampAssigner&lt;UserBehavior&gt;() {
                @Override
                public long extractTimestamp(UserBehavior element, long recordTimestamp) {
                    return element.timestamp;
                }
            })
    );

buyStream
    .join(pvStream)
    .where(r -&gt; r.userId)
    .equalTo(r -&gt; r.userId)
    .window(TumblingEventTimeWindows.of(Time.hours(1)))
    .apply(new JoinFunction&lt;UserBehavior, UserBehavior, String&gt;() {
        @Override
        public String join(UserBehavior first, UserBehavior second) throws Exception {
            return first + " =&gt; " + second;
        }
    })
    .print();</programlisting>
</section>
<section xml:id="_多流转换_cogroup">
<title>多流转换: COGROUP</title>
<programlisting language="java" linenumbering="unnumbered">buyStream
    .coGroup(pvStream)
    .where(r -&gt; r.userId)
    .equalTo(r -&gt; r.userId)
    .window(TumblingEventTimeWindows.of(Time.hours(1)))
    .apply(new CoGroupFunction&lt;Example1.UserBehavior, Example1.UserBehavior, String&gt;() {
        @Override
        public void coGroup(Iterable&lt;Example1.UserBehavior&gt; first, Iterable&lt;Example1.UserBehavior&gt; second, Collector&lt;String&gt; out) throws Exception {
            out.collect(first + " =&gt; " + second);
        }
    })
    .print();</programlisting>
</section>
<section xml:id="_状态变量_valuestate">
<title>状态变量: VALUESTATE</title>
<simpara>每个品类的PV值求解</simpara>
<programlisting language="java" linenumbering="unnumbered">env
    .addSource(new UserBehaviorSource())
    .assignTimestampsAndWatermarks(
        WatermarkStrategy.&lt;UserBehavior&gt;forMonotonousTimestamps()
        .withTimestampAssigner(new SerializableTimestampAssigner&lt;UserBehavior&gt;() {
            @Override
            public long extractTimestamp(UserBehavior element, long recordTimestamp) {
                return element.timestamp;
            }
        })
    )
    .keyBy(r -&gt; r.categoryId)
    .process(new KeyedProcessFunction&lt;String, UserBehavior, String&gt;() {
        private ValueState&lt;Long&gt; valueState;
        private ValueState&lt;Long&gt; timerTs;

        @Override
        public void open(Configuration parameters) throws Exception {
            super.open(parameters);
            valueState = getRuntimeContext().getState(new ValueStateDescriptor&lt;Long&gt;("pv", Types.LONG));
        }

        @Override
        public void processElement(UserBehavior value, Context ctx, Collector&lt;String&gt; out) throws Exception {
            if (valueState.value() == null) {
                valueState.update(1L);
            } else {
                valueState.update(valueState.value() + 1L);
            }
            if (timerTs.value() == null) {
                timerTs.update(value.timestamp + 10 * 1000L);
            }
        }

        @Override
        public void onTimer(long timestamp, OnTimerContext ctx, Collector&lt;String&gt; out) throws Exception {
            super.onTimer(timestamp, ctx, out);
            out.collect("品类 " + ctx.getCurrentKey() + " 的PV是：" + valueState.value());
            timerTs.clear();
        }
    })
    .print();</programlisting>
</section>
<section xml:id="_状态变量_liststate">
<title>状态变量: LISTSTATE</title>
<simpara>实时热门商品</simpara>
<programlisting language="java" linenumbering="unnumbered">// 每个小时中的实时热门商品
public class Example {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        env
            .addSource(new UserBehaviorSource())
            .filter(r -&gt; r.behaviorType.equals("pv"))
            .assignTimestampsAndWatermarks(
                WatermarkStrategy.&lt;UserBehavior&gt;forBoundedOutOfOrderness(Duration.ofSeconds(0))
                    .withTimestampAssigner(new SerializableTimestampAssigner&lt;UserBehavior&gt;() {
                        @Override
                        public long extractTimestamp(UserBehavior element, long recordTimestamp) {
                            return element.timestamp;
                        }
                    })
            )
            .keyBy(r -&gt; r.itemId)
            .window(TumblingEventTimeWindows.of(Time.hours(1)))
            .aggregate(new CountAgg(), new WindowResult())
            .keyBy(r -&gt; r.windowEnd)
            .process(new TopN(3))
            .print();

        env.execute();
    }

    public static class TopN extends KeyedProcessFunction&lt;Long, ItemViewCount, String&gt; {
        private ListState&lt;ItemViewCount&gt; listState;
        private Integer N;

        public TopN(Integer n) {
            N = n;
        }

        @Override
        public void open(Configuration parameters) throws Exception {
            super.open(parameters);
            listState = getRuntimeContext().getListState(
                    new ListStateDescriptor&lt;ItemViewCount&gt;("list", Types.POJO(ItemViewCount.class))
            );
        }

        @Override
        public void processElement(ItemViewCount itemViewCount, Context context, Collector&lt;String&gt; collector) throws Exception {
            listState.add(itemViewCount);
            context.timerService().registerEventTimeTimer(itemViewCount.windowEnd + 100L);
        }

        @Override
        public void onTimer(long timestamp, OnTimerContext ctx, Collector&lt;String&gt; out) throws Exception {
            super.onTimer(timestamp, ctx, out);
            ArrayList&lt;ItemViewCount&gt; itemViewCounts = new ArrayList&lt;&gt;();
            for (ItemViewCount e : listState.get()) {
                itemViewCounts.add(e);
            }
            listState.clear();

            itemViewCounts.sort(new Comparator&lt;ItemViewCount&gt;() {
                @Override
                public int compare(ItemViewCount t1, ItemViewCount t2) {
                    return t2.count.intValue() - t1.count.intValue();
                }
            });

            StringBuilder result = new StringBuilder();
            result
                    .append("=====================================\n");
            for (int i = 0; i &lt; N; i++) {
                ItemViewCount itemViewCount = itemViewCounts.get(i);
                result
                        .append("窗口结束时间是：" + new Timestamp(timestamp - 100L))
                        .append("第" + (i + 1) + "名的商品id是：" + itemViewCount.itemId)
                        .append("浏览次数是：" + itemViewCount.count + "\n");
            }
            result
                    .append("=====================================\n");
            out.collect(result.toString());
        }
    }

    public static class CountAgg implements AggregateFunction&lt;UserBehavior, Long, Long&gt; {
        @Override
        public Long createAccumulator() {
            return 0L;
        }

        @Override
        public Long add(UserBehavior value, Long accumulator) {
            return accumulator + 1L;
        }

        @Override
        public Long getResult(Long accumulator) {
            return accumulator;
        }

        @Override
        public Long merge(Long a, Long b) {
            return null;
        }
    }

    public static class WindowResult extends ProcessWindowFunction&lt;Long, ItemViewCount, String, TimeWindow&gt; {
        @Override
        public void process(String s, Context context, Iterable&lt;Long&gt; iterable, Collector&lt;ItemViewCount&gt; collector) throws Exception {
            collector.collect(new ItemViewCount(s, iterable.iterator().next(), context.window().getStart(), context.window().getEnd()));
        }
    }

    public static class ItemViewCount {
        public String itemId;
        public Long count;
        public Long windowStart;
        public Long windowEnd;

        public ItemViewCount() {
        }

        public ItemViewCount(String itemId, Long count, Long windowStart, Long windowEnd) {
            this.itemId = itemId;
            this.count = count;
            this.windowStart = windowStart;
            this.windowEnd = windowEnd;
        }

        @Override
        public String toString() {
            return "ItemViewCount{" +
                    "itemId='" + itemId + '\'' +
                    ", count=" + count +
                    ", windowStart=" + new Timestamp(windowStart) +
                    ", windowEnd=" + new Timestamp(windowEnd) +
                    '}';
        }
    }

    public static class UserBehavior {
        public String userId;
        public String itemId;
        public String categoryId;
        public String behaviorType;
        public Long timestamp;

        public UserBehavior() {
        }

        public UserBehavior(String userId, String itemId, String categoryId, String behaviorType, Long timestamp) {
            this.userId = userId;
            this.itemId = itemId;
            this.categoryId = categoryId;
            this.behaviorType = behaviorType;
            this.timestamp = timestamp;
        }

        @Override
        public String toString() {
            return "UserBehavior{" +
                    "userId='" + userId + '\'' +
                    ", itemId='" + itemId + '\'' +
                    ", categoryId='" + categoryId + '\'' +
                    ", behaviorType='" + behaviorType + '\'' +
                    ", timestamp=" + new Timestamp(timestamp) +
                    '}';
        }
    }
}</programlisting>
</section>
<section xml:id="_状态变量_mapstate">
<title>状态变量: MAPSTATE</title>
<simpara>数据倾斜的解决</simpara>
<programlisting language="java" linenumbering="unnumbered">public class DataSkew {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        env
            .fromElements(
                Tuple3.of("a", 1L, 1000L),
                Tuple3.of("a", 1L, 2000L),
                Tuple3.of("a", 1L, 3000L),
                Tuple3.of("a", 1L, 4000L),
                Tuple3.of("a", 1L, 5000L),
                Tuple3.of("a", 1L, 6000L),
                Tuple3.of("a", 1L, 7000L),
                Tuple3.of("a", 1L, 8000L),
                Tuple3.of("a", 1L, 9000L),
                Tuple3.of("a", 1L, 10000L),
                Tuple3.of("b", 1L, 11000L)
            )
            .map(new MapFunction&lt;Tuple3&lt;String, Long, Long&gt;, Tuple3&lt;String, Long, Long&gt;&gt;() {
                @Override
                public Tuple3&lt;String, Long, Long&gt; map(Tuple3&lt;String, Long, Long&gt; value) throws Exception {
                    Random rand = new Random();
                    return Tuple3.of(value.f0 + "-" + rand.nextInt(4), value.f1, value.f2);
                }
            })
            .assignTimestampsAndWatermarks(WatermarkStrategy.&lt;Tuple3&lt;String, Long, Long&gt;&gt;forMonotonousTimestamps()
            .withTimestampAssigner(new SerializableTimestampAssigner&lt;Tuple3&lt;String, Long, Long&gt;&gt;() {
                @Override
                public long extractTimestamp(Tuple3&lt;String, Long, Long&gt; element, long recordTimestamp) {
                    return element.f2;
                }
            }))
            .keyBy(r -&gt; r.f0)
            .process(new KeyedProcessFunction&lt;String, Tuple3&lt;String, Long, Long&gt;, Tuple2&lt;String, Long&gt;&gt;() {
                private ValueState&lt;Tuple2&lt;String, Long&gt;&gt; sum;
                private ValueState&lt;Long&gt; timerTs;
                @Override
                public void open(Configuration parameters) throws Exception {
                    super.open(parameters);
                    sum = getRuntimeContext().getState(new ValueStateDescriptor&lt;Tuple2&lt;String, Long&gt;&gt;("sum", Types.TUPLE(Types.STRING, Types.LONG)));
                    timerTs = getRuntimeContext().getState(new ValueStateDescriptor&lt;Long&gt;("timer", Types.LONG));
                }

                @Override
                public void processElement(Tuple3&lt;String, Long, Long&gt; value, Context ctx, Collector&lt;Tuple2&lt;String, Long&gt;&gt; out) throws Exception {
                    if (sum.value() == null) {
                        sum.update(Tuple2.of(value.f0, value.f1));
                        ctx.timerService().registerEventTimeTimer(value.f2 + 10 * 1000L);
                        timerTs.update(value.f2 + 10 * 1000L);
                    } else {
                        Long cnt = sum.value().f1;
                        sum.update(Tuple2.of(value.f0, cnt + value.f1));
                        if (timerTs.value() == null) {
                            ctx.timerService().registerEventTimeTimer(value.f2 + 10 * 1000L);
                            timerTs.update(value.f2 + 10 * 1000L);
                        }
                    }
                }

                @Override
                public void onTimer(long timestamp, OnTimerContext ctx, Collector&lt;Tuple2&lt;String, Long&gt;&gt; out) throws Exception {
                    super.onTimer(timestamp, ctx, out);
                    out.collect(Tuple2.of(ctx.getCurrentKey(), sum.value().f1));
                    timerTs.clear();
                }
            })
            .map(new MapFunction&lt;Tuple2&lt;String, Long&gt;, Tuple3&lt;String, Integer, Long&gt;&gt;() {
                @Override
                public Tuple3&lt;String, Integer, Long&gt; map(Tuple2&lt;String, Long&gt; value) throws Exception {
                    return Tuple3.of(value.f0.split("-")[0], Integer.parseInt(value.f0.split("-")[1]), value.f1);
                }
            })
            .keyBy(r -&gt; r.f0)
            .process(new KeyedProcessFunction&lt;String, Tuple3&lt;String, Integer, Long&gt;, Tuple2&lt;String, Long&gt;&gt;() {
                private MapState&lt;Long, Long&gt; mapState;

                @Override
                public void open(Configuration parameters) throws Exception {
                    super.open(parameters);
                    mapState = getRuntimeContext().getMapState(
                            new MapStateDescriptor&lt;Long, Long&gt;("map", Types.LONG, Types.LONG)
                    );
                }

                @Override
                public void processElement(Tuple3&lt;String, Integer, Long&gt; value, Context ctx, Collector&lt;Tuple2&lt;String, Long&gt;&gt; out) throws Exception {
                    mapState.put((long)value.f1, value.f2);
                    long sum = 0L;
                    for (Long v : mapState.values()) {
                        sum += v;
                    }
                    out.collect(Tuple2.of(value.f0, sum));
                }
            })
            .print();

        env.execute();
    }
}</programlisting>
</section>
<section xml:id="_状态变量_aggregatingstate">
<title>状态变量: AGGREGATINGSTATE</title>
<programlisting language="java" linenumbering="unnumbered">public class AggregatingStateDemo {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.addSource(new RandomLetterAndNumberSource())
            .keyBy(0)
            .flatMap(new CountFunction())
            .print();
        env.execute();
    }

    public static class CountFunction extends RichFlatMapFunction&lt;Tuple2&lt;String, Integer&gt;, Integer&gt; {
        private int count = 0;
        private transient AggregatingState&lt;Tuple2&lt;String, Integer&gt;, Integer&gt; aggregatingState;

        @Override
        public void open(Configuration parameters) throws Exception {
            super.open(parameters);
            AggregatingStateDescriptor&lt;Tuple2&lt;String, Integer&gt;, Integer, Integer&gt; descriptor = new AggregatingStateDescriptor&lt;Tuple2&lt;String, Integer&gt;, Integer, Integer&gt;(
                    "aggregatingState", new AggregateFunction&lt;Tuple2&lt;String, Integer&gt;, Integer, Integer&gt;() {
                @Override
                public Integer createAccumulator() {
                    return 0;
                }

                @Override
                public Integer add(Tuple2&lt;String, Integer&gt; value, Integer accumulator) {
                    return accumulator + 1;
                }

                @Override
                public Integer getResult(Integer accumulator) {
                    return accumulator;
                }

                @Override
                public Integer merge(Integer a, Integer b) {
                    return a + b;
                }
            }, Types.INT);
            aggregatingState = getRuntimeContext().getAggregatingState(descriptor);
        }

        @Override
        public void flatMap(Tuple2&lt;String, Integer&gt; value, Collector&lt;Integer&gt; out) throws Exception {
            count++;
            if (count % 1000 == 0) {
                out.collect(aggregatingState.get());
                aggregatingState.clear();
            } else {
                // 增量更新AggregatingState，这里每来一个新元素，对ACC累加1
                aggregatingState.add(value);
            }
        }
    }
}</programlisting>
</section>
<section xml:id="_状态变量_checkpointedfunction">
<title>状态变量: CHECKPOINTEDFUNCTION</title>
<programlisting language="java" linenumbering="unnumbered">/*
CheckpointedFunction
CheckpointedFunction 接口提供了访问 non-keyed state 的方法，需要实现如下两个方法：
void snapshotState(FunctionSnapshotContext context) throws Exception;
void initializeState(FunctionInitializationContext context) throws Exception;
进行 checkpoint 时会调用 snapshotState()。 用户自定义函数初始化时会调用 initializeState()，初始化包括第一次自定义函数初始化和从之前的 checkpoint 恢复。 因此 initializeState() 不仅是定义不同状态类型初始化的地方，也需要包括状态恢复的逻辑。
当前 operator state 以 list 的形式存在。这些状态是一个 可序列化 对象的集合 List，彼此独立，方便在改变并发后进行状态的重新分派。 换句话说，这些对象是重新分配 non-keyed state 的最细粒度。根据状态的不同访问方式，有如下几种重新分配的模式：
Even-split redistribution: 每个算子都保存一个列表形式的状态集合，整个状态由所有的列表拼接而成。当作业恢复或重新分配的时候，整个状态会按照算子的并发度进行均匀分配。 比如说，算子 A 的并发读为 1，包含两个元素 element1 和 element2，当并发读增加为 2 时，element1 会被分到并发 0 上，element2 则会被分到并发 1 上。
Union redistribution: 每个算子保存一个列表形式的状态集合。整个状态由所有的列表拼接而成。当作业恢复或重新分配时，每个算子都将获得所有的状态数据。 Do not use this feature if your list may have high cardinality. Checkpoint metadata will store an offset to each list entry, which could lead to RPC framesize or out-of-memory errors.
下面的例子中的 SinkFunction 在 CheckpointedFunction 中进行数据缓存，然后统一发送到下游，这个例子演示了列表状态数据的 event-split redistribution。
 */
public class BufferingSink implements SinkFunction&lt;Tuple2&lt;String,Integer&gt;&gt;, CheckpointedFunction {
    private final int threshold;

    private transient ListState&lt;Tuple2&lt;String, Integer&gt;&gt; checkpointedState;

    private List&lt;Tuple2&lt;String, Integer&gt;&gt; bufferedElements;

    public BufferingSink(int threshold) {
        this.threshold = threshold;
        this.bufferedElements = new ArrayList&lt;&gt;();
    }

    @Override
    public void invoke(Tuple2&lt;String, Integer&gt; value, Context context) throws Exception {
        bufferedElements.add(value);
        if (bufferedElements.size() == threshold) {
            for (Tuple2&lt;String, Integer&gt; element: bufferedElements) {
                // send it to the sink
            }
            bufferedElements.clear();
        }
    }

    @Override
    public void snapshotState(FunctionSnapshotContext context) throws Exception {
        checkpointedState.clear();
        for (Tuple2&lt;String, Integer&gt; element : bufferedElements) {
            checkpointedState.add(element);
        }
    }

    /*
initializeState 方法接收一个 FunctionInitializationContext 参数，会用来初始化 non-keyed state 的 “容器”。这些容器是一个 ListState 用于在 checkpoint 时保存 non-keyed state 对象。
注意这些状态是如何初始化的，和 keyed state 类系，StateDescriptor 会包括状态名字、以及状态类型相关信息。
     */
    @Override
    public void initializeState(FunctionInitializationContext context) throws Exception {
        ListStateDescriptor&lt;Tuple2&lt;String, Integer&gt;&gt; descriptor =
                new ListStateDescriptor&lt;&gt;(
                        "buffered-elements",
                        TypeInformation.of(new TypeHint&lt;Tuple2&lt;String, Integer&gt;&gt;() {}));

        /*
        调用不同的获取状态对象的接口，会使用不同的状态分配算法。比如 getUnionListState(descriptor) 会使用 union redistribution 算法， 而 getListState(descriptor) 则简单的使用 even-split redistribution 算法。
当初始化好状态对象后，我们通过 isRestored() 方法判断是否从之前的故障中恢复回来，如果该方法返回 true 则表示从故障中进行恢复，会执行接下来的恢复逻辑。
正如代码所示，BufferingSink 中初始化时，恢复回来的 ListState 的所有元素会添加到一个局部变量中，供下次 snapshotState() 时使用。 然后清空 ListState，再把当前局部变量中的所有元素写入到 checkpoint 中。
另外，我们同样可以在 initializeState() 方法中使用 FunctionInitializationContext 初始化 keyed state。
         */
        checkpointedState = context.getOperatorStateStore().getListState(descriptor);

        if (context.isRestored()) {
            for (Tuple2&lt;String, Integer&gt; element : checkpointedState.get()) {
                bufferedElements.add(element);
            }
        }
    }
}
/*
ListCheckpointed
ListCheckpointed 接口是 CheckpointedFunction 的精简版，仅支持 even-split redistributuion 的 list state。同样需要实现两个方法：
List&lt;T&gt; snapshotState(long checkpointId, long timestamp) throws Exception;
void restoreState(List&lt;T&gt; state) throws Exception;
snapshotState() 需要返回一个将写入到 checkpoint 的对象列表，restoreState 则需要处理恢复回来的对象列表。如果状态不可切分， 则可以在 snapshotState() 中返回 Collections.singletonList(MY_STATE)。
 */</programlisting>
</section>
<section xml:id="_广播状态">
<title>广播状态</title>
<programlisting language="java" linenumbering="unnumbered">public class Example {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        DataStreamSource&lt;Action&gt; actions = env.fromElements(
                new Action(1L, "login"),
                new Action(1L, "pay"),
                new Action(1L, "login"),
                new Action(1L, "buy")
        );

        DataStreamSource&lt;Pattern&gt; patterns = env
                .fromElements(
                        new Pattern("login", "pay"),
                        new Pattern("login", "buy")
                );

        KeyedStream&lt;Action, Long&gt; actionsByUser = actions
                .keyBy((KeySelector&lt;Action, Long&gt;) action -&gt; action.userId);
        MapStateDescriptor&lt;Void, Pattern&gt; bcStateDescriptor = new MapStateDescriptor&lt;&gt;(
                "patterns", Types.VOID, Types.POJO(Pattern.class));
        BroadcastStream&lt;Pattern&gt; bcastedPatterns = patterns.broadcast(bcStateDescriptor);
        DataStream&lt;Tuple2&lt;Long, Pattern&gt;&gt; matches = actionsByUser
                .connect(bcastedPatterns)
                .process(new PatternEvaluator());

        matches.print();

        env.execute();
    }

    public static class PatternEvaluator
            extends KeyedBroadcastProcessFunction&lt;Long, Action, Pattern, Tuple2&lt;Long, Pattern&gt;&gt; {

        // handle for keyed state (per user)
        ValueState&lt;String&gt; prevActionState;

        @Override
        public void open(Configuration conf) {
            // initialize keyed state
            prevActionState = getRuntimeContext().getState(
                    new ValueStateDescriptor&lt;&gt;("lastAction", Types.STRING));
        }

        @Override
        public void processBroadcastElement(
                Pattern pattern,
                Context ctx,
                Collector&lt;Tuple2&lt;Long, Pattern&gt;&gt; out) throws Exception {

            BroadcastState&lt;Void, Pattern&gt; bcState = ctx.getBroadcastState(
                    new MapStateDescriptor&lt;&gt;("patterns", Types.VOID, Types.POJO(Pattern.class)));

            bcState.put(null, pattern);
        }

        @Override
        public void processElement(Action action, ReadOnlyContext ctx,
                                   Collector&lt;Tuple2&lt;Long, Pattern&gt;&gt; out) throws Exception {
            Pattern pattern = ctx.getBroadcastState(
                    new MapStateDescriptor&lt;&gt;("patterns", Types.VOID, Types.POJO(Pattern.class))).get(null);

            String prevAction = prevActionState.value();
            if (pattern != null &amp;&amp; prevAction != null) {
                if (pattern.action1.equals(prevAction) &amp;&amp; pattern.action2.equals(action.action)) {

                    // Found a MATCH
                    out.collect(new Tuple2&lt;&gt;(ctx.getCurrentKey(), pattern));
                }
            }
            prevActionState.update(action.action);
        }
    }

    public static class Pattern {
        public String action1;
        public String action2;

        public Pattern() {
        }

        public Pattern(String action1, String action2) {
            this.action1 = action1;
            this.action2 = action2;
        }

        @Override
        public String toString() {
            return "Pattern{" +
                    "action1='" + action1 + '\'' +
                    ", action2='" + action2 + '\'' +
                    '}';
        }
    }

    public static class Action {
        public Long userId;
        public String action;

        public Action() {
        }

        public Action(Long userId, String action) {
            this.userId = userId;
            this.action = action;
        }

        @Override
        public String toString() {
            return "Action{" +
                    "userId=" + userId +
                    ", action='" + action + '\'' +
                    '}';
        }
    }
}</programlisting>
</section>
<section xml:id="_状态后端的配置">
<title>状态后端的配置</title>
<programlisting language="java" linenumbering="unnumbered">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
env.setStateBackend(new FsStateBackend("file:///tmp/checkpoints", false));
env.enableCheckpointing(10000L);</programlisting>
</section>
<section xml:id="_状态机">
<title>状态机</title>
<simpara>连续三次登录失败</simpara>
<literallayout class="monospaced">digraph finite_state_machine {
    rankdir=LR;
    INITIAL -&gt; SUCCESS [label = "success"];
    INITIAL -&gt; S1 [label = "fail"];
    S1 -&gt; S2 [label = "fail"];
    S2 -&gt; FAIL [label = "fail"];
    S1 -&gt; SUCCESS [label = "success"];
    S2 -&gt; SUCCESS [label = "success"];
    SUCCESS -&gt; INITIAL [label = "重置"];
    FAIL -&gt; INITIAL [label = "重置"];
}</literallayout>
<programlisting language="java" linenumbering="unnumbered">public class Example {
    private static HashMap&lt;Tuple2&lt;String, String&gt;, String&gt; hashMap = new HashMap&lt;Tuple2&lt;String, String&gt;, String&gt;();

    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        hashMap.put(Tuple2.of("initial", "fail"), "S1");
        hashMap.put(Tuple2.of("S1", "fail"), "S2");
        hashMap.put(Tuple2.of("S2", "fail"), "fail");
        hashMap.put(Tuple2.of("initial", "success"), "success");
        hashMap.put(Tuple2.of("S1", "success"), "success");
        hashMap.put(Tuple2.of("S2", "success"), "success");

        env
                .fromElements(
                        Tuple3.of("user-1", "fail", 1000L),
                        Tuple3.of("user-1", "fail", 2000L),
                        Tuple3.of("user-1", "fail", 3000L),
                        Tuple3.of("user-2", "fail", 4000L),
                        Tuple3.of("user-2", "success", 5000L)
                )
                .assignTimestampsAndWatermarks(
                        WatermarkStrategy.&lt;Tuple3&lt;String, String, Long&gt;&gt;forMonotonousTimestamps()
                        .withTimestampAssigner(new SerializableTimestampAssigner&lt;Tuple3&lt;String, String, Long&gt;&gt;() {
                            @Override
                            public long extractTimestamp(Tuple3&lt;String, String, Long&gt; element, long recordTimestamp) {
                                return element.f2;
                            }
                        })
                )
                .keyBy(r -&gt; r.f0)
                .process(new KeyedProcessFunction&lt;String, Tuple3&lt;String, String, Long&gt;, String&gt;() {
                    private transient ValueState&lt;String&gt; valueState;

                    @Override
                    public void open(Configuration parameters) throws Exception {
                        super.open(parameters);
                        valueState = getRuntimeContext().getState(new ValueStateDescriptor&lt;String&gt;("state", Types.STRING));
                    }

                    @Override
                    public void processElement(Tuple3&lt;String, String, Long&gt; value, Context ctx, Collector&lt;String&gt; out) throws Exception {
                        if (valueState.value() == null) {
                            valueState.update("initial");
                        }
                        String newState = hashMap.get(Tuple2.of(valueState.value(), value.f1));
                        if (newState.equals("success")) {
                            out.collect(value.f0 + "登录成功");
                            valueState.update("initial");
                        } else if (newState.equals("fail")) {
                            out.collect(value.f0 + "登录失败");
                            valueState.update("initial");
                        } else {
                            valueState.update(newState);
                        }
                    }
                })
                .print();

        env.execute();
    }
}</programlisting>
</section>
</section>
</article>